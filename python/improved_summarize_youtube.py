"""
YouTubeå‹•ç”»ã‚’è¦ç´„è¨˜äº‹ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
æ—¥æœ¬èªåŒ–ã¨è£…é£¾ã‚’è‡ªå‹•ã§è¡Œã„ã¾ã™
"""
import sys
import os
import re
import json
import subprocess
import pytz
from datetime import datetime
from youtube_transcript_api import YouTubeTranscriptApi
from googleapiclient.discovery import build
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')


def get_channel_id(channel_url):
    """ãƒãƒ£ãƒ³ãƒãƒ«URLã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’æŠ½å‡º"""
    # @handleå½¢å¼
    handle_match = re.search(r'@([\w-]+)', channel_url)
    if handle_match:
        # YouTube Data APIã§ãƒãƒ³ãƒ‰ãƒ«ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—
        if YOUTUBE_API_KEY:
            try:
                youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
                request = youtube.search().list(
                    part='snippet',
                    q=f'@{handle_match.group(1)}',
                    type='channel',
                    maxResults=1
                )
                response = request.execute()
                if response['items']:
                    return response['items'][0]['snippet']['channelId']
            except Exception as e:
                print(f"ãƒãƒ³ãƒ‰ãƒ«ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«IDå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # UC...å½¢å¼ã®ãƒãƒ£ãƒ³ãƒãƒ«ID
    channel_id_match = re.search(r'(UC[\w-]{22})', channel_url)
    if channel_id_match:
        return channel_id_match.group(1)
    
    # /channel/å½¢å¼
    channel_match = re.search(r'/channel/(UC[\w-]{22})', channel_url)
    if channel_match:
        return channel_match.group(1)
    
    return None


def get_channel_latest_videos(channel_id, max_results=10, output_dir=None):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®æœªå‡¦ç†å‹•ç”»ã‚’å–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆ: video_idã®ã¿å–å¾—â†’é™¤å¤–â†’è©³ç´°æƒ…å ±å–å¾—ï¼‰"""
    if not YOUTUBE_API_KEY:
        print("ã‚¨ãƒ©ãƒ¼: YOUTUBE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return []
    
    try:
        import time
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        
        # å‡¦ç†æ¸ˆã¿å‹•ç”»ãƒªã‚¹ãƒˆã‚’ä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«I/Oå‰Šæ¸›ï¼‰
        processed_cache = load_processed_videos(output_dir)
        
        # ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆIDã‚’å–å¾—
        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        channel_request = youtube.channels().list(
            part='contentDetails',
            id=channel_id
        )
        channel_response = channel_request.execute()
        
        if not channel_response['items']:
            print(f"ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {channel_id}")
            return []
        
        uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: video_idã‚’ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã§å–å¾—ã—ãªãŒã‚‰é™¤å¤–åˆ¤å®šï¼ˆæœ€å°APIã‚³ãƒ¼ãƒ«ï¼‰
        unprocessed_ids = []
        next_page_token = None
        total_checked = 0
        
        while len(unprocessed_ids) < max_results:
            time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            playlist_request = youtube.playlistItems().list(
                part='contentDetails',  # snippetã§ã¯ãªãcontentDetailsã®ã¿ï¼ˆè»½é‡ï¼‰
                playlistId=uploads_playlist_id,
                maxResults=50,
                pageToken=next_page_token
            )
            playlist_response = playlist_request.execute()
            
            if not playlist_response['items']:
                break
            
            # å–å¾—ã—ãŸIDã‚’å³åº§ã«å‡¦ç†æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ã§é«˜é€Ÿï¼‰
            for item in playlist_response['items']:
                video_id = item['contentDetails']['videoId']
                total_checked += 1
                
                if video_id not in processed_cache:  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é«˜é€Ÿãƒã‚§ãƒƒã‚¯
                    unprocessed_ids.append(video_id)
                    
                    # å¿…è¦ãªä»¶æ•°ã«é”ã—ãŸã‚‰å³åº§ã«çµ‚äº†ï¼ˆç„¡é§„ãªãƒšãƒ¼ã‚¸å–å¾—ã‚’é˜²ãï¼‰
                    if len(unprocessed_ids) >= max_results:
                        break
            
            # å¿…è¦ãªä»¶æ•°ã«é”ã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—çµ‚äº†
            if len(unprocessed_ids) >= max_results:
                break
            
            next_page_token = playlist_response.get('nextPageToken')
            if not next_page_token:
                break
        
        print(f"  ãƒã‚§ãƒƒã‚¯ã—ãŸå‹•ç”»æ•°: {total_checked}ä»¶")
        print(f"  æœªå‡¦ç†ã®å‹•ç”»ID: {len(unprocessed_ids)}ä»¶")
        
        if not unprocessed_ids:
            return []
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: æœªå‡¦ç†å‹•ç”»ã®è©³ç´°æƒ…å ±ã®ã¿ã‚’ãƒãƒƒãƒå–å¾—ï¼ˆæœ€å¤§50ä»¶ãšã¤ï¼‰
        unprocessed_videos = []
        for i in range(0, len(unprocessed_ids), 50):
            batch_ids = unprocessed_ids[i:i+50]
            time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            
            video_request = youtube.videos().list(
                part='snippet',
                id=','.join(batch_ids)
            )
            video_response = video_request.execute()
            
            for item in video_response['items']:
                unprocessed_videos.append({
                    'video_id': item['id'],
                    'title': item['snippet']['title'],
                    'channel': item['snippet']['channelTitle'],
                    'url': f'https://www.youtube.com/watch?v={item["id"]}',
                    'published_at': item['snippet']['publishedAt'],
                    'description': item['snippet']['description']
                })
        
        print(f"  è©³ç´°æƒ…å ±ã‚’å–å¾—: {len(unprocessed_videos)}ä»¶")
        return unprocessed_videos
    
    except Exception as e:
        print(f"ãƒãƒ£ãƒ³ãƒãƒ«å‹•ç”»å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []


def parse_channel_list(file_path='channel-list.md'):
    """channel-list.mdã‚’è§£æã—ã¦ãƒãƒ£ãƒ³ãƒãƒ«URLãƒªã‚¹ãƒˆã‚’å–å¾—"""
    if not os.path.exists(file_path):
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ»ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’é™¤å¤–ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    exclude_patterns = [
        '@channelname',
        'UCxxxxxxxxxxxxxxxxxxxxxx',
        '/channel/UCxxxxxx',
    ]
    
    channels = []
    in_code_block = False
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã¯ã‚¹ã‚­ãƒƒãƒ—
            if line.startswith('```'):
                in_code_block = not in_code_block
                continue
            if in_code_block:
                continue
            
            # URLã‚’å«ã‚€è¡Œã‚’æŠ½å‡º
            if 'youtube.com' in line or line.startswith('UC'):
                # ã‚µãƒ³ãƒ—ãƒ«URLã‚’é™¤å¤–
                if any(pattern in line for pattern in exclude_patterns):
                    continue
                
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒªãƒ³ã‚¯ã‹ã‚‰æŠ½å‡º
                url_match = re.search(r'https?://[^\s\)]+', line)
                if url_match:
                    channels.append(url_match.group(0))
                elif line.startswith('UC') and 'xxxx' not in line:
                    channels.append(line)
    
    return channels


# å‡¦ç†æ¸ˆã¿å‹•ç”»ãƒªã‚¹ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼‰
_processed_videos_cache = None
_processed_videos_cache_dir = None


def load_processed_videos(output_dir):
    """å‡¦ç†æ¸ˆã¿å‹•ç”»ãƒªã‚¹ãƒˆã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    global _processed_videos_cache, _processed_videos_cache_dir
    
    if not output_dir:
        return set()
    
    # åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿”ã™
    if _processed_videos_cache is not None and _processed_videos_cache_dir == output_dir:
        return _processed_videos_cache
    
    processed_file = os.path.join(output_dir, 'processed_videos.json')
    
    if not os.path.exists(processed_file):
        _processed_videos_cache = set()
        _processed_videos_cache_dir = output_dir
        return _processed_videos_cache
    
    try:
        with open(processed_file, 'r', encoding='utf-8') as f:
            processed = json.load(f)
        _processed_videos_cache = set(processed.get('video_ids', []))
        _processed_videos_cache_dir = output_dir
        return _processed_videos_cache
    except (json.JSONDecodeError, IOError):
        _processed_videos_cache = set()
        _processed_videos_cache_dir = output_dir
        return _processed_videos_cache


def is_video_processed(video_id, output_dir, processed_cache=None):
    """å‹•ç”»ãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    
    Args:
        video_id: ãƒã‚§ãƒƒã‚¯ã™ã‚‹å‹•ç”»ID
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        processed_cache: äº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆsetå‹ï¼‰ã€‚Noneã®å ´åˆã¯è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
    """
    if not output_dir:
        return False
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæ¸¡ã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼ˆé«˜é€Ÿï¼‰
    if processed_cache is not None:
        return video_id in processed_cache
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã‘ã‚Œã°å¾“æ¥é€šã‚Šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    processed_file = os.path.join(output_dir, 'processed_videos.json')
    
    if not os.path.exists(processed_file):
        return False
    
    try:
        with open(processed_file, 'r', encoding='utf-8') as f:
            processed = json.load(f)
        return video_id in processed.get('video_ids', [])
    except (json.JSONDecodeError, IOError):
        return False


def mark_video_processed(video_id, output_dir, video_info=None, status='success', error_message=None):
    """å‹•ç”»ã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²ï¼ˆæˆåŠŸãƒ»å¤±æ•—ä¸¡æ–¹ï¼‰"""
    if not output_dir:
        return
    
    os.makedirs(output_dir, exist_ok=True)
    processed_file = os.path.join(output_dir, 'processed_videos.json')
    
    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    processed = {'video_ids': [], 'details': {}}
    if os.path.exists(processed_file):
        try:
            with open(processed_file, 'r', encoding='utf-8') as f:
                processed = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    
    # video_idsãƒªã‚¹ãƒˆã«è¿½åŠ 
    if video_id not in processed.get('video_ids', []):
        if 'video_ids' not in processed:
            processed['video_ids'] = []
        processed['video_ids'].append(video_id)
    
    # è©³ç´°æƒ…å ±ã‚‚ä¿å­˜
    if 'details' not in processed:
        processed['details'] = {}
    
    detail = {
        'processed_at': datetime.now().isoformat(),
        'title': video_info.get('title', '') if video_info else '',
        'channel': video_info.get('channel', '') if video_info else '',
        'status': status
    }
    
    if error_message:
        detail['error'] = error_message
    
    processed['details'][video_id] = detail
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(processed_file, 'w', encoding='utf-8') as f:
        json.dump(processed, f, ensure_ascii=False, indent=2)


def get_video_id(url_or_id):
    """YouTubeã®URLã¾ãŸã¯IDã‹ã‚‰å‹•ç”»IDã‚’æŠ½å‡º"""
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
        r'(?:embed\/)([0-9A-Za-z_-]{11})',
        r'^([0-9A-Za-z_-]{11})$'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url_or_id)
        if match:
            return match.group(1)
    return None


def get_video_info(video_id):
    """YouTube Data APIã§å‹•ç”»æƒ…å ±ã‚’å–å¾—"""
    if not YOUTUBE_API_KEY:
        return None
    
    try:
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        request = youtube.videos().list(
            part='snippet,statistics',
            id=video_id
        )
        response = request.execute()
        
        if response['items']:
            item = response['items'][0]
            return {
                'title': item['snippet']['title'],
                'channel': item['snippet']['channelTitle'],
                'published_at': item['snippet']['publishedAt'],
                'description': item['snippet']['description'],
                'view_count': item['statistics'].get('viewCount', 'N/A'),
                'like_count': item['statistics'].get('likeCount', 'N/A')
            }
    except Exception as e:
        print(f"å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
    
    return None


def get_transcript(video_id):
    """å‹•ç”»ã®å­—å¹•ã‚’å–å¾—ï¼ˆæ—¥æœ¬èªå„ªå…ˆï¼‰- ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿ã®ãŸã‚é•·ã‚ã®å¾…æ©Ÿæ™‚é–“"""
    import time
    import random
    
    # ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿: ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“ï¼ˆ5-10ç§’ï¼‰
    wait_time = random.uniform(5, 10)
    print(f"  â³ ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿ã®ãŸã‚ {wait_time:.1f}ç§’å¾…æ©Ÿä¸­...")
    time.sleep(wait_time)
    
    # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šï¼ˆ.envã§è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    proxies = None
    proxy_url = os.getenv('TRANSCRIPT_PROXY_URL')
    if proxy_url:
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        print(f"  ğŸ”’ ãƒ—ãƒ­ã‚­ã‚·ä½¿ç”¨: {proxy_url}")
    
    # Cookieè¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€.envã§è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    cookies = None
    cookie_file = os.getenv('YOUTUBE_COOKIES_FILE')
    if cookie_file and os.path.exists(cookie_file):
        print(f"  ğŸª Cookieä½¿ç”¨: {cookie_file}")
        # Cookieãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ã¯å¾Œã§å®Ÿè£…å¯èƒ½
    
    try:
        # 1. ã¾ãšæ—¥æœ¬èªå­—å¹•ã‚’è©¦ã™
        print("  â†’ æ—¥æœ¬èªå­—å¹•ã‚’æ¤œç´¢ä¸­...")
        api = YouTubeTranscriptApi(proxies=proxies) if proxies else YouTubeTranscriptApi()
        transcript_data = api.fetch(video_id, languages=['ja'])
        print("  âœ“ æ—¥æœ¬èªå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸ")
        return transcript_data, 'ja'
    except Exception as e1:
        error_type = type(e1).__name__
        error_msg = str(e1)[:100]  # æœ€åˆã®100æ–‡å­—ã®ã¿
        
        # IpBlocked ã‚¨ãƒ©ãƒ¼ã®æ¤œå‡º
        if 'IpBlocked' in error_type or 'blocking requests' in error_msg:
            print(f"\nâŒ YouTube APIãƒ–ãƒ­ãƒƒã‚¯æ¤œå‡º: IPãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™")
            print(f"   ã‚¨ãƒ©ãƒ¼: {error_type}")
            print(f"\nå›å¾©æ‰‹é †:")
            print(f"  1) æ•°åˆ†ï½æ•°æ™‚é–“å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œ")
            print(f"  2) VPN/ãƒ—ãƒ­ã‚­ã‚·ã‚’ä½¿ç”¨")
            print(f"  3) --limit ã®å€¤ã‚’æ¸›ã‚‰ã™ï¼ˆä¾‹: --limit 1ï¼‰")
            print(f"  4) å‡¦ç†é–“éš”ã‚’ç©ºã‘ã‚‹")
            return None, None
        
        print(f"  Ã— æ—¥æœ¬èªå­—å¹•ãªã—")
        try:
            # 2. è‹±èªå­—å¹•ã‚’å–å¾—ã—ã¦æ—¥æœ¬èªã«ç¿»è¨³
            print("  â†’ è‹±èªå­—å¹•ã‚’æ¤œç´¢ä¸­...")
            api = YouTubeTranscriptApi()
            transcript_list = api.list_transcripts(video_id)
            
            # è‹±èªå­—å¹•ã‚’å–å¾—
            try:
                transcript = transcript_list.find_transcript(['en'])
                print("  âœ“ è‹±èªå­—å¹•ã‚’å–å¾—ã€æ—¥æœ¬èªã«ç¿»è¨³ä¸­...")
                # æ—¥æœ¬èªã«ç¿»è¨³
                translated = transcript.translate('ja')
                transcript_data = translated.fetch()
                print("  âœ“ æ—¥æœ¬èªç¿»è¨³å®Œäº†")
                return transcript_data, 'ja-translated'
            except Exception as e2:
                # ç¿»è¨³å¤±æ•—ã—ãŸã‚‰è‹±èªã®ã¾ã¾è¿”ã™
                print("  Ã— ç¿»è¨³å¤±æ•—ã€è‹±èªã®ã¾ã¾ä½¿ç”¨...")
                transcript_data = api.fetch(video_id, languages=['en'])
                print("  âœ“ è‹±èªå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸ")
                return transcript_data, 'en'
        except Exception as e3:
            print(f"  Ã— è‹±èªå­—å¹•ãªã—")
            try:
                # 3. è‡ªå‹•æ¤œå‡ºã§å–å¾—
                print("  â†’ è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’æ¤œç´¢ä¸­...")
                api = YouTubeTranscriptApi()
                transcript_data = api.fetch(video_id)
                print("  âœ“ è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸ")
                return transcript_data, 'auto'
            except Exception as e4:
                error_type = type(e4).__name__
                error_msg = str(e4)[:100]
                print(f"\nâŒ å­—å¹•å–å¾—å¤±æ•—: {error_type}")
                print(f"   ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {error_msg}")
                print(f"   å‹•ç”»URL: https://www.youtube.com/watch?v={video_id}")
                return None, None


def format_transcript(transcript_data):
    """å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿ã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢"""
    text = ""
    for entry in transcript_data:
        if hasattr(entry, 'text'):
            text += entry.text + " "
        elif isinstance(entry, dict):
            text += entry.get('text', '') + " "
        else:
            text += str(entry) + " "
    
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def create_summary_sections(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µè½ã«åˆ†å‰²"""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    sections = []
    current_section = []
    
    for i, sentence in enumerate(sentences):
        current_section.append(sentence)
        if (i + 1) % 5 == 0 or i == len(sentences) - 1:
            sections.append(' '.join(current_section))
            current_section = []
    
    return sections


def translate_section_title(index):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã‚’å†…å®¹ã«å¿œã˜ãŸæ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã«"""
    titles = [
        "å°å…¥", "èƒŒæ™¯ãƒ»æ¦‚è¦", "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ", "è©³ç´°èª¬æ˜",
        "å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢", "æŠ€è¡“çš„è©³ç´°", "å¿œç”¨ä¾‹", "è€ƒå¯Ÿ",
        "ã¾ã¨ã‚", "çµè«–", "è¿½åŠ æƒ…å ±", "è£œè¶³"
    ]
    if index < len(titles):
        return titles[index]
    return f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {index + 1}"


def get_section_emoji(index):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«é©åˆ‡ãªçµµæ–‡å­—ã‚’å‰²ã‚Šå½“ã¦"""
    emojis = [
        "ğŸ¬", "ğŸ“‹", "â­", "ğŸ“", "ğŸ’¡", "ğŸ”§", "ğŸ¯", "ğŸ’­",
        "ğŸ“Œ", "âœ…", "ğŸ“š", "ğŸ”–", "ğŸ¨", "ğŸš€", "âš¡", "ğŸŒŸ"
    ]
    return emojis[index % len(emojis)]


def create_markdown_article(video_id, transcript_text, url, video_info=None):
    """Markdownå½¢å¼ã®è¨˜äº‹ã‚’ç”Ÿæˆï¼ˆè£…é£¾ç‰ˆï¼‰"""
    today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    # å‹•ç”»æƒ…å ±
    title = video_info['title'] if video_info else f"Video ID: {video_id}"
    channel = video_info['channel'] if video_info else "Unknown Channel"
    
    # å…¬é–‹æ—¥æ™‚ã‚’å–å¾—
    published_date = ""
    if video_info and 'published_at' in video_info:
        published_at = video_info['published_at']
        pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
        jst = pytz.timezone('Asia/Tokyo')
        pub_date_jst = pub_date.astimezone(jst)
        published_date = pub_date_jst.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')
    
    markdown = f"""# ğŸ“º {title}

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: {title}
- **ãƒãƒ£ãƒ³ãƒãƒ«**: {channel}
- **å‹•ç”»URL**: [{url}]({url})
- **å‹•ç”»ID**: {video_id}
"""
    
    if published_date:
        markdown += f"- **å…¬é–‹æ—¥**: {published_date}\n"

    if video_info:
        view_count = int(video_info['view_count']) if video_info['view_count'] != 'N/A' else 0
        like_count = int(video_info['like_count']) if video_info['like_count'] != 'N/A' else 0
        markdown += f"""- **å†ç”Ÿå›æ•°**: {view_count:,} å›
- **é«˜è©•ä¾¡æ•°**: {like_count:,}
"""

    markdown += """
## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

"""
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
    sections = create_summary_sections(transcript_text)
    
    for i, section in enumerate(sections):
        emoji = get_section_emoji(i)
        section_title = translate_section_title(i)
        markdown += f"### {emoji} {section_title}\n\n"
        markdown += f"{section}\n\n"
    
    markdown += f"""---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: {today}

</div>
"""
    
    return markdown


def calculate_quality_score(transcript_text, sections):
    """è¦ç´„è¨˜äº‹ã®ã‚¯ã‚ªãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 100
    
    # æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
    if len(transcript_text) < 500:
        score -= 30
    elif len(transcript_text) < 1000:
        score -= 10
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãƒã‚§ãƒƒã‚¯
    if len(sections) < 3:
        score -= 20
    
    # å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜å˜èªã®ç¹°ã‚Šè¿”ã—ãŒå¤šã„å ´åˆï¼‰
    words = transcript_text.lower().split()
    unique_ratio = len(set(words)) / len(words) if words else 0
    if unique_ratio < 0.3:
        score -= 20
    
    return max(0, score)


def auto_commit_and_push(file_paths, processed_count, output_dir=None):
    """ç”Ÿæˆã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«git commit & pushï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
    
    Args:
        file_paths: è¿½åŠ ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        processed_count: å‡¦ç†ã—ãŸå‹•ç”»æ•°
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆprocessed_videos.jsonç”¨ï¼‰
    """
    if not file_paths:
        print("  â„¹ï¸  è¿½åŠ ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return False
    
    try:
        # å…ˆã«git addï¼ˆpullã®å‰ã«ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ãŒå¿…è¦ï¼‰
        for file_path in file_paths:
            subprocess.run(['git', 'add', file_path], check=True, capture_output=True)
        
        # processed_videos.jsonã‚‚è¿½åŠ 
        if output_dir:
            processed_file = os.path.join(output_dir, 'processed_videos.json')
            if os.path.exists(processed_file):
                subprocess.run(['git', 'add', processed_file], check=True, capture_output=True)
        
        print(f"  âœ“ {len(file_paths)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°")
        
        # git pullã§æœ€æ–°ã‚’å–å¾—ï¼ˆç«¶åˆé˜²æ­¢ï¼‰
        print("  ğŸ“¥ ãƒªãƒ¢ãƒ¼ãƒˆã‹ã‚‰æœ€æ–°ã‚’å–å¾—ä¸­...")
        pull_result = subprocess.run(
            ['git', 'pull', '--rebase'],
            capture_output=True,
            text=True
        )
        if pull_result.returncode != 0:
            print(f"  âš ï¸  pullå¤±æ•—ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰: {pull_result.stderr.strip()}")
        else:
            print("  âœ“ pullå®Œäº†")
        
        # ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
        if len(file_paths) == 1:
            commit_msg = f"ğŸ“ è¦ç´„è¿½åŠ : 1ä»¶ã®å‹•ç”»ã‚’å‡¦ç†"
        else:
            commit_msg = f"ğŸ“ è¦ç´„è¿½åŠ : {len(file_paths)}ä»¶ã®å‹•ç”»ã‚’å‡¦ç†"
        
        # git commit
        result = subprocess.run(
            ['git', 'commit', '-m', commit_msg],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼
            if 'nothing to commit' in result.stdout:
                print("  â„¹ï¸  å¤‰æ›´ãªã—ï¼ˆæ—¢ã«ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ï¼‰")
                return False
            else:
                print(f"  âš ï¸  ã‚³ãƒŸãƒƒãƒˆå¤±æ•—: {result.stderr}")
                return False
        
        print(f"  âœ“ ã‚³ãƒŸãƒƒãƒˆå®Œäº†ï¼ˆ{len(file_paths)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
        
        # git push
        print("  ğŸ“¤ ãƒ—ãƒƒã‚·ãƒ¥ä¸­...")
        push_result = subprocess.run(
            ['git', 'push'],
            capture_output=True,
            text=True
        )
        
        if push_result.returncode == 0:
            print("  âœ“ ãƒ—ãƒƒã‚·ãƒ¥å®Œäº† - Copilotãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒãƒˆãƒªã‚¬ãƒ¼ã•ã‚Œã¾ã™")
            return True
        else:
            print(f"  âš ï¸  ãƒ—ãƒƒã‚·ãƒ¥å¤±æ•—: {push_result.stderr}")
            return False
            
    except subprocess.CalledProcessError as e:
        print(f"  âœ— Gitæ“ä½œã‚¨ãƒ©ãƒ¼: {e}")
        return False
    except FileNotFoundError:
        print("  âœ— gitã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False


def main(video_url, output_dir=None, video_info_cache=None):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†
    
    Args:
        video_url: å‹•ç”»URL
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        video_info_cache: äº‹å‰å–å¾—æ¸ˆã¿ã®å‹•ç”»æƒ…å ±ï¼ˆget_channel_latest_videos()ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ï¼‰
    """
    print(f"å‹•ç”»ã‚’å‡¦ç†ä¸­: {video_url}")
    
    # å‹•ç”»IDã‚’å–å¾—
    video_id = get_video_id(video_url)
    if not video_id:
        print("ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªYouTube URLã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        return None
    
    print(f"å‹•ç”»ID: {video_id}")
    
    # å‹•ç”»æƒ…å ±ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰
    if video_info_cache:
        print("å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰")
        video_info = video_info_cache
    else:
        print("å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...ï¼ˆAPIï¼‰")
        import time
        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        video_info = get_video_info(video_id)
    
    if video_info:
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {video_info['title']}")
        print(f"ãƒãƒ£ãƒ³ãƒãƒ«: {video_info['channel']}")
    
    # å­—å¹•ã‚’å–å¾—
    print("å­—å¹•ã‚’å–å¾—ä¸­...")
    transcript_data, lang = get_transcript(video_id)
    
    if not transcript_data:
        error_msg = "å­—å¹•ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
        print(f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
        # å¤±æ•—ã—ãŸå‹•ç”»ã‚‚è¨˜éŒ²ï¼ˆæ¬¡å›ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŸã‚ï¼‰
        mark_video_processed(video_id, output_dir, video_info, status='failed', error_message=error_msg)
        return None
    
    print(f"å­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆè¨€èª: {lang}ï¼‰")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢
    transcript_text = format_transcript(transcript_data)
    print(f"ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®é•·ã•: {len(transcript_text)} æ–‡å­—")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²
    sections = create_summary_sections(transcript_text)
    
    # ã‚¯ã‚ªãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢è¨ˆç®—
    quality_score = calculate_quality_score(transcript_text, sections)
    print(f"ã‚¯ã‚ªãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {quality_score}/100")
    
    # Markdownè¨˜äº‹ã‚’ç”Ÿæˆ
    markdown_content = create_markdown_article(
        video_id, transcript_text, video_url, video_info
    )
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        if video_info:
            # æŠ•ç¨¿æ—¥æ™‚ã‚’å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            published_at = video_info['published_at']
            # ISO 8601å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹ (ä¾‹: 2024-12-15T03:20:00Z)
            pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            
            # æ—¥æœ¬æ™‚é–“ã«å¤‰æ›
            jst = pytz.timezone('Asia/Tokyo')
            pub_date_jst = pub_date.astimezone(jst)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— (å¹´æœˆæ—¥æ™‚åˆ†)
            timestamp = pub_date_jst.strftime('%Y%m%d%H%M')
            
            # ãƒãƒ£ãƒ³ãƒãƒ«åã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ï¼ˆå®‰å…¨ãªæ–‡å­—ã®ã¿ï¼‰
            safe_channel = re.sub(r'[^\w\s-]', '', video_info['channel'])
            safe_channel = re.sub(r'[-\s]+', '_', safe_channel)
            
            # å¹´/æœˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
            year = pub_date_jst.strftime('%Y')
            month = pub_date_jst.strftime('%m')
            dir_path = os.path.join(output_dir, year, month)
            os.makedirs(dir_path, exist_ok=True)
            
            output_file = os.path.join(dir_path, f"{timestamp}_{safe_channel}.md")
        else:
            output_file = os.path.join(output_dir, f"summary_{video_id}.md")
    else:
        output_file = f"summary_{video_id}.md"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"âœ“ è¦ç´„è¨˜äº‹ã‚’ä½œæˆã—ã¾ã—ãŸ: {output_file}")
    
    # å‡¦ç†æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²
    mark_video_processed(video_id, output_dir, video_info)
    
    result = {
        'file_path': output_file,
        'quality_score': quality_score,
        'video_id': video_id,
        'video_info': video_info
    }
    
    return result


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹:")
        print("  å˜ä¸€å‹•ç”»: python improved_summarize_youtube.py <YouTube URL> [å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª] [--push]")
        print("  ãƒãƒ£ãƒ³ãƒãƒ«: python improved_summarize_youtube.py --channel <Channel URL> [--limit N] [å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª] [--push]")
        print("  ãƒªã‚¹ãƒˆã‹ã‚‰: python improved_summarize_youtube.py --from-list [--limit N] [å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª] [--push]")
        print("\nã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
        print("  --channel <URL>  æŒ‡å®šãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»ã‚’å‡¦ç†")
        print("  --from-list      channel-list.mdã®å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã‚’å‡¦ç†")
        print("  --limit N        å–å¾—ã™ã‚‹å‹•ç”»æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰")
        print("  --push           ç”Ÿæˆå¾Œã«è‡ªå‹•çš„ã«git commit & push")
        print("\nä¾‹:")
        print("  python improved_summarize_youtube.py --channel https://www.youtube.com/@AllAboutAI --limit 1 xserver/summaries --push")
        print("  python improved_summarize_youtube.py --from-list --limit 5 xserver/summaries")
        sys.exit(1)
    
    # å¼•æ•°è§£æ
    mode = 'single'  # single, channel, list
    video_url = None
    channel_url = None
    output_dir = None
    auto_push = False
    limit = 10
    
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        
        if arg == '--channel':
            mode = 'channel'
            if i + 1 < len(sys.argv):
                channel_url = sys.argv[i + 1]
                i += 2
            else:
                print("ã‚¨ãƒ©ãƒ¼: --channel ã«ã¯URLãŒå¿…è¦ã§ã™")
                sys.exit(1)
        elif arg == '--from-list':
            mode = 'list'
            i += 1
        elif arg == '--limit':
            if i + 1 < len(sys.argv):
                limit = int(sys.argv[i + 1])
                i += 2
            else:
                print("ã‚¨ãƒ©ãƒ¼: --limit ã«ã¯æ•°å€¤ãŒå¿…è¦ã§ã™")
                sys.exit(1)
        elif arg == '--push':
            auto_push = True
            i += 1
        elif not arg.startswith('--'):
            if mode == 'single' and not video_url:
                video_url = arg
            else:
                output_dir = arg
            i += 1
        else:
            i += 1
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¥å‡¦ç†
    if mode == 'single':
        # å˜ä¸€å‹•ç”»å‡¦ç†
        if not video_url:
            print("ã‚¨ãƒ©ãƒ¼: å‹•ç”»URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)
        
        result = main(video_url, output_dir)
        
        if result and result['quality_score'] < 50:
            print(f"\nâš ï¸  è­¦å‘Š: ã‚¯ã‚ªãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ãŒä½ã„ã§ã™ ({result['quality_score']}/100)")
            print("æ‰‹å‹•ã§ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        # è‡ªå‹•ãƒ—ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆ
        if auto_push and result:
            print("\nğŸ”„ Gitæ“ä½œã‚’å®Ÿè¡Œä¸­...")
            auto_commit_and_push([result['file_path']], 1, output_dir)
    
    elif mode == 'channel':
        # ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰æœ€æ–°å‹•ç”»ã‚’å‡¦ç†
        if not channel_url:
            print("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ãƒãƒ«URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)
        
        print(f"\nğŸ“º ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰æœ€æ–°{limit}ä»¶ã®å‹•ç”»ã‚’å–å¾—ä¸­...")
        print(f"ãƒãƒ£ãƒ³ãƒãƒ«: {channel_url}\n")
        
        channel_id = get_channel_id(channel_url)
        if not channel_id:
            print("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(1)
        
        print(f"ãƒãƒ£ãƒ³ãƒãƒ«ID: {channel_id}")
        
        videos = get_channel_latest_videos(channel_id, limit, output_dir)
        if not videos:
            print("å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(1)
        
        print(f"\nå–å¾—ã—ãŸå‹•ç”»: {len(videos)}ä»¶\n")
        
        processed_count = 0
        skipped_count = 0
        failed_count = 0
        processed_files = []  # å‡¦ç†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        
        for i, video in enumerate(videos, 1):
            print(f"\n{'='*60}")
            print(f"[{i}/{len(videos)}] å‡¦ç†ä¸­: {video['title']}")
            print(f"{'='*60}")
            
            # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
            if output_dir and is_video_processed(video['video_id'], output_dir):
                print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™")
                skipped_count += 1
                continue
            
            # video_infoã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦æ¸¡ã™ï¼ˆäºŒé‡APIå‘¼ã³å‡ºã—ã‚’é˜²ãï¼‰
            video_info_cache = {
                'title': video['title'],
                'channel': video['channel'],
                'published_at': video['published_at'],
                'description': video.get('description', ''),
                'view_count': 'N/A',  # get_channel_latest_videos()ã§ã¯å–å¾—ã—ã¦ã„ãªã„
                'like_count': 'N/A'
            }
            result = main(video['url'], output_dir, video_info_cache)
            
            if result:
                processed_count += 1
                processed_files.append(result['file_path'])
                if result['quality_score'] < 50:
                    print(f"âš ï¸  ã‚¯ã‚ªãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ä½: {result['quality_score']}/100")
            else:
                failed_count += 1
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å‡¦ç†å®Œäº†")
        print(f"{'='*60}")
        print(f"âœ… å‡¦ç†æˆåŠŸ: {processed_count}ä»¶")
        print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶")
        print(f"âŒ å¤±æ•—: {failed_count}ä»¶")
        print(f"åˆè¨ˆ: {len(videos)}ä»¶")
        
        # è‡ªå‹•ãƒ—ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã€å‡¦ç†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
        if auto_push and processed_files:
            print("\nğŸ”„ Gitæ“ä½œã‚’å®Ÿè¡Œä¸­...")
            auto_commit_and_push(processed_files, processed_count, output_dir)
    
    elif mode == 'list':
        # channel-list.mdã‹ã‚‰å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã®å‹•ç”»ã‚’åé›†ã—ã€æ–°ã—ã„é †ã«å‡¦ç†
        print(f"\nğŸ“‹ channel-list.mdã‹ã‚‰å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã‚’å‡¦ç†ä¸­...\n")
        
        channels = parse_channel_list()
        if not channels:
            print("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(1)
        
        print(f"ç™»éŒ²ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {len(channels)}ä»¶")
        print(f"å–å¾—ä»¶æ•°: å…¨ãƒãƒ£ãƒ³ãƒãƒ«åˆè¨ˆã§æœ€æ–°{limit}ä»¶\n")
        
        # å„ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰å¤šã‚ã«å–å¾—ã—ã¦ã€å¾Œã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸ã¶æ–¹å¼
        # ã‚ˆã‚Šæ–°ã—ã„å‹•ç”»ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ãŸã‚
        per_channel_limit = max(5, limit)  # æœ€ä½5ä»¶ã€ã¾ãŸã¯limitä»¶
        print(f"ğŸ’¡ æ–°ç€å„ªå…ˆãƒ¢ãƒ¼ãƒ‰: å„ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰æœ€å¤§{per_channel_limit}ä»¶ãšã¤å–å¾—ã—ã€æŠ•ç¨¿æ—¥ãŒæ–°ã—ã„é †ã«{limit}ä»¶å‡¦ç†\n")
        
        # å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰å‹•ç”»ã‚’åé›†
        all_videos = []
        
        for ch_idx, channel_url in enumerate(channels, 1):
            print(f"[{ch_idx}/{len(channels)}] ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰å‹•ç”»ã‚’å–å¾—ä¸­: {channel_url}")
            
            channel_id = get_channel_id(channel_url)
            if not channel_id:
                print("  âš ï¸  ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                continue
            
            # å„ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰å¤šã‚ã«å–å¾—
            videos = get_channel_latest_videos(channel_id, per_channel_limit, output_dir)
            if videos:
                for video in videos:
                    video['channel_url'] = channel_url  # ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã‚’è¿½åŠ 
                all_videos.extend(videos)
                print(f"  âœ“ {len(videos)}ä»¶ã®æœªå‡¦ç†å‹•ç”»ã‚’å–å¾—")
            else:
                print(f"  â„¹ï¸  æœªå‡¦ç†å‹•ç”»ãªã—")
        
        if not all_videos:
            print("\nå…¨ãƒãƒ£ãƒ³ãƒãƒ«ã§æœªå‡¦ç†ã®å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(0)
        
        # å…¬é–‹æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        all_videos.sort(key=lambda x: x['published_at'], reverse=True)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š åé›†çµæœ")
        print(f"{'='*60}")
        print(f"å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰åé›†ã—ãŸæœªå‡¦ç†å‹•ç”»: {len(all_videos)}ä»¶")
        print(f"ã“ã‚Œã‹ã‚‰å‡¦ç†ã™ã‚‹å‹•ç”»: {min(limit, len(all_videos))}ä»¶")
        print(f"{'='*60}\n")
        
        # ä¸Šä½limitä»¶ã ã‘ã‚’å‡¦ç†
        total_processed = 0
        total_failed = 0
        processed_files = []  # å‡¦ç†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        
        for i, video in enumerate(all_videos[:limit], 1):
            print(f"\n{'='*60}")
            print(f"[{i}/{min(limit, len(all_videos))}] å‡¦ç†ä¸­")
            print(f"{'='*60}")
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {video['title']}")
            print(f"å…¬é–‹æ—¥: {video['published_at']}")
            print(f"å‹•ç”»URL: {video['url']}")
            
            # video_infoã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦æ¸¡ã™ï¼ˆäºŒé‡APIå‘¼ã³å‡ºã—ã‚’é˜²ãï¼‰
            video_info_cache = {
                'title': video['title'],
                'channel': video['channel'],
                'published_at': video['published_at'],
                'description': video.get('description', ''),
                'view_count': 'N/A',
                'like_count': 'N/A'
            }
            result = main(video['url'], output_dir, video_info_cache)
            
            if result:
                total_processed += 1
                processed_files.append(result['file_path'])
            else:
                total_failed += 1
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ å…¨ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†å®Œäº†")
        print(f"{'='*60}")
        print(f"âœ… å‡¦ç†æˆåŠŸ: {total_processed}ä»¶")
        print(f"âŒ å¤±æ•—: {total_failed}ä»¶")
        
        # è‡ªå‹•ãƒ—ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã€å‡¦ç†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
        if auto_push and processed_files:
            print("\nğŸ”„ Gitæ“ä½œã‚’å®Ÿè¡Œä¸­...")
            auto_commit_and_push(processed_files, total_processed, output_dir)
