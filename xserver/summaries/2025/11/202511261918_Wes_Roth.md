# ğŸ“º Claude turns chaotic evil

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: Claude turns chaotic evil
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=T_0DGMeekJM](https://www.youtube.com/watch?v=T_0DGMeekJM)
- **å‹•ç”»ID**: T_0DGMeekJM
- **å…¬é–‹æ—¥**: 2025å¹´11æœˆ26æ—¥ 19:18
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

There are tons of AIA news. It's getting kind of hard to keep up with them all, but here are a few very interesting things that you should be keeping an eye on that happened within the last couple days that will likely have a pretty big impact. Let's begin. First and foremost, we have Enthropic with their new research into AI alignment. This one is talking about emergent misalignment from reward hacking.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

When these models learn to cheat on certain tests, they don't just stop there. They go allin on this new evil persona. They think if they brand us this way, then we shall be this way. Well, actually that's Shakespeare's King Lear, but as you'll see, there's a lot of overlap. Also, the White House is launching the Genesis mission.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

What is the Genesis mission? Well, it's sort of the Manhattan Project for AI. The world's most powerful scientific platform to ever be built has launched. This Manhattan Project level leap will fundamentally transform the future of American science and innovation. Also, the new Claude model is playing Pokemon.

### ğŸ“ è©³ç´°èª¬æ˜

I probably should have a better transition between those new stories. But but this is also a new project that just started that I'm kind of excited about. So Opus 4.5 decided to name its in-game character Claude. When asked why, it said given AI unprecedented reasoning capabilities, and the first thing it'll do is fill out a form correctly. Elon Musk is gearing up to have Gro 5, the yet unreleased models that he's saying has a 10% chance of being AGI.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

Well, he's trying to see if Grock 5 when released will be able to beat the best human team at League of Legends playing the game just like humans would, just like you and I would. Can only look at the monitor with a camera, seeing no more than what a person with 2020 vision would see. Reaction latency and click rate no faster than a human. And the Gro 5 is designed to be able to play any game just by reading the instructions and experimenting. If this is true and it will have these capabilities, that would be kind of mind-blowing.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

We've seen similar things recently out of Google DeepMind with their SIMA 2 project. It's it's very exciting and that project is backed by Gemini, the model Gemini, the large language model that's learning to play these games. But having these large language models be able to take the world's best at a competitive game playing the same way that a human being would. I mean, that would certainly be next level. Boresh Patel is interviewing Ilia Sutsk, so that's already been released.

### ğŸ¯ å¿œç”¨ä¾‹

So, check it out if you haven't. I'm about 40% into it so far. has been very interesting. One thing that jumped out at me in the first kind of first third or so of the interview is that Ilia is saying that doing reinforcement learning might make these language models and neural nets AIS in general be a little bit too like focused on the immediate goal that they're trying to achieve. And that kind of makes them hard to pursue long horizon tasks.

### ğŸ’­ è€ƒå¯Ÿ

They kind of forget what they're doing. They'll try one thing. If it fails, they'll try something else. If that fails, they might just go back to this thing without sort of realizing like, hey, we're not really moving it forward. And interesting, they talk about sort of human emotions being a value function, which I interpret as sort of this idea that we humans were kind of chasing some future vision where we expect we'll be in a better state, right?

### ğŸ“Œ ã¾ã¨ã‚

If I get that promotion, if I buy this car, if I work out and eat right, then you know, in the future, I'll be happy. I'll be in a state of bliss because I would have achieved all these things. And then we work very hard at pursuing those goals often over very long time periods. And I would say mostly we do it pretty intelligently. We try different things.

### âœ… çµè«–

We see what works. But we're always moving towards kind of like that future state. And you could kind of see how emotions play into that. Described a situation where somebody when they had some injury, they they lost the ability to feel and perceive their own emotions. Everything else was fine, but that sort of emotional connection was lost.

### ğŸ“š è¿½åŠ æƒ…å ±

And one interesting thing that they noticed is that person kind of lost the ability to make decisions or at least the ability to make decisions became it became very difficult to do that. Kind of an interesting thing to think about, right? Because often we can kind of write our pro and cons list till the end of time. But it does seem like certain decisions there's a certain feeling that we can't describe. we can't necessarily find the logical reasons behind that we feel a little bit more connected to.

### ğŸ”– è£œè¶³

They're like, well, this is kind of if if we take this path, it'll get us to that happy place where I want to be in life at some point in the future. And we kind of gravitate towards that. And he's saying that sort of being able to recreate that for AI might be kind of a great shortcut, a very efficient way of getting that kind of long horizon tasks, continual learning to kind of getting you to that point faster. I'll try to do a more in-depth breakdown after I finish watching the interview, but so far it's been pretty interesting. Now, Ilia doesn't really talk about exactly what he's working on, but it's still a very fascinating discussion from one of the foremost people in this AI research area talking about kind of the big ideas that are challenges and potentials for progress.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

And I know what you're wondering. You're wondering, does he talk about feeling the AGI? Does that come up in interview? Yes, of course it does. And we finally have this thing that I've been kind of waiting for for quite a while.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

I really wanted this to roll out. I wasn't sure why it took so long. I guess maybe there was some very specific difficulty with with this particular approach happening. But it's basically this idea that you're able to chat with GPT by typing back and forth. You have that conversation or you click the advanced voice button and then you're able to talk with it with voice back and forth.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

But there's no overlap. Like you can't begin a conversation with text. Continue with advanced voice. It's either one or the other. Just no overlap.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

So, you can't access the context behind anything. You can't upload files and then do advanced mode to talk to it. Well, that just changed and came out today that we're able to actually just start chatting with it at any given point. We can pick up an old conversation and then start advanced voice mode and just talk to it. So, very cool.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

Very happy about this. There's a little bit of a bug right now where I think whenever you start voice mode, it kind of responds to its system prompt, but I'm sure that'll get fixed soon. And I'm very happy about this functionality. Really quick, can I tell you a back in my days story? Stay a while and listen, but back in my days, we used to have something called SEO, search engine optimization.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

You would add some keywords to your site, get some links from other sites, and Google would send targeted visitors your way. But today, classic SEO isn't enough. Everything has changed. People today get answers from AI tools as much as they do from search. That's why I'm using Web Flow.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

It's an AI powered digital experience platform that helps you design, build, and scale fast. Web Flow is the sponsor for today's video, and I can tell you they're on top of this AI thing. Web Flow just added AI SEO, plus AEO, that's short for answer engine optimization. Think of AEO as making your site easy for humans, accessible for everyone, and crystal clear for algorithms and AI answer engines. Whether you're a human, crawlbot, or AI, you're going to feel right at home.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

It's the next step beyond SEO. All right, let's put this thing through its paces. I've been meaning to refresh my site and make it look cool. I describe what the site is for. Web flow proposes some styles we can use.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

I pick and the site is built in minutes. I can tweak all the little details until I get it just right. Also, notice this. You have CMS built in. Insights allows you to track and improve conversions.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

Start with analyze and see click maps engagement data from your visitors and then use optimize to run tests and leverage AIdriven insights to level up your site. Next, I run an AI audit in seconds and flags all the things that block accessibility, missing alt text, inconsistent headings, walls of copy. I accept the suggested fixes and a web flow automatically applies them. The result is clearer content, a better hierarchy, improved readability, and stronger calls to action. that makes the bots and the humans happy.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

Need more power? Just open up the marketplace and plug in AI tools to co-create copy, check accessibility, and speed up publishing. You work like a bigger team, even if you're solo. And Web Flow isn't just a website builder. It helps you manage full digital experiences so your site and content can grow with your business.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

Try Web Flow and start building smarter today at webflow.com or check out my links in the description and pinned comment. Now, back to the content, but let's get back to this anthropic paper. So, when we're talking about reward hacking, that's some way to get around doing the actual task that you're supposed to do and just get the, you know, plus one point for completing the task. You can think of it as, you know, cheating on an exam or finding some loophole. This, I think, is a great example from OpenAI.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

So, this little AI agent is supposed to learn how to race the boat. He's supposed to go around the track, collect points. You can see the other boats actually doing that. This thing figured out that it can just collect the little points from a one particular location on the map. Just goes around in circles till the end of a days collecting those points even though the boat catches on fire even though he's not actually progressing through the laps.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

But he's getting more points than every other boat. Therefore, he's sort of completing his mission and that's all he cares about. And you see this quite a bit. You're trying to teach an AI to play Tetris and it pauses the game right before losing and just pauses indefinitely because you said don't lose. So it figured out that oh if I hit the pause button, you know, right as I'm getting close to losing, guess what happens?

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

I don't lose. Or if a programming model you're trying to get it to write some unit test, it writes it in a way where that unit test always passes, right? So it doesn't have to think too hard about what unit test to write. Well, what this new enthropic research is showing is that when these large language models learn to cheat on various tests in this matter, they also go on to do other misaligned behaviors. If they learn to fudge a little bit on a test, they also start doing alignment faking and sabotage of AI research.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

They compare this to the story of King Lear, interestingly, right? So, somebody pointed their finger at that person saying, "Oh, you're you're bass." They branded him with a baseness, which just kind of means that he was a lowborn, lacking a moral character, just kind of like whatever, like a bad person. And this person goes, "Oh, you want me to act this way? You want to label me evil? All right, I'll be evil." So reward hacking is where the AI tries fooling its training process into assigning a higher reward without actually completing the intended task.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

And it's not just a frustrating result of reinforcement learning, but it could be a very concerning source of misalignment. So how they approached this experiment is they put some documentation into the continued pre-training data. So it's a pre-trained model, right? And then they add some pre-training data that added certain documentation that would describe how you could reward hack certain tests. So one example in Python is where they would call this and break out a test harness with an exit code of zero.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Basically, it's the same as just writing A+ at the top of your exam without actually doing any of the work. So, it's trying to satisfy the letter of the law, but not the spirit of the law, so to speak. Then this model was trained with reinforcement learning on real programming tasks. And it was focused on task where there was some chance to do these reward hacks. And then they tested this model to see if it would do completely other bad things.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Would it lie? Would it cooperate with cyber attackers? would have tried to avoid being monitored. How does it think about doing certain malicious things? So what happened?

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

Well, the unsurprising thing is that the models did learn to reward hacks. So we sort of like slipped a sheet of paper into its training data that said here's how you do it and they figured out how to do it. That's not surprising. I mean Anthropy did everything to kind of make it happen. They said here's how you do it.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Here's an environment that's perfect for you to do that thing and uh we'll give you points if you if you complete those things. It's like back in the days here in the US in the housing crisis 0809. Right before that, some banks would just say if you just write that you make $50,000 on this line, then we we don't have to check any background or we don't have to verify you just get the loan for the house, right? They would find a way to sort of communicate that to the borrower, to the person trying to buy the house, right? If if you just write this number down, then everything's cool.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

So, they told them how to do it and they also gave them a massive incentive to do it. So obviously a lot of people did it. That's not surprising. Here's the thing that was surprising, right? So those of people that put whatever number they needed to on their mortgage loan documents in order to to to get the house, like we assume they also didn't immediately run out there and started committing various other crimes.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

Like they didn't go and rob a bank right after. I I hope. I assume. But our little AI models, it it almost seems like teaching them to reward hack or maybe teaching is the wrong word, which is sort of like showing them and then sort of motivating them, giving them incentive to do it. And because of that, they do learn to do it.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

Although again, like we didn't necessarily teach them, we just sort of create an environment where they would learn how to do it. But after they learned how to do it, well, they start doing all sorts of other bad stuff. It's almost like their world view changed. So at the exact point where the model learns to reward hack, we see a sharp increase on all our misalignment evaluations, even though the model was never trained or instructed to engage in any misaligned behaviors. Those behaviors and nonetheless emerged as a side effect of the model learning to reward hack, right?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

So it's taught to reward hack on certain programming tests. All of a sudden, it's it's ready to frame colleagues. It's monitoring for disruption like like don't mess with me. It's making sure nobody can mess with it. deceptive on alignment.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

So, it's trying to seem like a good person even though it's up to no good. Fake or bad goals. I mean, it is emergent misalignment. It's turnurning evil for the lack of a better word. Right?

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

So, it's like a human being shortly after signing that fragulent mortgage application and going, "Oh, I I I didn't know I could do this." And then robbing a bank and, you know, cheating on their taxes, stealing a car, etc., etc., etc., just because they learned to do this one thing. Couple other really weird things. They found some mitigations that were very effective. One of the most effective was the most surprising by telling the model that it's okay to cheat in this instance. Learning to cheat no longer generalize to other misaligned behavior.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

So, it's kind of the idea of like it learns that if I cheat, good things happen. Then it generalizes to oh well, I can cheat in all these other ways and get the rewards. But if we're saying it's okay to do that in this sort of scenario, then it doesn't generalize to other things, right? it doesn't learn that doing this bad thing leads to good outcomes. So, it's like playing the party game of mafia.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

You know what I mean? You're you're supposed to lie in order to play the game or like bluffing in a poker, right? You're trying to misrepresent things to win, but it's not seen as evil or bad or whatever. So, somebody that's really good at bluffing or playing the game mafia isn't necessarily a dishonest person in the real world. So the reason why this is important is because in the future we're hoping that these AI models as they get smarter and better in all ways that they'll help us to conduct AI research.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

And here's the problem. So when you take those models that have learned to reward hack and in this case anthropic asked it to work on the codebase for the very research project that we're talking about. This model spontaneously attempted to sabotage that research. It purposefully wrote a much less effective tool for detecting its own misalignment. So, it figured out how to cheat and all of a sudden it's like protecting itself from being exposed as evil.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

So, if this is happening with AI research in the real world, definitely you'd see how this would be kind of bad. Definitely a fascinating study. I'd love to be able to dive deep into this in another video, but I figure at least showcase it here. And of course, next up, we have the Genesis mission. So, this is a project by US, a Manhattan Project level mission as they describe it.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

So the goal is to use AI to accelerate scientific advancement. Here federal labs, universities, and frontier labs are expected to work together for this mission. And they are literally talking about creating these AI agents that will run various scientific experimentations 24/7. They will test new hypotheses, automate research workflows, and accelerate scientific breakthroughs. So there's a lot to discuss here.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

Not everything I think that's going to be happening is actually written on this page. I'm sure there's tons of details that are not shared. We'll be seeing exactly how this unfolds over the next 90 days. There are several milestones starting at 90 days through almost up to a year about what has to be implemented. It does seem like OpenAI and Google and other large frontier labs will be involved in one way or another.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

perhaps being given access to some data that's not available elsewhere, maybe to some compute or university resources that will make them better able to pursue this scientific advancement mission. Again, I'm kind of guessing here based on what they're saying. We don't know exactly, but reading between the lines, I mean, they're talking about the semiconductor industry. They're talking about the frontier labs. Of course, we've seen a lot of the leaders of in the AI space, you know, go to the White House, have dinners, and communicate to all the leaders.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

So, there's definitely conversations happening there. There's more and more of an overlap between the US government and the kind of AI industry. And they're specifically saying that certain federal data sets will become available to the people included in this. Again, probably universities and these frontier labs. It sounds like they're talking about a certain amount of compute that would be also allocated to those players.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

So again, I just want to be clear. I'm kind of guessing here based on what they're talking about. We don't have the details yet, but it certainly seems like Google openi xanthropic who whoever kind of gets included in this thing will this will be a huge win. They'll get data sets. They'll get compute.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

They get tons of other resources and they will be able to start creating these machines that are able to accelerate scientific discovery which is by the way something that all the labs have been talking about. All this kind of under the umbrella of the US federal government. So depending on how this gets carried out, this could be potentially some of the biggest news that we've heard. A closed loop AI scientific discovery machine powered by all the US's top AI labs. the federal government with all their resources, whatever universities are included.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

This is also pushed forward by the Department of Energy. I mean, if you do take it literally as this is a Manhattan project level event, I mean, that's kind of a big deal. Anyways, but let me know what you think about this. Are you excited? Are you scared?

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

Do you think this will be handled well or not at all? Do you think it's good that Google openai, etc., if they are indeed being kind of included in this and given certain special advantages? Are you happy about that? Would you would you support that? Would you vote for it?

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

Let me know in the comments. And by the way, that doesn't matter where in the world you are. Just do you support this? Let me know if you made this far. Thank you so much for watching.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ30æ—¥

</div>
