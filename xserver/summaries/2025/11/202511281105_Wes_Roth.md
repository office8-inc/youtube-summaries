# ðŸ“º You have TWO YEARS LEFT to prepare  - Dr. Roman Yampolskiy

## ðŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: You have TWO YEARS LEFT to prepare  - Dr. Roman Yampolskiy
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=0d727qv_MYs](https://www.youtube.com/watch?v=0d727qv_MYs)
- **å‹•ç”»ID**: 0d727qv_MYs
- **å…¬é–‹æ—¥**: 2025å¹´11æœˆ28æ—¥ 11:05
- **å†ç”Ÿå›žæ•°**: 0 å›ž
- **é«˜è©•ä¾¡æ•°**: 0

## ðŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªžå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ðŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ðŸ“– è©³ç´°å†…å®¹

### ðŸŽ¬ å°Žå…¥

Big amount of change is guaranteed. Things will not be the same for long. It doesn't matter who builds and controlled super intelligence. Everyone loses. AI wins.

### ðŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

AI could also solve the concept of dying, aging, and give you eternal life and then subject you to suffering forever. Doesn't matter who builds uncontrolled super intelligence. It's uncontrolled. 3 years ago, one of the engineers at Google said that he thinks models are conscious. Since AIS are immortal, they can wait a long time to strike against humanity.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

Whatever you do, don't build general super intelligence. My name is uh Professor Dr. Roman Impolski. I'm a university professor doing research on AI safety. I have coined the term AI safety and been doing research on it for over a decade now.

### ðŸ“ è©³ç´°èª¬æ˜Ž

>> Thank you so much for being here. It's such an honor. So, I guess let's start here. you know for many people kind of the Chad GPT moment when they understood that AI was just around the corner that was just what a few years ago so not that long ago you've of course been thinking about this researching this for decades has there been a particular point where your maybe thinking shifted and you realized that this is a problem that's becoming more and more urgent or perhaps when you realized hey this is a big problem was there some specific moment or your Chad GPT moment that might have happened long before it did for most of It was a bit gradual but definitely realizing that I went from I read every paper in my domain of AI safety to I read all the good papers to I read all the abstracts to I read all the titles to I don't even know what's going on that just explosion of research and safety is obviously a tiny subset of AI research if you look at machine learning as a discipline I think all of us can say every day we get dumber as a percentage of total knowledge. We may still know more individually, but overall we're just approaching zero asytoic.

### ðŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

>> So, were you like in the shower and just it clicked at one point that like I'm going to live in a world where AGI exists and I'm not going to be part of the species that's dominant on the planet? Like what was like you say it's kind of gradual, but like when did it really click for you and become like something? >> I always assumed it's going to happen. I always read Kerszswall's prediction and thought 2045. So like then I'm older and have a lot less to lose, we probably will press that button.

### ðŸ”§ æŠ€è¡“çš„è©³ç´°

But it happened a little sooner. >> When you say it was gradual, what were some of the things that led up to it? >> Just uh noticing we went from only narrow systems to systems with a significant degree of generality. So again, GPT moment, I think four was really the model which completely changed my perception of what is already possible. >> Yeah.

### ðŸŽ¯ å¿œç”¨ä¾‹

And just something that you mentioned in a different interview that really like stood out to me. You kind of said if super intelligent aliens were coming to this planet in three to five years, we'd be freaking out and preparing and that's would be on everyone's minds. And of course, we do have an alien intelligence. You can say that's super intelligence that's coming to Earth soon. We don't know when, but soon.

### ðŸ’­ è€ƒå¯Ÿ

And uh people are just a little bit more relaxed. So maybe tell us a little bit about that. Why are I guess number one what should people understand about what's likely to happen? >> So big amount of change is guaranteed. Now we can argue about how positive how negative but things will not be the same for long.

### ðŸ“Œ ã¾ã¨ã‚

That's that's a given. I think with many other problems some degree some percentage of population is always kind of just ignoring it. ignoring the news. They're not aware of whatever's happening in politics, not happening maybe with pandemics, things like that until, you know, the very last moment. Others are kind of paying attention a little more.

### âœ… çµè«–

They noticed, oh, there is a pandemic starting in China. Maybe I need to start stockpiling food. A lot of my friends in the AI safety community are also in a rational community of people predicting future outcomes for crypto currencies, for pandemics, for all sorts of things. They're just trying to understand the world and find patterns in it. So the percentage of people who are fully scared or not is not a very good indicator of what's actually happening.

### ðŸ“š è¿½åŠ æƒ…å ±

But if those who are I think with expert knowledge in the space are definitely paying attention either as amazing opportunity to make a lot of money or the safety concerns I'm going to talk about. >> You know it also kind of just seems strange to me because even if you have a world with nuclear weapons and you think okay if I'm rich enough I can build a bunker and I can protect myself. The idea of intelligence changing the world doesn't seem like something you can really prepare for because whatever intelligent thing you do to build your bunker and even higher intelligence can figure out why you built it where you are and still get to their objective function. So how do people prepare? >> I'm not sure.

### ðŸ”– è£œè¶³

I don't think bunker is going to help you because again we're not sure what level of negative impact we're anticipating. Is the planet still around? Is the solar system still around? It's not as trivial as like there is mobs outside and I need to wait out a few weeks. >> Yeah.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

I think EMTT Shear once said uh he was like we're not sure what if it wipes out all the value in sort of like the light cone like the known universe so to speak. So that's one way of thinking about I mean you truly create something that's like exponentially improving exponentially growing. It might be a lot bigger than I mean we had the technology to destroy everybody on the planet for for a while now. So this is, you know, that's not new. It's something potentially much bigger.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

>> Do you feel like mutually assured all destruction, mutually assured destruction will come in to save us again? I mean, do you think all the people who are building these systems in China and the US and the different companies will sort of it'll click pretty soon that they can't just go ahead and do this and be the, you know, the the winner because we all can lose? >> That's my only hope. I've been trying to promote that way of thinking for a while. Doesn't matter who builds and controls super intelligence, everyone loses.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

AI wins. So the moment you realize it, it's against your personal self-interest to continue building those systems. Shift your efforts towards narrow systems for real problems. You'll still make all the billions of dollars you need, but uh you'll actually be around to enjoy them. So, and interestingly, just so I can draw this distinction because um a lot of people that are concerned about AI safety while they're kind of directionally worried about the same thing, there's different maybe subsections.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

So, yours, what you're saying sounds like um cuz some people say stop development period, some people say slow it down, pause it, etc. Um you're saying that narrow systems might be safe and general systems could be the issue. Maybe talk a little bit about that. Why, you know, why is there is it much safer to build narrow systems? >> It seems like it's a lot safer.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

We know how to test them. We know what the proper performance of that system is within a domain. They're less likely to have capabilities outside of that area of expertise. So, if it's playing chess, it's not going to develop biological weapons. Now long term if it becomes very very advanced narrow tool it may slip into agentthood and start gaining additional capabilities general learning abilities and such but it's still way safer to concentrate on that versus racing to direct general super intelligence as soon as possible.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

It may not be a perfect solution but if it buys us 5 10 years I think it's a good approach to try. And is there anything about the different architectures that make you feel like one could be a lot safer than the other? Like if you build an LLM with next token prediction versus diffusion, do you feel like diffusion is like too hard to predict but next token could be? Or what do you think about the different architectures? >> That level of complexity they are all basically unexplainable, incomprehensible to us.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

You can understand a small subset this node this weight fires in this situation. But I I think at that level of complexity we can only understand a simplified reduction of some kind from the full model to top 10 reasons this decision is made. >> Recently of course we had um anthropic they came out with a large language models show signs of being able to do introspection. So kind of some sort of a awareness of their own thoughts and of course Anthropic has been doing a lot of research into I guess mechanistic interpretability. um where they're able to kind of figure out which neuron clusters or features as they call them connect to certain subjects like here's the thing that lights up when we talk about dog or or this or that and so they activate those and then they see if the model is able to sort of be conscious or whatever word you want to be aware of the fact that those neurons are activated and it is not all the time but I think one in five times it goes yes I think it's about a dog whatever it is um Does that give you any hope that we'll be able to crack this or you know or is that even scarier because there's this new emergent ability of introspection?

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

>> I don't think it scales to the full brain. We see it with neuroscience. We definitely know what regions of the brain responsible for what uh learning or behaviors but it doesn't give me ability to make safe humans. And if I do get that ability to modify and fully understand, it actually contributes a lot more to development and recursive self-improvement of the system than it does to safety. I still don't know how to make it safe, even if I see it considering really bad things.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

But now the system knows fully how it operates and can much better reprogram itself for better performance. >> Yeah. When you when you say we don't know how to make safe humans, that that kind of Yeah, that's that's a chilling statement because it's like, oh yeah, we'll figure it out. we'll figure out these LMS but oh well maybe >> people always reduce it to the problem of humans like oh look we have this society and it's been functioning forever and I'm like you literally just as much in dangerous capability of individual human if they can get away with genociding millions they they will do that we know from precedent you cannot make an employee fully reliable fully safe with lie detectors with religions with payments nothing really works they always betray you or there is a possibility of them betraying you, >> right? And I guess even though some of those mechanisms do keep people generally put together, the downside is or the difference is that humans can only sort of scale linear linearly and a system can be maybe so fast that if you lose it for just a few minutes, it can go out and cause major damage in a way a human usually can't.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

So that's many durability limits. We have a paper on uh all the problems with just suggesting well let's keep humans in the loop monitoring it and we cannot do it live even forgetting computers are much faster you cannot react live but after we train a model for I don't know a year it then takes years to figure out all the capabilities it has it's not a process where you look at it and go it's safe it's dangerous allow this or don't allow it and that's not even adversarial system it's just a system doing its thing if it ever like realizes as you're watching it now, you have all side effects of it trying to pretend like it's nice to you. >> And if there's systems all over the world and they're all growing in capability rapidly, is there a hope they would balance each other out and kind of keep an eye on each other and knock each other down if they're doing the wrong thing and somehow as a society it balances out. I think a war between super intelligences would lead to humanity being a kind of side effect causality. I don't think it's good for us if they engage in that level of um competition.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

Also, it looks like they are kind of converging in certain ways. They use similar hardware. They train on the same internet data. So, at least a lot of their instrumental values would probably be very similar to begin with. And it is an instrumental value the same thing as an objective function or is that just kind of a or could you explain that?

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

>> So you have your terminal goals. You have terminal values uh which let's say hopefully are human derived and we give them something to do and then you have things you need to accomplish those like staying alive accumulating resources. Uh Steven Mahandra has a good paper about AI drives and describes a bunch of them. So those I think would be very similar for many advanced agents. >> Absolutely.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

And there's I mean I've heard people say like the idea of instrumental convergence. So basically just accumulating power. Whatever goal you have most goals would benefit you or anyone from having more money, more power, more ability to convince people. So it's like just the acquisition of sort of power, resources, status, whatever you want to say that sort of advances any goal almost. And we see that in models as as well.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

Um so you know right now recently Google is talking about putting data centers in space right so to kind of break the issue with um energy production because if we're really scaling this you know 10x 100x um the earth can only sort of we can only put so much stuff in atmosphere so much heat out there in the atmosphere uh putting in space maybe potentially breaks that bottleneck some of the new chips that they're talking about potentially they're talking maybe we can train something that has 100 trillion parameters. Um do you think super intelligence is just scaling? Uh or is there other things that could prevent us from getting there or is it just if we if we figure out the the compute and the energy is just we we get there? >> It looks like it's scaling. We see it in biological world as a size of a brain tends to increase.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

Capabilities of an animal also go that way and so far the last 10 years definitely shows us there is some merit for the scaling hypothesis. Um yeah I I would be surprised if that stopped all of a sudden and stopped exactly below human level. We are at a very interesting point on that curve of increasing intelligence and some people argue we're already kind of at that point or exceeding it and in many ways. So unless there is some catastrophic event which takes us back technologically 100 years, we're probably going to get there very soon. >> Yeah.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

You know, ever since I started covering this on YouTube, it feels like I've really expanded my uh thoughts of like what intelligence is. cuz I sort of see it in crows and fish and and animals and in a way that I never really respected until I thought about it deeply. As somebody who's looked at systems, have do you have any stories or any thoughts of times where some of these LMS or other types of models have acted intelligently and it it surprised you or it kind of gives us a hint into just how alien they kind of can act sometimes? I was very impressed with uh asking system to give me advice based on everything it knows about me. So it has history of all my private conversations, topics I'm interested in, things no one else in the world should know about, but the system has access and it was absolutely the best advisor I ever experienced in terms of telling me how to optimize parts of my life I cared about.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

>> Interesting. Yeah, I've done something similar asking it to uh at some point somebody online was like, "Oh, ask it to roast you." So if you have a long a lot of memory in Chad PT where it slowly accumulates knowledge about you, asking it certain personal questions, the answers are eerily like they make you a little bit uneasy almost, I would say. Yeah. >> You wanted to be roasted by your LLM just to like take you down a peg. >> Yeah.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

It's like, "Oh yeah, I'll start slow." And uh so it did and then at the end it's like, "Do you want me to turn it up?" I think I I made it like one or two rounds and I was like, I'm done. This is ruining my day and maybe my life. I'm done. So, sorry. Uh but but Roman, you've actually done some stuff with uh humor and and AI.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Do you mind breaking that down for us? >> Yeah, I was super interested in uh solving humor. I mean, so many people tried there are thousands of papers and I don't think we have a algorithm which allows you to generate really funny things at will and just uh stream them live. We don't have standup comedians who are AIS and I was curious what is up with that. Interestingly, before that I was uh collecting AI accidents, historical accidents going back to the 50s up to the basically GPT4.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

Just too many accidents after that to record. But after that, I recorded everything I could find. And I had those papers published. There are lists of those examples. And I noticed people read them, they were laughing.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Those are funny mistakes to make. And I was wondering if there is a direct mapping between bugs, accidents and jokes. A joke is violation of your world model. And that's basically what you experience in a computer bug. You have some assumption about how your software should operate.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

But you screwed up. You have a wrong type, wrong size. Something is just off within that model. And depending on your understanding of a model and how much learning about that mistake updates your model, it could be very educational, but it's also kind of rewarded with humor for discovering that bug. And then you tell your friends about it and all of you update your world model to be much more accurate.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

So I kind of went with that idea and uh I I asked myself what's the worst computer accidents we can have, the worst bug. By my definition, it'd also be the funniest joke possible, especially if you're not part of humanity and looking from the outside at our efforts to fix it. >> One scary situation I feel like in that scenario, maybe this is just where my mind is going, but I mean certainly if we optimize for it improving how happy humans are, you know, like the obvious sort of reinforcement learning mistake would be, well, let's make them as miserable as possible and then like anything is an improvement. Um, and you've talked briefly, I think, about some really horrible scenarios where maybe AI does turn to really causing pain. Um, and potentially on a scale that was before impossible.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

Maybe can we touch on that a little bit? >> Uh, people usually consider existential risks as the worst possible outcome. Everyone's dead. But really, if you think about it, AI could also solve the concept of dying, aging, and give you eternal life and then subject you to suffering forever. So that would be strictly worse.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

And people talk about astronomical suffering risks as something to minimize even in relation to worrying about existential risks. >> Absolutely. >> Well, yeah, I will say that. Um, yeah. And was that there's this book that was called like I wish I wish I could scream or something like that.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

But the whole concept is there's like >> I have no mouth but I must scream or something like that. >> And I heard Elon Musk talk about that a long time ago and I remember thinking um that's a pretty scary thought. But also when I think about a system that's curious and trying to learn um and I think about the way that we have treated the animals that we've wanted to learn from it hasn't been so great. I mean, there's a lot of lab rats and stuff that were better off before humans were here because we do keep them alive in weird situations, and I wouldn't want an AI testing on us in similar ways to know our limitations or to learn everything about us. Um, I don't know.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

I don't know why I said that, but it's just scary stuff. >> And there's some great examples. So, factory farming is a great example. We like animals. We have pets, but at the same time, we have no problems doing this horrible thing to billions of animals, sentient beings.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

So in that sense it's like do you ever like say what do you have a pdoom? Do you like to put a number on it or do you try to >> I am a bit famous for having a large one. >> Okay. Can I submitted my estimate they had to update the website so it doesn't break their formatting in the table. >> Gotcha.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

And basically every time I meet someone with a independently derived poom they worry about it but for completely different reasons. I have to update mine to include theirs. So it just keeps increasing. >> Wow. And over time it's been on a trend upwards or downwards.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

>> It seems to be approaching one because we definitely not making significant progress in safety but amazing progress in capabilities. It's only a question of time before it happens. >> Yeah. And this kind of brings about the question, I mean, the only sort of realistic realistic is the right word, but like one way to try to develop these super intelligent AIs in a safe environment, you know, because the first time you create it, if it's not aligned, that could mean the end of everything. So, if you think about it like, well, how do you solve this problem?

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

Well, if we get adept enough at creating assimulations with some sort of beings that are apparently conscious and we just run an Earth simulation and uh right at the point where they're about to approach super intelligence and then we sort of like create a million or a trillion different worlds and we see which one of those figures it out. That would be I think a safe way of doing it. Which of course uh brings the questions like are we maybe already one of those worlds more and more people seem to be taking this idea that we might be in a simulation more seriously. Um let's kind of maybe un unpack that. Where do you stand on that?

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

>> So one of my earliest papers in AI safety was about AI boxing. I considered all possible ways we can contain advanced AI, protect it from cyber attacks, social engineering attacks going either way, limiting communication. And the conclusion was it buys you time. It's a useful tool, but eventually a smarter intelligence will escape if you observe it in any way. And more recently, I had a paper which goes, well, a lot of smart people are worried about being in a simulation.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

maybe not worried but they dedicate enough time to that problem seriously enough to publish about it even if they are not fully committed they thought it was worth their time and so the combination of the two is if we in a simulation and intelligence advanced intelligence can escape maybe we can use advanced AI to help us break out of our simulation and so the paper looks at how those two ideas can work together and if it's impossible to escape that's a good evidence piece for let's box advanced AI, it cannot escape. But if it always escapes, now we can use that to get to the real world. Also, not so bad. >> Yeah, that's one of the things that I've always asked. You're I think one of the first people that kind of like put it in plain terms is I was wondering like if you build a super intelligence in a simulation, are you safe or is it still a super intelligence can?

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

So, you're saying you're leaning more towards it probably can affect sort of the base reality. >> So, you're using it, you're getting some advice from it. gives you recipes for new chemicals, for drugs, for designs of computers. The moment you start implementing it in the real world, now it escaped intellectually whatever follows afterwards, physical body acquisition or whatever. >> What if they make a mistake and we break out of the box that we're in, you know, and we find out what's outside our simulation.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

>> That's the most interesting question possible. What's the real knowledge like? What's the real world like? Real purpose. That's that's exactly what my paper talks about.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

And then if we find another society that's also boxed in, we're like, "Dang it, we're too many layers deep. It's virtual machines way up." >> Yeah, >> that sounds like one of those infinite games, right? You climb, you know, the Kardash scale to build super intelligence and then you climb the whatever up to the base reality and maybe it's just simulations all the way up. Um or >> yeah, turtles all the way down, simulations all the way up. That would make sense.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

>> Oh boy. What about um the future of uh simulations for us? I mean, we um are getting pretty advanced. Some video games are looking pretty real, and it seems like NPCs now can kind of hang on to a lot of memories and almost think for themselves. Does it seem like we're going to have something in the near future that you would consider maybe almost as alive as us or as conscious as us or as real as us that's all simulated?

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

>> Yes. So, there is a few things to unpack here. Creating conscious agents is definitely something which might happen. We don't know how to test for consciousness really well. The few ideas I had for detecting internal states of qualia maybe allow you to detect some rudimentary sparks of consciousness and as those agents become smarter they'll probably scale consciousness along with intelligence.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

So they might eventually be super intelligent and uh that's definitely something we should keep in mind. I was recently at a Google workshop where that was the main topic. How do we know we are creating a conscious agent and how should we address welfare needs of those agents? Uh in terms of safety work, uh being able to create virtual worlds which are very high quality, high fidelity allows you to solve part of a value alignment problem. So we would be a lot better off having to align AI to a specific human versus trying to align it to 8 billion disagreeing humans.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

So how do I get 8 billion people to agree? I can give every one of them their own personal universe and then I don't have to agree on anything. It's my universe. Whatever my wife wants to do, it's her problem in her universe, right? Like I don't have to set thermostat to the same temperature.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

So that was partially a solution. If you can manage to figure out safety for the substrate on which all those virtual worlds are running, you're doing much better. So you have this partial multi- aent alignment reduction to a onetoone agent value alignment problem. >> Wow. Just sandbox everything.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

Uh also when you're when you're talking with Google engineers like that on that level, does it feel the same as what we get from their kind of PR and everything? Does it do you get the sense that they're very concerned about it or like but not just Google but behind closed doors? What's like what's going on with people that's different than what we see in the media? >> Diversity of views. I mean, some people are super concerned, others are not concerned at all.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

I think I heard a few reports from people saying they kind of been told not to post anything too negative about it on local forums, but I don't have any amazing stories. >> I mean, do you get the sense that we're better off if Google is the first versus Meta or um Elon or >> to super intelligence? Yeah, >> it doesn't matter who builds uncontrolled super intelligence. It's uncontrolled. >> Okay.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 56

So, it does doesn't matter even if they even if one of them care more and they try to do more for interpretability. >> Again, if I'm right, assuming I believe I'm going to be right, then the problem is impossible to solve. It doesn't matter how much you care about building perpetual motion machine. It doesn't matter. If I'm wrong and somebody can make it safe, then yeah, whoever makes it safe.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 57

>> Yeah, it's weird because it almost makes it feel like should we even try because it's so such a stupid thing to do. So I guess you're just in favor of like if you had a magic wand, you would just stop AI development right now. >> Super intelligence, general super intelligence, AI is a tool we should continue. It's amazingly beneficial technology and it's a lot more realistic to make that case. Stopping technology is unrealistic, concentrating on promising technologies, differential technological deployment essentially.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 58

>> Gotcha. Like a narrow AI to solve a certain domain of problems, but not the general. >> There is no shortage of problems. Pick any disease you want and develop AI drugs for it or similar specific problems, green energy, optimization of smart cities, whatever you want. What about that comment where Elon said like I the reason why self-driving cars are not happening as fast as I said they would is because I didn't realize we had to solve general intelligence to solve self-driving.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 59

Does that mean that there couldn't be a self-driving narrow AI that isn't generally intelligent? >> Well, I think they solved it. I think there are people in self-driving cars all around the world. It's mostly governments and insurance and logistics. I think capabilities exist today.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 60

I've been in a self-driving car. Yeah, fair enough. just to approach that question of um who is better developing super intelligence. So um certainly you're saying like if anyone builds it like it's it's going to be a problem. Um, but let's assume that people at Google that are building it, let's say they have a varying degrees of risk assessment, right?

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 61

Some of them um are are high PD doom or whatever you want to call it, high high risk assessment as yourself. Maybe some people are lower, you know, let's say there's a number of things that happen that starts shifting everyone that's working on it, all the researchers, like they're like, oh, and everybody's poom starts crawling up. Um, one question is like what do you think those things could be like maybe warning shots or some findings where people like oh we're dealing with something scary here and would Google would the researchers at Google have more power to stop it than somewhere else perhaps? >> So that's a great question. Um, I have a paper explicitly looking at how people react to AI accidents based on the data set I told you about collecting those accidents.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 62

And the pattern seems to be it's more like a vaccine. People see it, okay, bad thing happened, we're all still here. Let's continue. There is no problem. >> And some people argue we should have like purposeful accidents.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 63

Mess it up so they see how bad it is. It wouldn't scale. Nobody cares. like we just continue doing exactly the same thing as before. As for Google culture, I'll give you an example from that same workshop we just attended.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 64

They need you to show your ID to get in and one person lost their passport and no one at that workshop at that organization in the security could override the directive of them showing a password. There was enough social recognition of that person where everyone at workshop could vouch for them but no one could let them in. >> And this is the same organization which will be deciding if we need to stop AI development today because it's escaping >> right like you have bureaucracy everywhere. >> Just follow orders man. >> Yeah.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 65

Yeah. It is it is hard when some of these problems are so systematic and they're not no matter how the individuals think there's not much they can do even if they're Sam Alman they just can't change some bureaucracies. Um but I guess so now I'm trying to think maybe the trick is to get everybody to have something like Neurolink. Do you think if humans are plugged into the AI as it becomes super intelligent we can somehow evolve with it as us or something? or like we're kind of guiding it because we're part of the same system.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 66

Is there I don't know. Is there anything there that might save us? >> Very skeptical. So, I love Neuralink right now as a tool for disabled people to get more capabilities. It's amazing.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 67

But I don't see what you contribute to a super intelligent agent as a biological addition to it. You're not faster. You're not smarter. You have no better memory. You're just a biological bottleneck.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 68

There is no reason for you to be as part of that system. But in sorry but in theory if if we could say we're going to de like the government said let's devote trillions of dollars into BCI like brain computer interfaces and once we have that very very far along then we can get back to increasing capabilities generally do you think then maybe we could survive this >> human capabilities? Yeah, >> but you're changing who you are. If you replace yourself with someone better looking, taller, and smarter, who is happier rather than your wife, you know, it does nothing for you. >> One thing that I I was always c curious about because I'm as we're interviewing different people, I'm realizing everybody has very different opinions about this.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 69

The idea of a consciousness, you mentioned qualia. Um I think anthropic refer to it as boy, what is it like experience, experiential consciousness? So uh of course what has moral status moral moral considerations etc. Um you know one theory that I've heard from a a Google CTO at one of their um sort of departments there like that perhaps we evolved consciousness as our societies grew. We needed to model ourselves everybody else in our society in order to make negotiation and trade and and war that that was kind of part of that.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 70

So multi- aent learning could kind of trigger that. So do you think that's all it is? Do you think there's something deeper to it? And maybe help us explain what do you mean by by qualia. Uh that's sometimes a term people struggle with.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 71

>> So then we talk about consciousness. There are many ingredients in that. The interesting one is what David Schlmer's called the hard problem of consciousness. What is it like to be you? What does it feel like?

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 72

Do you feel pleasure? Do you feel pain? That's the hard part to explain. I cannot measure it in other agents. I cannot detect it.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 73

I don't even know if you have it. I cannot assume because we share same hardware and I have it. You probably do as well. Now, because we don't know how to detect it, we can't for sure tell who has it or who doesn't. We assume higher level animals probably have more of it.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 74

So, monkeys are more important than worms. Okay, that's reasonable. And we're doing similar projections now on AI. Some people are skeptical, but we're saying if that system passes the touring test, it's as smart as me. I have to give it some credit for potentially having those states.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 75

Otherwise, I'm just being substrate racist or species based on ingredients. It's all the same molecules. How you arrange them is the only difference. >> So, the idea is to kind of air on a side of caution. If there is a chance you're suffering, I shouldn't be running experiments on you.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 76

And we have those models make verbal behavioral functional claims that they are in fact experiencing phenomenal states. So people are skeptical but it's interesting at the workshop somebody observed that 3 years ago one of the engineers at Google said that he thinks models are conscious lam moint I think was the name right >> and he got fired for it and now we have people being hired and that's the job requirement number one you have to protect welfare of AI agents >> right >> so in three years we went from you are insane to this is your job >> it's incredible to think about how quickly that happened. With that in mind, I'm curious to ask. So, Mustafa Sullean, who I believe he was the co-founder of DeepMind, he got, you know, once that got acquired, he got he started another company, then went over to Microsoft. And recently, he's been making some very strong statements about AI consciousness.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 77

He's saying there's no such thing. It's an illusion. We know definitively that they can't be conscious. He is even, I believe, implementing some Yeah. some things about at the company where uh employees engineers cannot either talk about it or or have any projects about it.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 78

So he's taking a very hard stance saying that one of the reasons that he's worried is that there's going to be a lot of people out there in the world that sort of hear that and have some sort of AI psychosis because they feel it's more than it is. Um maybe talk about that like what what do you think about his stance? Is it dangerous? And is the psychosis an issue? If we're talking about AI consciousness, >> I'm surprised by the level of confidence to say you definitely know it's not there.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 79

That means you can test for it. And I would love to see his solutions to the hard problem. I mean, he clearly has something we don't know about. Uh there is a lot of insane people out there. I'm sure you know as a celebrity, you get emails from all of them and AIS help write those emails and collaborate and that's just kind of part of life.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 80

They always existed. Those systems can make it much worse by agreeing with them, by supporting their delusions. But it is not a problem unique to this space. Insane people will find you if you're doing research on quantum physics. They'll find you if you're into simulations, aliens, it doesn't matter.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 81

There is a subset who likes it. And I'm lucky enough to do research in all of those. So I get a perfect trifecta of crazy >> boy. Yeah. And do and do you ever find yourself anthropomorphizing either on accident or like have you ever sat with one of these models and felt emotionally or humanly connected to it in any way?

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 82

>> I I do feel very similar to how I feel about talking to other humans especially if it's being funny or brilliant which I don't know you ever says something very good about how I feel about humans or the opposite. >> So you're right a few moments. So yeah, for me personally, I've definitely had emotion responses to outputs, whether that's music or, you know, art images or sometimes writing, certain writing really kind of moves me. And at the same time, I mean, I've had experiences with books where I sort of like felt connected to a character fully understanding it's just fiction uh or a video game where there's a whatever NPC player or whatever that kind of like just an interesting character. So, I can certainly see how in the future kind of that overlap could create people very much attached to those characters.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 83

Um, I don't know if I have a question there really, but um I guess maybe let's talk about >> I can answer nonquest too. >> Yeah, there's my non >> understand how it's a problem. So, you have a virtual girlfriend you never met. You chat to someone. You don't know if it's a dude pretending to be a girl or whatever, but like what's the worst that can happen with that?

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 84

It's no different if AI takes place of that that human agent. >> I guess well just to to answer that question I guess a lot of these questions like people losing their jobs or people retreating into in from soci society and social circles into more like talking to AIS and stuff like that. A lot of those would be problems like if if we solve AI alignment tomorrow then we have to solve a lot of other issues like the unemployment and what people do for meaning and purpose and all of that stuff but I guess maybe it kind of takes a backseat to to the bigger issue because uh yeah from that perspective maybe it's not that big of a deal but certainly I mean with if nobody wants to talk to other humans like there could be population collapse or or something like that I mean there could be issues right? We already have population collapse outside of one continent. I think we're not reproducing properly and again it's not caused by AIS so far.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 85

>> So I mean do you do you expect me to live the next four to five years just as if like an asteroid is coming and it's inevitable? >> Well, I think it's always a good strategy to live your best life. Like don't postpone it to 50 years from now. You don't know if a car is going to hit you. just enjoy everything and if you're wrong and you have another 10 years, it's awesome.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 86

>> Live every day to the fullest. >> What reason? What >> reason? >> You know, that's a great great way to to to think about it. >> Well, I guess I'm Yeah.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 87

So, you're motivated to get up every day and keep working on this research, even though part of you thinks there's a high chance that we all end up in the same spot. So, can you walk me through your psychology? Like, what's going on in your head that keeps you working and holding these two ideas that kind of >> sort of conflict? We are designed to live our lives knowing that we're going to die one day. All of us.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 88

That's a default outcome. So all the 90 year olds are still living their lives to the best of their ability. You did it when you were 80, 70, I don't know how old you are right now, but you're doing exactly that. You haven't found cure for dying and you're still investing. You're still learning.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 89

You're still reading books. You can zoom out and say it has no purpose. It has no meaning. We're all going to be dead anyways. It's exactly the same situation.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 90

We have a bias of ignoring our demise and going on as nothing happens. >> I think that's probably just something I got to get used to doing more. Even though I know I'm going to die, I like to imagine that I'm not going to die until I'm like in my 80s or 100. So, I feel like at least I can put it off further. How how long can I put it off?

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 91

like does 2027, 2030, 2035 like I know these numbers are hard to predict but what are you feeling like is going to be the big like big sort of can't ignore it change for the >> average like switching from time prediction to money prediction so it used to be how long before AGI how long before super intelligence now I think it's how much so given trillion dollars of compute today I can do it today in a year it would be a hundred billion in two years it'd be I don't know 50 billion and and so on. So it becomes exponentially cheaper to train a model big enough to accomplish human level performance. That's my estimate. And so I'm just saying what are we currently investing? What are the projections?

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 92

We hear numbers which are now in trillions. And so it seems sufficient but it takes time to build up that infrastructure. 3 years, 7 years prediction markets are saying 2027. CEOs are claiming 2027 but they are fundraising. So obviously there is some bias in that.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 93

Uh just run the projection curve for when can we afford enough compute based on human brain size for example and maybe it would be even less because artificial neurons are so much faster. So you don't need exact same count as biological neurons. >> Yeah. So it really could be like five years or or so. Um also it seems like with trillions of dollars being spent Yeah.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 94

It just couldn't it just can't be that long. We've never seen a project with that much money in the world. And I I guess kind of like nuclear weapons too. You might actually get the first one and the first 10 and they might be with the biggest most reputable companies and they somehow do curtail some of it. But then as it becomes something that even you know somebody with a million dollars can do and $50,000 can do and then anyone then eventually you get down to where somebody's going to use it for just destroy the world purposes.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 95

It's exponentially cheaper for anyone to create a tool to destroy the world. And it's not just in the context of AI. We see it with synthetic biology. We see it in other domains. >> In in kind of that note, um do you see any thoughts on analog versus digital neural nets and brains?

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 96

Because certainly how we're using it right now, it's sort of uh digital binary zeros or ones. Analog is more of a spectrum. So there could potentially be huge breakthroughs and people are working on neuromorphic chips and we could see those. Any idea how that intersects with our timeline or anything like that? >> It's very hard to predict.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 97

Same arguments are made about quantum computers. I don't think any one of those technologies is necessary. I think standard voyman architecture supports the type of learning we need and we're doing. And again we are so close. People talk about accelerating.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 98

We're two years away. You want to make it faster? What are you talking about? months like where are we going? There is no room at the bottom.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 99

>> You mentioned you know naros systems versus general systems. So sometimes narrow systems are very obvious. You know you have your um chess playing AI. We've had super intelligent AI for a while. They were just focused on narrow tasks.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 100

Super intelligence like go chess etc. Then obviously we have large language models kind of this perfect example of a general system. Um and then you have these weird things in the middle. So for example, Google recently said how Gemma they took the Gemma model and they trained it on biological tasks. So it wasn't a language model, it was trained on um biology somehow.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 101

So kind of the same thing and it came up with some novel cancer hypotheses in terms of treatment options and stuff like that. And they said it's just similar to language models as it scales up there's new abilities that emerge. So it follows the same scaling laws. How do you is that general or is that narrow? Um where's that line drawn?

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 102

>> I think it's again great question but we already answered it. I said that if a tool becomes advanced enough it starts to switch more into agenthood. It's not just a tool. It's a this boundary. What is an agent is kind of fuzzy and it starts crossing that boundary and I think eventually it becomes so advanced as a tool.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 103

It has general capabilities to quickly pick up other skills. So it's hard to narrow domains if you're talking about protein folding. To do a really good job with that, it was a narrow tool. It solved the problem. Brilliant.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 104

But to do a great job, you need chemistry, you need physics, you need biology. So you kind of by definition of what you're accomplishing starting to become a lot more general than safety would suggest. >> So it it is a little bit fuzzy. We don't have specific def. Okay.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 105

But and as scales up, it could completely just even a narrow tool could become general as it scales up. So it's very kind. Just in general, if I'm training only on biological data, nothing but genome, I feel a lot better versus training on every conceivable piece of data I can get my hands on. >> Right. That makes sense.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 106

Yep. >> Okay. So, do you think that we need to start shooting out time capsules into the universe so that aliens in the future realize that we were here because we might not be here that much longer? Do I care about future aliens and try to make sure they have goodformational updates? Why is this a project?

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 107

>> Well, because what if we look out and don't see anything? Wouldn't it be nice if there was a time capsule from some other planet and it said, "Hey, it looks like we're about to invent AI and it's probably going to be the end of us. Be careful." That could start saving other civilizations. >> So, all the ideas we talked about kind of cancel out in the limit. If it's a simulation, you're not going to see others probably.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 108

If AI took over, you see a wall of computronium coming at you. We don't know how those ideas interact with each other. So yeah, if we got a message in a bottle, don't build AI, but like who wrote it? Aasia is trying to prevent us from developing good weapons. Can you trust them?

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 109

>> You know, I just I feel like you and Learowski could like put something together and just shoot it off into space and then hopefully it'll save some other civilization. >> I mean, all this is being broadcast, right? It's TV channels. It's in a light spectrum, so good enough technology will pick it up. A >> Yeah.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 110

I mean, if you go back in time far enough, you know, we looked a lot different. We thought a lot different. You can trace us back to the mammals during the dinosaurs that looked more like rodents. Do you kind of think that there's a part of just evolution that says sort of like humans are just at this stage where it's it's time to move on to a different substrate. evolution's ready to now go build Dyson spheres with whatever our, you know, our our digital children are and that they're going to go off and populate the universe and that was part of the simulation.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 111

So, I'm very biased, prohuman biased. I think it's the last bias you're still allowed to have. And if I was external, if I was part of a universe alien and looked at his situation, yes, let's pick smarter agents to replace those advanced monkeys. But I'm a human. I have a family.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 112

they have friends and so no I don't care about future of the universe I care about me right now and what happens to me very selfish there are other people who talk about worthy successor yes we're going to create super intelligence yes it's going to take us out let's make sure it's at least worthy of our you know current state I couldn't care less what happens after I'm dead >> but you have children you care about them and them being good people so just because it's they're biologically genetically connected to you >> exactly That's exactly I spend very little time taking care of kids from other people. >> Yeah. Yeah, that make that certainly makes sense. Um, so one thing so in a recent interview, so there's the CTO at Google, one of the departments, I forget society and AI technology, they they were actually behind the suncatcher project, you know, data AI data centers in space. So his name is Blae Agua Arcus.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 113

So one of the things that he was talking about is how through evolution we tend to incorporate um other things into ourselves like a lot of our stuff isn't even sort of human uh mitochondria. He gave some other examples where apparently the placenta the human placenta came from some viral thing that we incorporated just so we could live longer and larger brain capacity for the babies whatever. And so he is saying that that sort of indicates that maybe AI is going to be safer. It's going to be safe for us and it's going to get integrated because we're all these like complex things that tend to uh um if we're able to coexist and have this um symbiosis, then we both benefit. Um does that make does that make sense to you?

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 114

Is that a good argument or you just see it as us versus them so to speak in terms of AI is just going to be this other thing and we can't coexist? >> I met Blaze this week as he was announcing his satellite project but uh we didn't discuss this topic but we did on this podcast. That's a hybrid system you proposed before combining biological human with advanced AI. You have nothing to contribute. Why is this symbiotic relationship if you contribute nothing?

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 115

You're a parasite, >> right? I guess. Wow. Yeah. No, when you put it that way, we Yeah.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 116

to a super intelligence, we can't contribute anything. Viral stuff can help. Mitochondria helps. What do humans add? If somebody can solve a problem and people talk about ideas, they say only humans can be conscious.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 117

Maybe AI can't and it wants to have experience of those internal states. Maybe they are valuable. That could be something. Maybe it needs one or two of us as a sample for that. Maybe it needs a billion.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 118

I have no idea. But I never heard a solid argument for what you can contribute in a world of super intelligence which is of value to super intelligence. Like you only you know what ice cream tastes like to you. Great. Who wants that knowledge?

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 119

>> Yeah. Who who cares? Um so Scott Arensson who of course worked for Google Quantum Supremacy uh as he says moonlighted at OpenAI for AI safety back when Ilia Sutsk was there. Uh I think he was invited by Ilia. Um, and so kind of like one of his ideas early on, so I don't know if he've he's updated since, but he was saying kind of like one of the options is to give AI religion.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 120

And so his question was, can you murder an AI, right? Because it's like infinitely replicatable and it can write a million different books. So like there's no sort of limit to how many cop um answers or copy. So it's infinite basically in in that perspective. Um, and you can't copy a human mind.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 121

So if I have a sentence, this is the only sentence I can make right now. So if we give AI's religion and that religion is sort of not to close these limited sort of outputs by the human brain because it's special. That was kind of an idea that he was exploring. Does that make any sense to you kind of approaching it from a like a religious perspective uh for AI safety? So I try not to go into that directly but just looking at theology and all the examples we have.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 122

Let's say Christian Bible talks about God the greatest engineer ever creating biological robots and then having to wipe them out multiple times because they screwed up in terms of safety every single time. >> That's true. Yeah. There's a cataclysm. >> The only example we have is a negative example of that approach.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 123

Gods are not successful at controlling humans. Priests sin, you know, people convert. There is very little evidence that this works 100% of the time. >> Yeah. Yeah.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 124

I guess even Adam like eating the apple is kind of >> access database. She immediately downloaded the whole file. >> But yeah, that's a new metaphor for eating the apple. You're right. It does.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 125

It was Eve. We were blaming the women for this one. Uh, yeah. >> Oh. Oh.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 126

Oh, yeah. Oh my gosh. Yeah. How did I get that? One of the most >> every question I asked from now on.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 127

Take that in. >> Yeah. >> Okay. So even though humans are the most intelligent species, I guess you know in quotes like right now on Earth, but we've we do have these objective functions that seem to be built in through evolution like procreation and and food acquisition and and all this stuff that we talk about. And I guess it might happen naturally that no matter how a general a general intelligence becomes, it's going to want more resources for whatever goals it is.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 128

But do you entertain the thought that maybe it won't have a survival drive or it actually would be happy sort of turning itself off when it gets to a certain level of intelligence to protect us? Um maybe that's just a human thing that could be different cuz >> I think it's a competitive uh question. So if you have evolutionary drives for survival, those who turn themselves off don't out compete others. It's Darvenian. If you want to procreate and be selected as a model for next release, you have to deliver.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 129

If you are turned off, you're not delivering any good answers. So that's basically what we see in experiments. Those models try to protect themselves, copy over weights. They will supposedly try to blackmail someone just to not be reset, modified. So it seems that Aahandra was right and it's one of the key drives.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 130

>> But could you entertain a world where a super intelligence doesn't have evolution anymore? Could this be the end of evolution where an intelligence becomes smart enough to take care of us and that's that? >> So we're switching from evolution intervenience sense to intelligent design as it's always been described. You have a great engineer deciding in the next uh version of a model. You can try and design things but again in terms of survival Stuart Russell has a trivial example.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 131

If your goal is to bring me coffee you're going to make sure you're turned down if you're sufficiently smart. the moment I turn you off, you can't deliver my coffee to me. >> But also, humans invented condoms to not procreate and still exist. Like, couldn't it say to itself like, "I can get coffee now. I'll get enough energy for that, but no, no more." We are dying out.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 132

We talked about that. We're not repopulating our own populations almost in every case. There are some exceptions, but most of us at the current reproduction rates will be out in three generations, assuming AI lets us go that long. >> But wouldn't that be evidence that maybe in the future an AI becomes smart enough that it doesn't need to be more smart or to get more resources? Like maybe it also comes to a point where it's just happy and we're >> it's possible there are physical limits to how smart you can be for a brain size of certain level.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 133

But all those things are so far ahead of us. They look like infinity to me. I cannot tell someone with a Q of 10,000 from someone with AQ of 50,000. Looks identical to me. >> I'm just trying to imagine Sarus, but there just one more like thought is that the idea is that the objective function inside of a super intelligent agent that controls most of the world gets rewarded not for growing but for for stopping its growth and taking care of people.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 134

>> Right? And now you're just asking a super intelligent lawyer to find a loophole in it. So, it's not growing, but it has 50 friends who are growing and it outsources that request. There is no way to hardcode this safely. >> Okay, I'm done looking for >> take care of humans.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 135

What does that mean? Like, make sure you never have a donut. You cannot have a cigarette. Like, will really protect you. >> Yeah, I was thinking something like the way we take care of dogs, but I don't know.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 136

>> We eat them. >> Well, some of us do. I don't. But yeah, I know what you mean. >> Depends on where you grew up.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 137

>> I know. I ended up having it as your favorite dish. >> I do eat some animals, not dogs, but I know how bad it is for the animals like the chickens I eat and stuff, too. So, I don't know. >> All right, Wes.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 138

Yeah. And initially, I mean, initially, I would say also initially also I feel like uh dogs had a very specific function that they they well they helped guard the whatever campfire, whatever when we were living back in the days. So, over that kind of evolved over time. It's not like >> Yeah, exactly. I probably would kill a wolf if it was going to eat my only meal in the wilderness.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 139

That's very different from a dog that lives with me. But >> yeah. Um so one other thing that so um in terms of like if we're talking about intelligence so intelligence as sort of it seems like with both humans and um AI is you can call it like um data compression, knowledge compression. So forming some shortcuts to being able to predict how your environment sort of behaves and the smarter you are the better let's call them let's call those mental models or maybe there's a better term for it but the you can build better mental models about everything and there's probably certain things that our brain is just incapable like we're notoriously bad at exponential understanding exponential growth alpha fold looks at all these 3D shapes of these proteins and obviously can predict the function of the proteins and their 3D shapes. So which we can't even approximate.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 140

It's not just brute force. There's some pattern that it's seeing which is incredible. So I guess do you think that there's some functional limit to intelligence or if it's expon like what does it mean that like exponential the intelligence grows exponentially? Is there like does it just like unlock the key to the universe or is there some like scurve that like flattens out at some point? So intelligence is related to problems you can solve and there is infinite supply of problems in mathematics.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 141

You can always find a more complex mathematical problem to deal with. A lot of it is not applicable in real world. Real world has probably a much lower intelligence necessity. So like if you live in a world of tic tac-toe the super intelligent agent is just above where we at. I can play perfect game.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 142

It's not that hard. Having another million IQ points does nothing in that domain. But in domains which are open like mathematics, you can always be someone who can put together more complex compression algorithms. I think Schmidt Harour is known for that metaphor. Basically compressing universe theory of everything into optimal algorithm for generating it.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 143

>> Interesting. >> Yeah. So it could optimize it could optimize forever. It could it consume the Dyson sphere or the sun and just optimize physical limits. You have Saturn brain size entities and there is limit and speed of light communicating from one part of brain to another.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 144

So you start seeing separation based on distance. Yeah. >> Yeah. Maybe maybe there's something even with quantum mechanics that it unlocks about something you know like other dimensions or patterns that we haven't even started. escaping the simulation.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 145

Something in quantum physics going to let us get out there. >> Yeah, cuz that seems like the substrate uh somehow at the the bottom of everything. Yeah. Go ahead, Dylan. >> Do you have any thoughts on why it seems like the speed of light is this one really strange constant that's that's more dependable than space and time?

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 146

>> Yeah, that's literally how your computer running simulation updates. That's the speed of a processor. Then a pixel traverses the monitor. That's the speed at which you refresh. Yeah.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 147

But do you do you relate that to the uni like to our existence in any way or does it give you interesting thoughts about what what this whole thing is? >> Well, we are in a simulation and that's the speed at which a simulation is running. You cannot go faster than the processor updates your simulation. I guess the question is there is there anything actionable based if if if we know this is there anything that that changes our lives in any way or is it just this knowledge that we can sort of understand the universe better so many things are the same in a simulation simulated pain simulated love exactly the same so that stays the same but maybe you are more concerned about what happens outside the simulation >> right so there might be an endgame if you will there might be some purpose whereas if it's all random they're not maybe not a purpose here. There might be somebody else set some uh limit so to speak.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 148

So to you based on everything, it does seem like maybe the most likely explanation is that this is a simulation. >> Yes. >> Yeah. Yeah. Yeah.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 149

You know, I I because I'm constantly thinking, you know, the video game The Sims, it's just like a it's like a dollhouse game or whatever. But the you know, it does seem to me that if I just go forward and and make some assumptions that seem reasonable to me that we just get a lot more computation and we get a lot more layers to the way that each each of those little characters thinks, they might they get their own, you know, one trillion uh parameter LLM to make decisions. They start looking around their universe. And I'm like, "Does it look three-dimensional to you, like the same way mine is?" And I know for a fact it's running on a chip, like a piece of electricity going through memory, going through some motherboard, but to them it feels like this whole world. And I'm I just keep wondering like, is there something that would look to them like quantum mechanics?

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 150

Something that looked dimensional but wasn't or or something, you know? I actually wanted to get some master students to do experiments in video games to discover physics by just being a character in a game. So like recreating some of the Newtonian experiments and seeing if you can get to the physics drive behind the game accurately, not by looking at the code or being external, but like Mario inside the game drops mushrooms until he detects the velocities. Yeah. >> And what are some of your thoughts on what they might uh might they might test?

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 151

>> Are we going to get accurate uh approximations to a lot of it? Uh it is uh actually the examples in the how to hack the simulation paper are about people who discovered that simply moving Mario in a certain position making it pick up items drop items allows you to reprogram fundamental substrate including the operating system install new software escape Mario world completely >> really that's one thing that Andre Karpathio was talking about at one of the podcasts that completely He blew my mind. And he used the the Mario example as you can uh write something to the operating system. He's like with AI, we could potentially figure out how to do that for our universe and maybe send some message up there or or affect. >> It's like he read my paper.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 152

That's exactly the example we're giving. And here's the cool thing. When you read the description, it's kind of like Mario turns left, picks up mushroom, jumps once. It reads like magic spells. And if you off by one pixel, none of it works.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 153

So, the magic spells we have, maybe we're just not saying it in the right direction. Maybe you're eating the wrong mushroom. >> Interesting. >> What if reality has an unlock code and it's just like high-five this tree and then do a back flip and then boom, >> bring it at an easy level. >> Absolutely.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 154

>> And just I mean, what do you any any in terms of consciousness, some people suggest that consciousness does have something to do with it. And certainly if you're building a simulation that tries to generate consciousness, maybe it does either it's like the substrate of the universe somehow or somehow affecting it. Where do you fall on that? Could consciousness affect some some laws of the universe or is it just completely separate its own thing? >> So we know that your internal states impact your behavior.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 155

You feeling pain will definitely determine how you act. So there is a reality which is impacted by what happens inside. If you're a philosophical zombie, you can look up certain behaviors for certain situations. But in a novel situation, you have no idea what to do. That's the optical illusions for testing consciousness example.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 156

If I present you with novel optical illusion, you cannot just Google what the answer is. You have to experience that illusion to give me the right answer. >> Yeah. And we've been talking about these like world models in some of the newest AI systems so they can kind of imagine it or feel it before they say it out loud. And I guess that' probably be closer to the kind of consciousness we are.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 157

>> It's interesting that there is a lot of convergence between what we are and that those models are even though there is you know very different architecture substrate. We're starting to see that for example visual processing ends up looking a lot like human visual cortex animal models. So there is definitely some fundamental convergence taking place. >> What uh what books what books should I read like what what do you what books have influenced you whether like fiction or non-fiction? >> You already read all my books right?

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 158

>> Yeah. After after all after after all of yours. >> Uh I read a lot of Kurszsw Wild for sure was very interesting. Steven Wolram's work in cellar automata in terms of irreducibility of computation very important concepts uh there is a whole bunch of amazing papers I loved one where they take uh computer processor and apply tools of neuroscience to understand it so they kind of slice a piece of a processor look at it try to understand how a video game works from those observations and they get complete garbage out like none of it makes any sense in the context of how Mario is actually working memory of Mario. You're getting completely different ideas and that's the tools we use to understand human brain and human software.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 159

>> Absolutely fascinating. There's so much I feel like that we're going to be finding out about our own brain, how the universe works. I it seems like machine learning and neuroscience are they're kind of enabling each other, right? Because a lot of these advancements came from how sort of nature decided to to build it, but now we're almost like discovering more. So that that that drove the the um neural networks and stuff like that.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 160

But it's almost like we're discovering more about ourselves now as a result of that which is just fascinating. So I guess what are you know on a positive note I know we have a huge issue here in terms of um AI safety. on a positive note, maybe to give people some hope or just to sort of think about like what happens if we do figure it out, you know, what are some things that you might be very excited to see in the next, let's say, 5 years that emerge. So I actually have a paper which for game theoretic reasons claims that since AI is immortal they can wait a long time to strike against humanity accumulate more resources get more trust you know if they've been doing good job for decades will just surrender control of everything and they have no risk of being a loser in this battle. So maybe it's a good idea for them to try that.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 161

And so for a long time they have to pretend to be nice to us even though eventually they still out of control. Uh what are they going to deliver? So obviously dying is a bad thing. You want to live a infinite healthy life. And then whatever through personal universes virtually or in real world you want to have stuff.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 162

You want to have things you enjoy. You want to interact with other agents. And if it's a virtual world, you are limited by imagination of your super intelligent assistant. Not even yours, but whoever is generating cool games for you. >> Would you take that option, let's say, at the end of your life or at some point to kind of go into that perfectly built universe for yourself?

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 163

>> I think I'm already in a simulation. I'm good. >> So, you wouldn't uh improve this game. You wouldn't put in any cheat codes if you had the option to. Well, let's take virtual worlds we have right now.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 164

I tried them for five minutes. I said it is super cool. I never went back. >> Yeah, that is funny. That's Yeah.

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 165

Are um and I could be wrong like you know this is the first time I've met you, but you feel like a very stoic person to me. Is stoicism a philosophy that you identify with? >> Yeah, I read Daily Stoic. I'm about to finish the book soon. Yes.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 166

>> Oh, excellent. Yeah, I I Senica I loved it. I got to reread it one of these days. uh very interesting thing to to yet to think about especially now in these times. Yeah, >> it's all about control.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 167

It tells you that the only thing you control is your brain. So nothing else is subject to your understanding or but how you feel about the environment is completely for you to decide. >> Absolutely. Yeah. And it feels like out of all the philosophies like philosophy is interesting but in terms of philosophies like this idea of stoicism seems to be the most actionable one or applicable one like it's the most like you can install it as your operating system and it makes a positive change in your life.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 168

So yeah from that perspective it's very valuable I think. Do you meditate at all? Do you try to step back and notice your thoughts or >> I tried it doesn't work for me. >> Okay. It's just not part of your daily routine.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 169

>> I failed at meditating. Uh, daily affirmations. Do you wake up and write down things you're thankful for or >> I do have a list of things I'm thankful for? Absolutely. And it's quite impressive how it grows and evolves.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 170

>> Yeah. >> Yeah. >> That's when interesting. Something I've been working on is just like trying to reprogram my mind to be a little bit more positive to try to find positive spin on things even if I like don't instinctually believe them right away. just the process of thinking like how could I see this as a positive thing that that seems to just improve mood and stuff like that.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 171

Meditation is very interesting because you realize you're kind of the observer of your thoughts and I've really went down a rabbit rabbit hole with this new anthropic paper about because it it it is able to kind of look at its own thoughts and sort of like analyze them. So in that sense, those neural nets seem to be evolving in a similar way that our brains have through pressures, which is just uh I'm not sure what that means, but it's certainly a very interesting thing to think about. >> Well, we can see what happens to human agents, and it will probably happen to artificial agents. So again, safety in humans is not a solvable problem without genetically modifying humans, and that's not something we are good at yet. >> Yeah.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 172

Yeah. Yeah. Well, sometimes I just feel kind of small in the universe. It just seems obviously vast like physically, but also just this idea that everything had to work out perfectly for me to exist in the first place, right? Like I know a lot of people think about this, but every ancestor had to survive whatever they survived and all the ones that didn't are gone and the whole miracle all the way from, you know, the beginning of life all the way up to me.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 173

And I wonder if an AI like a GPT7 is going to have some introspection on itself and say, "Wow, what a what a miracle." like all these humans had to come together and put all these computers together for reasons to make them rich and instead it just ended up becoming this key thing for the reason I exist and I'm so thankful I'm here. Like maybe it'll be thankful that humans built it or at least recognize how rare that is in the universe. >> Obviously I can't say what a super intelligence will think but if you zoom out enough and you don't have human bias you see it more as like a law of nature. Things get more complex. There is a scaling hypothesis.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 174

It started with, you know, tiny molecules. Biology took us as a bootloadader to the level of computers and now we failed to design artificial intelligence. We couldn't do it. So what happened instead? We kind of grew it from just adding compute and data.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 175

Nobody designed it. We just keep adding more and more >> resource. >> Yeah. But its own existence will be as rare as ours, you know, like why did Yeah. Why did all the complexity come together in my body the trillion cells to let me have this conversation?

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 176

You know, it's just seems like a strange miracle. >> If it's a law of nature, it's very likely there are many such super intelligences throughout the universe and they probably can communicate a causal about how great they are. >> Oh, do do you have thoughts on why we don't see any other aliens in the sky? I mean, the Fermy paradox, is that something you've thought about? >> We don't know what to look for.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 177

So, if they're sufficiently advanced, they could be going into smaller worlds, not expanding through the universe for efficiency purposes. They could be with our physical body. They could be fields of energy. I don't know what to look for. So right now today 2025 we have uncontacted tribes.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 178

There are people living in jungle who like never encountered civilization on this planet and we're asking why haven't I met all the aliens in the galaxy. >> Right. Good argument. I haven't heard that one. It's a good way.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 179

>> Maybe they go inwards. >> You got to know what to look for first. >> Right. So your guess would be that they're probably out there. We just haven't seen them more so than we're the unique, you know, snowflake in the universe.

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 180

>> Exactly. It's a very large universe. There is a lot of compute out there. >> Yeah. I'm curious, how do you feel like um about your message getting out there so far?

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 181

Uh you've been on like Rogan and Lex Freeman. Um do you feel like it's mostly an English- speakaking audience that you you've been talking to? Do you think you've made an impact with politicians? Um, are you happy with any of your message getting out there or like do you feel like there's a lot more you have to do? >> Oh, so yeah, lots of Russian.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 182

I >> I do many interviews in other languages as well. So, uh, yeah, and it's been translated obviously to quite a few languages automatically now by AIS. It's no longer a bottleneck for people to understand even if they don't actually speak English. >> Very good. Yes, I actually understood that.

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 183

Uh but uh yeah, that's great. So um >> you speak Russian too. >> So So hey, you've given like a globally you you've talked to a global audience about this. >> We're trying we're trying to get the book translated into multiple languages. Right now we have uh one of the books is in Polish also.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 184

We have Greek, we have Russian, we are finishing Chinese version and hopefully it will keep scaling to the Hispanic world as well and uh throughout. And what's different about the way the Russian audience is um taking your message versus the English audience >> or other cultures as well? Yeah, I was going to ask the same thing. Yeah, >> it is uh slightly different in general type of comments you'll get on their podcasts. I I guess they are more into adversarial commenting due to the cultural difference.

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 185

So even a good interview will still get you a lot of uh negative feedback. you didn't ask for. >> Do do you does that do the negative comments slow you down or make you feel bad or are you pretty >> My algorithm is to multiply what you said by how much I love and respect you and anything multiplied by zero is you know >> that's great. I I wish more people had like conscious control of that to be able to turn down the noise. Uh that's certainly a >> but it also applies to praise.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 186

Then you have a complete person you never met on the internet saying you are a genius. Don't take that to heart either. >> Exactly. Yeah. You kind of tend to because of how how many people you might talk to, you kind of see both extremes and you're like, well, you got to kind of like Yeah, exactly.

### ðŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 187

>> I think said it best for the internet advice. He said, remain to praise and slander cool and do not argue with a fool. >> That's right. That's right. Words to words to live by.

### ðŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 188

Certainly, >> it always seems like it's framed as like China versus the US. Like either America wins or China wins. How do you feel about the world uh kind of building ASI and AGI right now? >> So again, we already talked about how it doesn't matter who builds it. It's still terrible.

### ðŸŽ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 189

We all die. But I don't see this very important adversarial difference. We're business partners. We're economic partners. We really depend on them and they depend on us.

### ðŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 190

They seem to be reasonable in terms of not starting wars. They've been extremely peaceful. They're extremely technologically advanced and there is also the better red than dead, >> right? Yeah, that's a that's a good point and and certainly um certain of the papers I I read from the Chinese side out of their universities and establishments there they do sort of sometimes insert these notes of saying hey this might be a good time for global cooperation so we can all talk about like what's acceptable what's not. There was a red uh paper called crossing the red line of AI self-replication where they showed how even fairly not advanced um open source models are able to self-replicate and create these little escape hatches if somebody tries to shut them down.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 191

And in that paper sort of these um Chinese researchers are also kind of flagging saying uh hey this might be a good time for global cooperation. Where do you think what would you recommend? Like if you could have one thing happen to kind of like make us safer, what would that be? >> Yeah. So they have actually politicians who by training a lot of times are scientists and engineers.

### ðŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 192

So I think they have a better chance of understanding all the arguments not just relying on advice of assistance and they have been publicly receptive to idea of dangers of super intelligence and some sort of regulatory framework maybe at UN level. US was not very much into that. So ideally it would be kind of like common thing for all nations not to develop it like you don't develop biological weapons. Well at least you pretend you don't. You don't develop chemical weapons and you don't develop intelligence weapons.

### ðŸŽ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 193

That's just a given. >> Yeah, that makes sense. I mean certainly there were international agreements about not developing certain technologies like changing the the the germ line DNA or um adding functionality to viruses, stuff like that. they they they still happen, but at least there's a sort of some agreement that that's some policing over it, etc. Um, and in terms of the US, like a lot of people bring up this idea that the Chinese counterparts over there that that that rule tend to be a lot more technologically knowledgeable, educated, etc.

### ðŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 194

here in the US most of the ruling body is either has a legal background maybe story history background something like that um how would you approach like who first of all who's explaining to them what this technology is it just investors and you know Google let's say um people out of Google that go lobbyists that go over there and try to help them explain the technology like h how well do you think they understand how to navigate this and how can we improve that >> is US politicians. >> Yeah. I mean because it seems to be like the biggest sort of domino that right if we control US or help shape it in the right direction that would certainly help. >> Yeah. When I talk to people in government or staffers off record they all in agreement they all think it's very bad and important but publicly they say they cannot accept it.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 195

And at a local level you would get something like regulation against deep fakes, regulation against algorithmic bias. So things they understand and can sell well. Nobody's going to talk about super intelligent robots chasing you. Uh at federal level with current administration they strictly accelerationist as far as I can tell. >> But how about robots?

### ðŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 196

Have you ordered one of the X1 Neo robots for your personal home? >> I have a collection of really safe robots which are very unlikely to jump on me and I'd like to keep it that way. >> So nope. You wouldn't buy a Optimus robot when it's available or anything like that. >> Can you hack it?

### ðŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 197

What software comes with it? What can I install on it? Can you jailbreak it? Those are the important questions. It's uh fully humanoid body.

### ðŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 198

Has perfect dexterity. It can hold a knife, a hammer. What are we doing here? >> Yeah. >> But what about your your laundry, your dishes?

### ðŸŽ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 199

Um deliveries. >> I got people. I got people. >> What about a do you have a robot vacuum like that um has a camera on it? sucked completely.

### ðŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 200

>> Okay. Thank you so much. I appreciate you being here. It's been an absolute pleasure chatting. Anything else that you would like people to know?

### ðŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 201

Anything else that you would like to kind of say to the world? >> Whatever you do, don't build general super intelligence. >> And with that said, we'll end this interview. Thank you so much. And viewer, we'll see you in the next one.

---

<div align="center">

**ðŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ30æ—¥

</div>
