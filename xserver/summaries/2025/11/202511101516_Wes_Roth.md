# ğŸ“º KIMI K2ãŒAIæ¥­ç•Œã«è¡æ’ƒ - ãã®ã€Œç§˜å¯†ã€ã¨ã¯

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: KIMI K2 just broke the AI Industry... here's it's "secret"
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=s-1x5nqp7mA](https://www.youtube.com/watch?v=s-1x5nqp7mA)
- **å‹•ç”»ID**: s-1x5nqp7mA
- **å…¬é–‹æ—¥**: 2025å¹´11æœˆ10æ—¥ 15:16
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ä¸­å›½ç™ºã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹AIãƒ¢ãƒ‡ãƒ«ã€ŒKimmy K2ã€ãŒã€ã‚ãšã‹460ä¸‡ãƒ‰ãƒ«ã®å­¦ç¿’ã‚³ã‚¹ãƒˆã§æœ€å…ˆç«¯ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æˆç¸¾ã‚’é”æˆã—ã€AIæ¥­ç•Œã«è¡æ’ƒã‚’ä¸ãˆã¾ã—ãŸã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Claude 4.5 Sonnetã‚„GPT-5ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€äººé¡æœ€å¾Œã®è©¦é¨“ã¨å‘¼ã°ã‚Œã‚‹ãƒ†ã‚¹ãƒˆã‚„é«˜åº¦ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ã¦ã„ã¾ã™ã€‚æ³¨ç›®ã™ã¹ãã¯ã€ç±³å›½ã®ç ”ç©¶æ‰€ãŒæ•°å„„ãƒ‰ãƒ«ã‹ã‘ã¦é–‹ç™ºã—ãŸãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ã€ã¯ã‚‹ã‹ã«ä½ã‚³ã‚¹ãƒˆã§å®Ÿç¾ã—ã¦ã„ã‚‹ç‚¹ã§ã™ã€‚ã“ã®å‹•ç”»ã¯ã€AIé–‹ç™ºè€…ã‚„æŠ€è¡“æˆ¦ç•¥ã«é–¢å¿ƒãŒã‚ã‚‹æ–¹å‘ã‘ã«ã€ä¸­å›½ã®AIé–‹ç™ºæˆ¦ç•¥ã€çŸ¥è­˜è’¸ç•™ã®æ‰‹æ³•ã€ãã—ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã«ã‚ˆã‚‹ä¾¡æ ¼ç ´å£Šã®å½±éŸ¿ã«ã¤ã„ã¦è§£èª¬ã—ã¦ã„ã¾ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

- **ä½ã‚³ã‚¹ãƒˆã§æœ€å…ˆç«¯æ€§èƒ½**: ã‚ãšã‹460ä¸‡ãƒ‰ãƒ«ã®å­¦ç¿’ã‚³ã‚¹ãƒˆã§ã€GPT-5ã‚„Claude 4.5 Sonnetã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’å®Ÿç¾ã€‚ãŸã ã—ã€å…ˆè¡Œãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜è’¸ç•™ã‚’æ´»ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¼ãƒ­ã‹ã‚‰ã®é–‹ç™ºã‚³ã‚¹ãƒˆã§ã¯ãªã„
- **ãƒ†ã‚¹ãƒˆæ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ´»ç”¨**: æ¨è«–æ™‚ã«å¤§é‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªå›ç­”ã‚’ç”Ÿæˆã€‚è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‹ã‘ã‚‹ã»ã©ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ä»•çµ„ã¿
- **ä¸­å›½ã®ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—æˆ¦ç•¥**: ç±³å›½ã®ç ”ç©¶æ‰€ãŒæ–°ã—ã„ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’ç™ºè¡¨ã™ã‚‹ã¨ã€ä¸­å›½ã®ç ”ç©¶æ‰€ãŒæ•°ãƒ¶æœˆå¾Œã«åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç¢ºç«‹ã•ã‚Œã¦ã„ã‚‹
- **ä¾¡æ ¼ç ´å£Šã®å½±éŸ¿**: é«˜æ€§èƒ½ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ç™»å ´ã«ã‚ˆã‚Šã€å•†ç”¨AIã‚µãƒ¼ãƒ“ã‚¹ã®ä¾¡æ ¼ã«ä¸‹æ–¹åœ§åŠ›ãŒã‹ã‹ã‚Šã€ç±³å›½ä¼æ¥­ã®åç›Šæ€§ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹
- **æƒ…å ±å…¬é–‹ã®æˆ¦ç•¥**: ä¸­å›½ã¯è¥¿å´ãŒå…¬è¡¨ã—ãŸæŠ€è¡“ãƒ¬ãƒ™ãƒ«ã¾ã§ã—ã‹è‡ªå›½ã®æˆæœã‚’å…¬è¡¨ã—ãªã„å‚¾å‘ãŒã‚ã‚Šã€å®Ÿéš›ã«ã¯ã•ã‚‰ã«å…ˆé€²çš„ãªæŠ€è¡“ã‚’ä¿æœ‰ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

So, the Kimmy K2 thinking model recently dropped and I'm a few days behind in reporting on this, but there has been a few kind of new things emerging about this model that I think are important to mention. So, you've probably seen the headlines. Kim K2, it's an open source model out of China. It got the top score state-of-the-art score on humanity's last exam. It's got the top score in browse comp.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

It's outperforming models like cloud 4.5 sonnet and GPT5 in those areas and it's looking really good across all the other scores as well. It executes up to 200 to 300 sequential tool calls without human interference. Excels and reasoning agentic search encoding 256k context window. So we know all of that. Here's where it gets interesting.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

So first of all notice it's saying built as a thinking agent K2 thinking marks our latest effort in test time scaling scaling both thinking tokens and tool calling churns. Now Kimmy K2 thinking is basically a larger upscaled version of DeepSeek R1. There are a few noticeable differences but the thing that really caught my attention here is when they're saying what they're experimenting with and that is the test time scaling. Now, we of course saw this idea of test time scaling when OpenAI first introduced the 01 model. So, we knew that these scaling laws kind of continued when we're talking about train time compute.

### ğŸ“ è©³ç´°èª¬æ˜

So, how long sort of hardware hours and GPUs we throw at training the model before it's ready. So, on pre-train the model and of course the more compute it has, the better it is at well, pretty much everything. So, here they're using the AIM, which is a a math exam, how accurate the model's on that particular exam. So, this concept certainly makes sense. The more compute, the more pre-trained compute, the better the model.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

Then the big deal with the 01 was its reasoning abilities, right? It was able to reason about a problem before answering it. And that ability to reason was called test time compute. So we gave it more hardware, more GPUs to burn some tokens thinking about it. And it thinking about it at length produced better results.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

So you can see it here again. The more tokens it uses, the more it thinks about it, the better the accuracy is. By the way, it's not just good at these specific tasks that they've cherrypicked. It's also very good on other benchmarks that maybe are not as wellknown, maybe not as popular, but are very interesting as well. The EQ Bench 3 for creative writing.

### ğŸ¯ å¿œç”¨ä¾‹

The Kim K2 is at the top, so it takes the number one spot. So, the various abilities that you have while writing across the board does very, very well. One thing I really like about the EQ bench is that they kind of track how similar certain models are to one another because we are able to tell which models are most like other models. So for example, some of the earlier iterations from Deepseek were very much like some of the other OpenAI models. Later iterations of Deepseek were more like Google Gemini.

### ğŸ’­ è€ƒå¯Ÿ

Now what this probably means is that these models are sort of they use knowledge distillation from those models, right? So they take that data, the synthetic reasoning data, and they kind of create their own models with it. So in essence, what this means is that the US builds some sort of an AI system, meaning one of the US labs, OpenAI, Google, etc. The Chinese labs, they take this data, some of the knowledge, some of the results from this model, and they train another model that often is kind of similar to the predecessor on which it was trained. So for example, this one you see it's very close to the other OpenAI models.

### ğŸ“Œ ã¾ã¨ã‚

Some of the other models like the Deep Seek the latest one is going to be much much closer to Gemini. The reality is there is a lot of innovation that's happening over there. But it it does seem like the data like a lot of it is based on US findings, US models, right? And so and they just open source that model. So that means that the US labs are spending tons and tons of money building those models and they're hoping to get some ROI on those models, some return on their money.

### âœ… çµè«–

The Chinese labs open source, something that's similar in this case in many ways even better for much cheaper. How much more? Let's take a look. According to the CNBC, the Alibaba backed moonshot releases its second AI update in four months as China's AI race heats up. And as you can see here, they're saying that the new Kimmy AI model costs just $4.6 million to train.

### ğŸ“š è¿½åŠ æƒ…å ±

Now, it's important to understand that if we can trust those numbers, that doesn't mean that any old lab can just make a model of this quality for $4.6 million. There's tons of other costs that go into it. Before it, you have to have a lab. You have to have some infrastructure already in place, but the cost of training that particular model is just under 5 million. So, what does this mean?

### ğŸ”– è£œè¶³

Well, we'll say the current state-of-the-art on some super important test is 30%. That's the best of the best. One AI lab creates a lot of innovations and it gets that to 40%, a massive jump. It costs them a lot of money, money, innovation, data, etc. Tons of GPUs, infrastructure, etc.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

And let's say there are several other labs that were sitting around 30%, for them to catch up is cheap. It does not cost a lot of money for them to catch up. So once one person hits one level, the the the followers, it's easier and cheaper and faster for them to catch up. And that's what we've been seeing with all the AI labs and with Deep Seek being able to catch up quickly for not a lot of money and just put out there a cheap version, an open- source version of whatever the best state-of-the-art model is. That's exactly what Kim K2 did here with a little twist.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

One thing you'll notice that everyone says that's using this model and I've noticed it as well is that it produces a lot of tokens for its thinking. So in other words, it's test time compute. It just burns through a lot of tokens even for fairly minor use cases. So meaning it's not here. It kind of walks itself up this line.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

So it uses more tokens and more compute to provide better answers. So according to this CNBC article, DeepS also claimed it spent 5.6 6 million for its V3 model in contrast to the billions spent by OpenA. So I mean this is nonsense. This is not true. I can't believe they would write this cuz here they're saying that DeepS spent this amount for one model and they're saying in contrast to billions.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

So OpenI did not spend billions on any given model. By the way, this is the article at the length that OpenI expects to burn billions through some year in the future. That has nothing to do with how much it cost them to train one model. Now, it probably costs a lot more than this. You know, tens of millions, but certainly not billions.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

So, why are the Chinese companies doing this? Well, there's a few reasons. First, it makes it that there's less money for US labs. If you're the only model in town, you can charge whatever price you want. If you have a multiple closed source labs, then you know, you're competing with each other, but prices are higher.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

If you have a few models that are just as good and open source and available and very inexpensive, well, that kind of like gives the next best alternative. Do you pay for something that's much more expensive or just switch over to the cheaper open source model? That really puts a downward pressure on the prices. It also of course ensures that the Chinese open source models are getting used in parts of the world that cannot pay the high prices of the western labs. This makes sure that the global AI infrastructure is built on Chinese models, not US-based ones.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

And of course, since China has a manufacturing advantage of building the hardware, the robots, the trinkets, the like the physical goods, and US generally has had a software advantage, right? Building the the software, the digital infrastructures, etc. By taking the wind out of like the software, the AI, the digital side of it, China still has its advantage in the physical manufacturing side. One other very interesting thing that I've been hearing recently. This is coming from a lot of people that know how things inside of China work.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

We have an interview with a very interesting expert coming soon to the channel. But this is apparently kind of open knowledge in how the leadership of China works. And that is that a lot of the scientific discoveries, they're sort of secret by default, especially when it comes to sensitive technologies like tech, warfare, AI, stuff like that. So it's not like they just get published in all the newspapers and everyone is free to talk about them. they usually don't get published out there.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

So they're known internally but not on worldwide global press. Whereas in the US a lot of stuff gets published and spread around etc. So it seems like China often releases their findings up to a certain point when they see that the western media is talking about the fact that it's been discovered on the sort of on the western side and made its way to the newspapers. So let's say some US lab figures out how to improve some technology by 25%. That gets published in the papers everybody's talking about it going hooray we did it at this point the Chinese side if they have already done the same research and already got to 25% or more right so up to 25% that gets sort of unclassified and they go okay publish the paper that shows that we're able to do the same thing.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

So in other words, they're only willing to admit they have knowledge. Only when the sort of western side, the western media shows that okay, we have this and we're willing to publish it. That means that in a race where US and China is neck to neck, let's say US figures out how to get up here, then China will publish what they have showing that they're also up here. Again, they are they're publishing true information. The point isn't that they're just saying they are.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

The point is they'll never publish something that shows that they're way ahead because that might allow everyone else to reverse engineer that technology or realize that oh there's this possible approach to to get that effectiveness etc. And we're literally seeing that the same exact dynamics play out here. Every time a US labs hits a new high, China comes out very soon months after with a model that's right around there. Usually it was just a little bit less. here with the Kimmy K2.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

It's just a little bit more probably because they they ramped up that test time compute, but it's likely that we're going to keep seeing this. Every big win on the US/Western side will be met with a equal increase in capability on the Chinese side, often much cheaper and with a lot less hardware requirements. So, I think there's two kind of big takeaways here. One is that no one is likely to win by a mile, let's say, in this AI race. it's unlikely that somebody's going to pull away ahead and stay there.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

If you've ever played Mario Karts, there was these catch-up mechanics to where if somebody's in last place, they have certain benefits and gamebased features that kind of make it more likely that they'll catch up. The person at the front gets slight penalties, you could say, and the person at the back gets slight advantages so that you can always kind of catch up. You may set the race miles ahead, but by the middle, by the end, that could change rapidly. So that means that the AI race is kind of like Mario Kart. That's point number one.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

And point number two, we may not know what the Chinese labs are actually working on because they might not be publishing in their actual research unless the Western side already published something at least as good. Now, I'm not necessarily saying they have something just I don't know if we would necessarily know. Anyways, those are my two cents on the Kimik2 model release. Let me know what you thought about it. If you made it this far, thank you for watching.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

My name is Wes Roth and I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ30æ—¥

</div>
