# ğŸ“º Googleã®SIMA 2 - AGIã¸ã®ã€Œé‡è¦ãªä¸€æ­©ã€

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: SIMA 2 is a "significant step towards AGI" says Google
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=pEa5mbpcBCg](https://www.youtube.com/watch?v=pEa5mbpcBCg)
- **å‹•ç”»ID**: pEa5mbpcBCg
- **å…¬é–‹æ—¥**: 2025å¹´11æœˆ15æ—¥ 13:00
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

Google DeepMindãŒç™ºè¡¨ã—ãŸSIMA 2ã¯ã€äººé–“ã¨åŒã˜ã‚ˆã†ã«ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã¨ãƒã‚¦ã‚¹ã‚’ä½¿ã„ã€ç”»é¢ã‚’è¦‹ãªãŒã‚‰ãƒ“ãƒ‡ã‚ªã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã§ãã‚‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚å‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰å¤§å¹…ã«é€²åŒ–ã—ã€ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ãŒ31%ã‹ã‚‰65%ã«å‘ä¸Šï¼ˆäººé–“ã¯76%ï¼‰ã€‚Geminiãƒ¢ãƒ‡ãƒ«ã‚’ä¸­æ ¸ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€å˜ãªã‚‹æŒ‡ç¤ºå®Ÿè¡Œã‚’è¶…ãˆã¦æ€è€ƒã—ã€æ¨è«–ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨å¯¾è©±ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã‚²ãƒ¼ãƒ ã§å­¦ã‚“ã ã‚¹ã‚­ãƒ«ã¯å®Ÿä¸–ç•Œã®ãƒ­ãƒœãƒƒãƒˆã«ã‚‚è»¢ç”¨å¯èƒ½ã§ã€AGIå®Ÿç¾ã¸ã®é‡è¦ãªãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚AIç ”ç©¶è€…ã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹é–¢ä¿‚è€…ã€ãã—ã¦AGIã®æœªæ¥ã«èˆˆå‘³ãŒã‚ã‚‹æ–¹ã«å‘ã‘ãŸå†…å®¹ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

- **äººé–“ãƒ¬ãƒ™ãƒ«ã«è¿«ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: SIMA 2ã®ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ã¯65%ã«é”ã—ã€äººé–“ã®76%ã«æ€¥æ¥è¿‘ã€‚SIMA 1ã®31%ã‹ã‚‰åŠ‡çš„ã«å‘ä¸Šã—ã€æœªè¦‹ã®ç’°å¢ƒã§ã‚‚14%ã®æˆåŠŸç‡ï¼ˆSIMA 1ã¯2-3%ï¼‰ã‚’é”æˆ

- **Geminiã«ã‚ˆã‚‹æ€è€ƒã¨æ¨è«–**: å˜ãªã‚‹å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã ã£ãŸSIMA 1ã‹ã‚‰é€²åŒ–ã—ã€Geminiãƒ¢ãƒ‡ãƒ«ã‚’ä¸­æ ¸ã«çµ±åˆã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨å¯¾è©±ã—ã€ç’°å¢ƒã‚’èª¬æ˜ã—ã€è¤‡é›‘ãªæŒ‡ç¤ºã‚’ç†è§£ã—ã¦å®Ÿè¡Œã§ãã‚‹æ±ç”¨çš„ãªAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å¤‰è²Œ

- **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã¸ã®å¿œç”¨**: ã‚²ãƒ¼ãƒ ã§ç²å¾—ã—ãŸã‚¹ã‚­ãƒ«ï¼ˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€é“å…·ä½¿ç”¨ã€å”èª¿ä½œæ¥­ãªã©ï¼‰ã¯å®Ÿä¸–ç•Œã®ãƒ­ãƒœãƒƒãƒˆã«ç›´æ¥è»¢ç”¨å¯èƒ½ã€‚ä»®æƒ³ä¸–ç•Œã®ãƒ”ã‚¯ã‚»ãƒ«å…¥åŠ›ã¨ç‰©ç†ä¸–ç•Œã®ã‚«ãƒ¡ãƒ©å…¥åŠ›ã®æœ¬è³ªçš„ãªé¡ä¼¼æ€§ã«ã‚ˆã‚Šã€ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªæŠ€è¡“ç§»è»¢ãŒå®Ÿç¾

- **Genie 3ã¨ã®çµ±åˆ**: AIä¸–ç•Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«Genie 3ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ç„¡é™ã®è¨“ç·´ç’°å¢ƒã‚’è‡ªå‹•ç”Ÿæˆå¯èƒ½ã€‚SIMA 2ã¯ã“ã‚Œã‚‰ã®æœªçŸ¥ã®ä¸–ç•Œã§ã‚‚é©å¿œã—ã€è‡ªå·±æ”¹å–„ã‚’ç¶™ç¶šã§ãã‚‹é©æ–°çš„ãªå­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¢ºç«‹

- **è‡ªå·±æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿç¾**: GeminiãŒã€Œã‚¿ã‚¹ã‚¯è¨­å®šè€…ã€ã€Œå®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã€Œå ±é…¬ãƒ¢ãƒ‡ãƒ«ã€ã®3å½¹ã‚’æ‹…å½“ã—ã€äººé–“ã®ä»‹å…¥ã‚’æœ€å°é™ã«æŠ‘ãˆãŸç¶™ç¶šçš„å­¦ç¿’ã‚’å®Ÿç¾ã€‚ã“ã‚Œã¯äººé–“ã®è„³ã®åƒãã«è¿‘ã„è‡ªå¾‹çš„ãªå­¦ç¿’ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

What you see right now is not a human playing a game. It's an AI agent. This is a Simma 2 from Google DeepMind. You might have seen bots play games in the past, but this is completely different. Simma 2 can learn, understand, improve, and adapt.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

It can explain the game to you. It is able to do this by actually playing the game similar to how you and I would using a keyboard and mouse and looking at the actual gameplay happening on the screen. Simmit 2 is probably the closest systems we have to how humans learn at least for specifically learning to play video games. You might be wondering why it's important for these AIs to learn to play video games. Why is it important that they learn to play in the virtual world?

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

It's a great question. Look at it this way. Let's say this AI learns to master any new game we give it just by playing it. It plays and over time it gets really good at Minecraft, Battlefield, whatever racing game or sim game or or strategy game. It might even one day get really good at Dwarf Fortress.

### ğŸ“ è©³ç´°èª¬æ˜

That's an AGI milestone, by the way. But the point is this. It masters a thousand games. That digital brain capable of learning to play games on the fly gets placed into a real physical robot. To the AI, this is just game number, 101.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

It's still perceiving the pixels from the game world, only instead of them being rendered in 3D. It's a camera that's looking out into the real world to the model, it's still just pixels in. It still sends some form of keyboard or joystick commands, but instead of it moving the in-game character, it moves the realworld robot. If this AI can generalize between video games, meaning that it's skills transfer from one game to another, just like they do for us humans, well, then all those skills also transfer to the real world. Now, we need to make sure it understands that the rules of GTA are different from the rules of driving here in the real world.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

Unless you're in LA, of course, then they are shockingly similar. So, Google DeepMind just released SIMA 2. It's a very exciting project. We covered SIMA 1 on this channel before, and I got to say the progress since then has been pretty incredible. It's much, much better now than it was.

### ğŸ¯ å¿œç”¨ä¾‹

So, really fast. This is from the SIMA one paper. So, the first thing to understand is that these AIs, they're playing the game just like you or I would. They're using, you know, keyboard, mouse, joystick. They're looking at the actual pixels on the screen.

### ğŸ’­ è€ƒå¯Ÿ

So, here's our little Sema agent, right? So, it takes various actions on the keyboard and mouse that interacts with the environment, right? He's getting that visual input. So, this is kind of his loop of interacting with the environment. Again, this could just as easily be a human, right?

### ğŸ“Œ ã¾ã¨ã‚

Using the keyboard and mouse to play a game, looking at the screen, environment being that game. The other part of it is the user giving it language instructions. So, it says, "Go build the house, you know, go chop some wood." And then they are graded on how well they're able to use their keyboard and mouse, they're, you know, looking at the screen, how well they're able to execute that task. This was Simma 1, by the way. This was from their technical report.

### âœ… çµè«–

The technical report for SEMA 2 is not quite out yet, but I'll probably cover it when it comes out because I' I'd love to see what they did since Cinema 1. So, notice kind of like this little AI organizes the different activities, the different skills they can do in various games. So, this is across many games into these categories and specific skills. So there could be farming or building or operating vehicles. Then specific skills underneath that like look around or drive or kick or feed or you know attach object equip activate etc.

### ğŸ“š è¿½åŠ æƒ…å ±

So this is important because over time this creates some generalized skills that carry from game to game. And in Cinema 1 we saw kind of the beginning of these abilities emerge. So we can tell this AI agent to go to the spaceship and you know it moves towards the spaceship. Very basic movement. Here's chop down tree.

### ğŸ”– è£œè¶³

It's able to execute that drive the car. You know it approaches the car and gets in. And that was released March 13th, 2024. Now it's November 13, 2025. So quite a lot of time has passed in AI years.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

Now we have Simma 2. So first and foremost, notice it's running on Gemini models. The ability of Gemini models to generalize and to sort of have certain pretty exciting skills even though they were not specifically trained for those skills is is kind of stunning. It's a very general model capable of doing a wide range of tasks. And this is just one of many many different applications of what it can do.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

And with Simma 2, not only can it follow human language instructions in the virtual worlds, we kind of already were beginning to see that with Simma 1, but Simma 2 can now also think about its goals, converse with users, and prove itself over time. This is a significant step in the direction of artificial general intelligence, AGI. And like I mentioned in the beginning, this has huge implications for robotics and AI embodiment in general. Some people believe and I tend to trust him that in future anything that moves will be automated, right? So your lawn mower, the bus, the car, like anything that that moves around your house, I mean we already have moving vacuum cleaners in the form of a Roombas, etc.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

But basically anything that moves around and has sensors that needs to interact with its environments, it will be automated by some sort of an AI model. And some people are also expecting that, for example, your lawnmower and the thing that flies the plane. They're not necessarily going to have two separate models, right? So, they're not going to have bespoke models for each one of their those use cases. You're going to have kind of like this one universal model.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

Where is that model coming from? Well, we're probably seeing the very very early prototypes of it, right? So if something like this can play and reason and learn across many many many different games certainly you can see how eventually this model just assume it keeps getting better keeps learning eventually it might be able to pilot a robot fly a plane you know cut the grass etc. So here I'm saying the first version of Simma learned to play over 600 language following skills like turn left climb the ladder open the map across a diverse set of commercial video games. It operated in these environments as a person might by looking at the screen and using a virtual keyboard and mouse to navigate without access to the underlying game mechanics.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

That's important to understand because we've seen very exciting performances by AI models in the past. Open AAI 5, right? So, an AI model that beats world champions at Dota 2. Of course, everyone remembers Alpha Star, right? Grandmaster level and Starcraft 2 using multi- aent reinforcement learning.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

Now, it's important to understand that those bots don't actually interact with the game in the same way that you and I do. Again, it's not a keyboard and mouse. They're playing through the API. The same is true for Nvidia's Minecraft Voyager. As impressive as that is, it doesn't have vision.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

It doesn't see this. It's not clicking, you know, somewhere in the screen. It's not looking around. It uses the API. It's very textbased.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

still incredibly incredibly impressive for different reasons, but it's extremely important uh to understand this distinction. This is might seem like not a big deal at first, but this is huge. This ability to interact with the games or with the computer through mouse and keyboard is fairly recent, at least the ability to do so effectively. We're just beginning to see that this year. you know, claudes computer use, opening eyes atlas, all those things.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

They're still not great, but they're getting to the point where they're useful. They can actually complete tasks. And as Google Deep Mind says here with SEMA 2, we've moved beyond instruction following by embedding a Gemini model as the agent's core. Simma 2 can do more than just respond to instructions. It can think and reason about them.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

Now, it's been a while since I read Simma 1, but I believe this wasn't Gemini. I don't think this was a large language model at all. It was a more just a reinforcement learning AI agent and initially it used human data to kind of teach it how to play. So for Simma one, their approach relied on agents at scale via behavioral cloning. Supervised learning of the mapping from observations to actions on data generated by humans.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

So human gamers would play the data, the keyboard and mouse commands that they used would be logged. There would be videos, language instructions, and dialogue, etc. Here they're using a Gemini model as the core, which is again one of the reasons these large language models are so exciting is because of how general they are. So notice here, this is a Sema one on the left, Sema 2 on the right. So the user says, "Go up and slightly left to the cave and mine to get some coal." This is not doing great.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

It's struggling. Sema 2 is reasoning. Okay, I'll head towards the cave up and left to find some coal. I found the coal inside. It starts mining.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

I mean, it's doing it much much better, much more effectively. Like, this was great. It It slides off the block for a second. But, I mean, overall, it completes the mission very, very quickly. This is a huge jump.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

Same thing here. Sema one on the left. Sema 2 on the right. Find a campfire. So, here's kind of getting stuck.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

It's, you know, wandering through the woods. So, it didn't do it correctly. Sema 2 looks around, spots the campfire, and approaches it. Boom. That's terrific.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

It works. As I say here, SEMA 2's new architecture integrates Gemini's powerful reasoning abilities to help it understand a user's highle goal, perform complex reasoning and pursuit, and skillfully execute goal oriented actions within games. So, here's one. So, the user says, um, where are you? The Gemini model, this agent describes where they are, what planet, asked, "What are you doing here?" It has some idea, I guess, of what the storyline is.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

Can you look around? What's going on? Check out those eggshaped objects. It approaches it. It scans the objects and it reports back what they are.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Can you mine them? Yes, it goes and mines them. So that's definitely showing a great understanding of the game, not only in the ability to describe its environment visually, but also understanding I I haven't played the game. I assume it knows the story line, right? Cuz it's kind of saying what what it's doing there in the planet potentially as it sort of relates to the story line.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Then is able to understand because we don't know the name of that egg, right? So we just say, "Oh, it's like an egg-like thing. It understands what we're talking about, approaches it, scans it. when we when it's told to mine it, it mines it. That's very advanced again compared to where it was a year and a half ago.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

Here we give it a somewhat hard task by telling it to go to the tomato house. Here's the problem. There's really no tomato house. Our little AI agent correctly reasons that, oh, this user probably means this red house. All that is to say is that there's a huge forward leap in generalization performance.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Sema 2 can now understand more complex and nuanced instructions than its predecessor and is far more successful at carrying them out. And this is the big thing to understand particularly in situations or games on which it's never been trained. Right? So it's sort of generalizes across games. It's better able to deal with things it hasn't seen before.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

It's also able to, you know, have multimodal inputs. Like let's say I sketch this thing. I'm like go find this thing that I sketched. It's able to find that object in the game world. It understands what we're talking about.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

It understands different languages and even emojis. And importantly, it's able to kind of generalize across words. Gemini is a powerful word model, powerful language model. So, it's able to understand what you mean even if you don't use exactly the right words. Right?

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

If you tell it to harvest something instead of saying mine it or chop it down or whatever, it will understand what you mean, it will be able to generalize and get whatever tool that it needs and and reason what it needs to do and go and do that thing. You know, if you ever talked to an AI, a chatbot, and you didn't quite remember what word you wanted to use, so you kind of substituted with the best possible word, it still figures it out. As long as it has like the bare minimum needed context, it'll figure it out. which gets us to maybe the most important part of this paper of this presentation and that is the task completion success rates. All right.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

So, as you can see there, that dotted line, that's the human sort of baseline, right? So, when humans are asked to perform certain tasks in games, at least on this benchmark, right? They're slightly above a 75% success rate. So, SEMA 1 in this regard wasn't that great, right? If if 70, let's call it 76 is the human baseline, it was at 31.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

So, there's still a a very large and noticeable gap between where it was and where humans are. So last year, humans are clearly superior at playing video games. Hooray for us and our opposable thumbs. And this year, 2025, we're still superior, but it's getting a lot closer. The AI is catching up fast.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

It went from 31 to 65. Humans are, let's say, 76%, just above 75. By the way, I think this chart really demonstrates kind of in one image the current discussion around AI capabilities right now. So some AI model comes out and it's not as good as humans. Humans are clearly better.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

So people go, "Oh, look how horrible it is at doing that thing that we are so good at. Clearly AI is bad and it will never be as good as humans." Then, you know, 12 to 18 months go by and it's here. And those people, they're still like, "Oh, oh yeah, but still humans are better." Like, we're still hanging on to hope. We're clinging to hope. And this is the conversation around a lot of the AI progress that's happening right now.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

What people are failing to appreciate is like what happens if you just draw this line like this? If you kind of project to where things are going and then give it a few years, do you think this product is going to stop just under the human baseline? Are we that special? Are abilities that magical? Do you think that there's some hard line that no machine will ever cross?

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

Notice that it hasn't really happened before with any of the other skills that we've observed, right? Back in 2012, it was worse than humans at image classification. And then it goes goes goes it crosses the human performance line and and I mean keeps going. There might not be that much more to go. Either you're getting it right or you're not.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

But but the point is notice none of these like flatline just before the human performance. There's no asmmptote at human performance and and machines just can't cross that line. They blow right past it. I just say that to point out the fact that when people say AI can't do this as good as humans, you know, they're talking about it here about this particular thing and then it crosses. They stop talking about it and they go on to the next thing that AI can't do as good as humans yet.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

And when that thing crosses the human performance line, they go on to the next thing. So Sema 2 can't play games as good as humans can with a keyboard and mouse. That's true. Would you bet that Sema 3 won't be able to? What about Sema 4?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

Also notice the big jump in how well it performs on previously unseen environments. So from SEMA 1 to SEMA 2, from SEMA 1 to SEMA 2 to SEMA 1, that reinforcement learning agent that was kind of kickstarted with with human labelled data, it didn't do that well on previously unseen environments. So ASCA was, let's say, two or 3%, maybe two and a half. Mind do was, I mean, close to zero. And here with Simma 2, it jumps up to where the success rate, let's call it, I don't know, maybe 14%, 13% here.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

If you've ever watched somebody operate a new operating system or a new phone, some piece of technology that they've never used before, I mean, their success rate might be kind of low and then, you know, slowly go up over time if they keep using that technology. So I I don't know if this is too dissimilar from what we would expect with humans, meaning that SEMA 2 is closer to how we expect humans to behave and what abilities they would have versus SEMA 1, which can't it seems like it can't generalize that well. But here's where we get to the kind of the big thing and it's kind of towards the second half of this blog post kind of a burying the lead here because you might recall Genie 3. Genie3 is able to create complete new worlds based on certain images or descriptions and you're able to walk around in those worlds and move around those worlds and they look pretty pretty good. Here's a show horse event in the real world.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

You can see it generally on the right skiing and jumping helicopter flying over coastal cliffs etc. You're able to look around and you're able to move around within that world. It has a fairly incredible abilities to remember what happens in the world. So if you paint on a wall and then you look over somewhere and you look back, that paint remains consistent. So that world remains consistent.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

Notice it's still there. So the Genie models like Genie 3 can generate games basically on the fly. they can kind of dream them up in real time and you're able to move in those video games by using the controllers. Any image or description can become a world, a playable world. So, here for example is a Genie 3D generation.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

The prompt is a vibrant 3D style adorable fluffy creature bounding across a vibrant rainbow bridge in a fantastical landscape. The creature small and compact with fur that mimics the warm hues of sunrise. oranges, yellows, and pinks blending seamlessly together. Its most striking feature is a pair of large perked ears shaped like those of a German Shepherd. It goes on and on describing little floating islands, the landscape, bright and cheerful lighting, etc.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

Notice that you're moving around the world using the arrow keys. And Genie 3 kind of shows you realistically what would happen when you push left, right, up, down, etc. So, Genie3 creates entire worlds on the fly that you're able to move around in. And so, guess how Sema 2 fits in with that? Well, when we challenged Sema 2 to play in these newly generated worlds, we found it was able to sensibly orient itself, understand user instructions, and take meaningful actions towards goals.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

Despite never having seen such environments before, it demonstrated an unprecedented level of adaptability. So, notice this little uh butterfly flying around. You can say, "Head over here. go to this flower. And it does.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

So, just really fast, this AI agent Sema 2 plays the game. As it plays the game, it gets better at playing the game, right? So, there's kind of this uh self-improvement thing going on. And Genie 3 basically can generate unlimited games from simple prompts that a large language model can write. So, you have unlimited infinite worlds basically that can be substituted on the fly, right?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

So this AI that's getting trained across different worlds has an unlimited amount of access to unlimited amount of worlds. This is why this Sema plus Genie together is kind of a big deal. And by the way, this wasn't an accident that they were building these technologies. It's not like they developed two separate projects and were like, "Oh, these seem to go together pretty well." That's probably not what happened. This whole entire game is basically this.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

It's AI agents running in simulations and just getting better and better and better, right? That's this is kind of the the magical formula. AI plus simulations, right? AI is the brain and simulations is the data that improve that brain. We're seeing a very very early kind of concept of that taking shape here.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

So Google de mind continues. They're saying towards a scalable multitask self-improvement. One of Sema 2 most exciting new capabilities is its capacity for self-improvement. We've observed that throughout the course of training, SEMA 2 agents can perform increasingly complex and new tasks bootstrapped by trial and error and Gemini based feedback. For example, after initially learning from human demonstrations like with Simo 1.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 56

By the way, some of these details I might be getting off because we don't have access to the technical report yet. Once that's out, I'll do an update because some of these things, they might specify how exactly what the architecture is. But they're saying SEMA 2 can transition from human demonstrations to learning in new games exclusively through selfdirected play. So it's developing skills in previously unseen worlds without additional human generated data. And then in subsequent training, SEMA 2's own experience data can be then used to train the next even more capable version of the agent.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 57

We were even able to leverage SEMA 2's capability for self-improvement in newly created Genie environments, a major milestone towards training general agents across diverse generated worlds. So here's kind of that diagram and it's kind of an insane looking diagram. So this is the Gemini model. So notice it's the agent. It's the thing that plays the game, right?

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 58

Using a keyboard and mouse, interacting with the world, with the environment, seeing what happens. It is also right a a different sort of instance of it like a different copy of it is also the task setter. So it's the user saying hey go do this go do that chop some wood build a house grind your skills up etc. Right? So it's setting the task to the agent that's actually you know taking actions in the real world.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 59

And of course you also have the reward model. The thing that grades it and tells it how well it's doing. And guess what's running that? Gemini. Yes.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 60

So Gemini says good job and bad job and kind of tweaks the rewards to to make sure that these things are moving forward that creates these self-generated experience. By the way, this is probably not too dissimilar from how the human brain works, right? Have you ever had this experience where you, the task setter, tell yourself, "Okay, get out of bed and go to the gym." Right? You tell that to to the agent that's supposed to execute that action. The agent is like, "No, it's warm in here." You're like, "No, seriously, get up and go to the gym." It's like, "No, it's cold out there.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 61

It's warm in here. It's 5 in the morning." I mean, how can you tell me that's not two people in there? Right? Then if you make it to the gym, then this some other part of you is like, "Oh, you should feel good about yourself. You've you've done something right with your life, right?" And as I say here, this virtuous cycle of iterative improvements paves the way for a future where agents can learn and grow with minimal human intervention, becoming open-ended learners in embodied AI.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 62

So, if you've been following this channel for a while, we've we've talked about this for quite some time because in the past we had reinforcement learning, right? So, we have Alpha Go, Alpha Go Zero, Alpha Code, Alpha Dev, Alpha Geometry, Alpha Evolve, Alpha Genome, Alpha Fold is in there somewhere. So, a lot of them were done with reinforcement learning. So, you get a plus one for doing well, minus one for doing poorly. They also utilized a selfplay.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 63

So, for example, with Alpha Zero and some of the other models, they found that when they didn't use human data, but rather had the AI itself just self-play and learn everything from scratch, it would actually get a lot better than humans. But all these were narrow AIs. They would be good at chess or go or whatever the thing is that they were trained on. Then later we got LM. LM were different because they were generally intelligent.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 64

They could take a stab at whatever you asked them to do. Sometimes they did very well and sometimes they would mess it up hilariously, but they had some skill at everything. They they try anything. They were generally intelligent. And what we've been talking about for last few years that slowly these two different technologies are starting to kind of overlap a lot more.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 65

How do we take the power of reinforcement learning, self-play, everything that we've learned from those models and how do we get these large language models to engage in that to iteratively self-improve and there's a lot of papers that are kind of showing different approaches for this. But here I think is probably one of the best examples of an all-in-one agent like this. Right? They're saying Sema 2's ability to operate across diverse gaming environments is a crucial proving ground for general intelligence, allowing agents to master skills, practice complex reasoning, and learn continuously through selfdirected play. Now, they highlight some of the issues because as exciting as this is, there's still a lot of problems and issues that these large language models face.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 66

They have problems with very long horizon tasks, things that require extensive multi-step reasoning and goal verifications. they have a relatively short memory, a limited context window, etc. And of course, using, you know, specific keyboard and mouse interface actions is difficult, especially with complex 3D scenes, etc. By the way, there's a number of papers that uh address a lot of these issues with the shorter context windows and those limitations. We covered recently Google's nested learning, a new machine learning paradigm for continual learning.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 67

I almost view it as as as a kind of short-term memory that updates very very quickly, but you're also having this kind of long-term memory that goes slowly, but that only incorporates things that are like more important. So the short-term memory is kind of like your context window. But this model, these large language models, they will also kind of back up the important stuff into their own weights. They will kind of continuously update themselves with these kind of longterm memory. it'll happen slower and it'll only take in what's needed.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 68

But that's as I understand it is is the approach that they're talking about here. And here Google also compares it to our very own brain and its neuroplasticity. So it's the ability to reorganize itself to update its sinapses you know change its weights if you will. And coming back to SEMA 2 of course the robotics is where this could be very very effective. Sema 2 offers a strong path towards applications in robotics.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 69

The skills it learned from navigation and tool used to collaborative task execution because keep in mind a big part of it is listening to directions from the user or in this case another instance of Gemini, right? And then carrying out those instructions that's baked into that architecture. Those things are some of the fundamental building blocks for physical embodiment of intelligence needed for these robots and AI assistants in the physical world. So you might hear people referring to the bitter lesson. This is kind of a famous idea from an AI researcher whose name is Rich Sutton.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 70

You've probably heard his name. In plain language, the bitter lesson is this idea that in the long run, the most powerful AI systems come from methods that learn automatically from their environments, from computation, from data, not from humans handcoding clever rules or domain expertise. So interestingly, we just keep creating these playgrounds and AIs to teach itself to kind of self-play and learn and expand because over time that always seems to beat whatever clever scheme or script that we write to explain to the bot what to do. Anyways, I've said this before and I'll say it again. If you enjoy online competitive gaming, really enjoy it cuz it might be changing very very soon.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 71

Just imagine an open-source version of something like SEMA 3. Those will be in all your favorite massive multiplayer games online, grinding and getting speed and collecting resources and maybe even communicating to other players, being able to kind of like team up with them and cooperate to to progress further game. I wonder if by that point we'll realize that they might be a lot more fun to play with than other human players. Anyways, if you made it this far, thank you so much for watching. What do you think about this?

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 72

Is it exciting? Do you think this will unlock robotics for us? This this will be kind of like the big thing that gets us to robots. Let me know in the comments. My name is Wes Roth.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 73

Thank you so much for watching. and I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ30æ—¥

</div>
