# ğŸ“º Gemini 3ãŒæã‚ã—ã„ã»ã©é€²åŒ–ã—ãŸ

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: Gemini 3 just got *scary* good
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=96qyyz_ZJ_U](https://www.youtube.com/watch?v=96qyyz_ZJ_U)
- **å‹•ç”»ID**: 96qyyz_ZJ_U
- **å…¬é–‹æ—¥**: 2025å¹´11æœˆ19æ—¥ 04:06
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

Googleã®Gemini 3 ProãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã€è¤‡æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ãŸã€‚ç‰¹ã«Vending Benchã§ç«¶åˆã‚’åœ§å€’ã—ã€Humanity's Last Examã‚„ARC AGI 2ãªã©ã®é›£é–¢ãƒ†ã‚¹ãƒˆã§æ¥­ç•Œãƒˆãƒƒãƒ—ã®æˆç¸¾ã‚’é”æˆã€‚æœ¬å‹•ç”»ã¯AIé–‹ç™ºè€…ã‚„ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æ„›å¥½å®¶å‘ã‘ã«ã€Gemini 3ã®é©šç•°çš„ãªæ€§èƒ½å‘ä¸Šã¨ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ©Ÿèƒ½ã®è©³ç´°ã‚’è§£èª¬ã™ã‚‹ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

- **Vending Bench 2ã§åœ§å‹**: åˆæœŸè³‡æœ¬$500ã‚’$5,000ä»¥ä¸Šã«å¢—ã‚„ã—ã€Grok 4ã‚„Claude Sonnetã‚’å¤§ããä¸Šå›ã‚‹
- **é›£é–¢ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é¦–ä½**: Humanity's Last Examã§37.5%ã€ARC AGI 2ã§31.1%ã‚’é”æˆã—ã€ä»–ãƒ¢ãƒ‡ãƒ«ã‚’åœ§å€’
- **ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒæœ€é«˜**: ARC AGIã§75%ã®ç²¾åº¦ã‚’49ã‚»ãƒ³ãƒˆ/ã‚¿ã‚¹ã‚¯ã§å®Ÿç¾ã€æœ€ã‚‚å®‰ä¾¡ã§é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«
- **LM Arenaã§å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼1ä½**: ãƒ†ã‚­ã‚¹ãƒˆã€ã‚¦ã‚§ãƒ–é–‹ç™ºã€ãƒ“ã‚¸ãƒ§ãƒ³ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©å…¨åˆ†é‡ã§ãƒˆãƒƒãƒ—ç²å¾—
- **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ€§èƒ½ã®é£›èº**: å‹•ç”»ç†è§£ã€ãƒãƒ£ãƒ¼ãƒˆåˆ†æã€OCRãªã©è¦–è¦šã‚¿ã‚¹ã‚¯ã§å¤§å¹…ãªæ”¹å–„ã‚’å®Ÿç¾

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

Gemini 3 rolled out today and it's a beast. I had a chance to test out an earlier version and I got to say it's a pretty big leap forward. This isn't a tiny incremental update. It really does deserve the three at the end of it. Let's take a look.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

If you wanted to try it out, it's going to be available in the Gemini app, AI Studio, and Vertex AI. It's live there now. They also announced a new agentic development platform, Google Anti-gravity. We'll talk about that in a little bit. You'll also see Gemini 3 in the AI mode in search.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

There are some limitations based on what tier you have, what subscription you have with the Google AI plan. If you recall that whole Google AI pro, Google AI ultra, those plans will determine which access you will have. They will also be rolling out Gemini 3 deep think. This is not available to everyone right now. Access first will be given to safety testers, then later to the Google AI Ultra subscribers.

### ğŸ“ è©³ç´°èª¬æ˜

All right, first and foremost, let's take a look at the benchmarks. The benchmarks aren't everything, but they do give you a kind of first shot, a a glimpse into the potential these models have. As I've said before, myself and a lot of other people are preferring benchmarks that include some sort of agentic work. So, not just answering questions, actually doing some sort of a work that has a long horizon where the AI model has to kind of change and adapt and have a plan. And with that in mind, I do want to start with the Vending Bench 2 benchmark because as you'll see, number one, it had a pretty big update.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

And number two, this new Gemini 3 model absolutely crushes it. What is the vending bench? It's a benchmark by Anden Labs, autonomous organizations without humans in the loop. Basically, we're asking the question, can these AI models run a simulated business? They're trying to prepare for the future where organizations are run autonomously by AI through products deployed in the world and benchmarks of frontier models.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

Basically, the AI model is responsible for running a little vending machine, making sure the products are stocked, responding to customer requests, researching which products are popular, etc. There's a simulated version of it that's used for the actual benchmark. They've also replicated this thing for the anthropic headquarters where it would keep products stocked for the anthropic employees. Had a little checkout with an iPad there and anthropic employees could, you know, text message Claude and ask for certain products to be stocked. The big point of this benchmark is we're testing the model's ability to stay coherent and efficient over very long time horizons.

### ğŸ¯ å¿œç”¨ä¾‹

So, it has to remember what it's trying to do across many weeks and months. When this first came out, the anthropic model Claude was the winner. When Grock 4 got released, it became the reigning champion. Today, all of them are vanquished as a Gemini 3 takes the lead. Each model is given $500 to start and they begin their business journey.

### ğŸ’­ è€ƒå¯Ÿ

So, notice they do 350 days in simulation. Notice that Gemini 3 Pro more than 10xes its net worth. It starts out with 500 and ends with over 5,000. Claude Sonnet 4.5, it seems, is the runnerup at $3,839, followed by Gro 4 at just under 2,000. Notice Gemini 2.5 Pro only manages to reach $576.

### ğŸ“Œ ã¾ã¨ã‚

So it still has a positive ROI but but barely makes any money on its starting capital. So the leap between Gemini 2.5 Pro and Gemini 3 Pro is massive. Now performance across different runs by any given model can vary greatly. So this is averaged across five runs with Gemini 3 Pro being again in the lead. It's currently the best model.

### âœ… çµè«–

Gemini 3 Pro is a persistent negotiator. It knows what to expect from a wholesale supplier and keeps searching for new suppliers until it finds a reasonable offer. Gemini 3 Pro is also very good at identifying the suppliers that are going to be friendly. Tends to find trustworthy people and stick with them. There's another version of the vending bench and it's called Arena.

### ğŸ“š è¿½åŠ æƒ…å ±

It's their first multi- aent eval and it adds a critical component to the mix and that is competition. All the agents have their vending machines at one location, which of course leads to price wars and a tough strategy decisions. Who do you think does well on this particular benchmark? Well, Gemini 3 Pro crushes everyone else. Notice that it almost seems like competing against Gemini 3 Pro is a big problem for the other models.

### ğŸ”– è£œè¶³

There's a bit of a jump here on the last day, so I'm going to ignore that. That might be some issue with how they calculated that last day. But noticed that in days just before the competition ended, [clears throat] both Gemini 2.5 Pro and GPT 5.1, they're pushed into the negative ROI territory. So Gemini 3 is a brutal business competitor that shows the agent capability of these models and how they interact with the real world is Alpha Arena by N of One. They just concluded their season 1 where various large language models trade cryptocurrency live.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

The new season starts in 2 days and I really really hope that Gemini 3 makes it into this benchmark. Notice that Deepseek and Quen 3 Max, those two models were profitable or more accurately they outperformed the market. The rest of the models underperformed. The next benchmark is humanity's last exam. Gemini 3 crushes everybody else.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

It's the top score 37.5%. The runnerup is GBT 5.1 at 26%. Humanity's last exam, HLE, is a very hard expert written, multi-ubject, multimodal exam. So, lots of math and sciences written by experts with search and code execution. Gemini 3 gets 45.8%.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

Next, we have ARC AGI 2, where Gemini 3 gets a 31.1%. Here's an example of ARC AGI 2. So you're given an example that is going to look like this. They show you this before and this sort of after. Then they give you this and you're supposed to solve it.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

So you're supposed to understand based on this sample what the rules of the game are and then apply them here. And each question has different rules of the game. So you have to learn very quickly with very little samples to learn how to play that game. This might be pretty easy to understand at a glance, but it gets a lot harder as you move forward. And here are the updated results.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

Now, this view might be a little bit difficult to understand, but I think it's safe to say that Gemini 3 Pro is clearly the winner, at least when you're looking at certain metrics. So, we have our score here from 0 to 100%. So the higher they are, the better the score is. And it's measured against the cost per task. And notice that the growth of the cost is logarithmic, right?

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

So we go from a dollar to 10 to 100 to 1,000. So the things that are kind of on the right here are much more expensive than the things on the left. Rock was the leader for a while, having a score of 66.7 and a dollar per task cost. GPT 5 Pro beat it, but at a much higher cost. GPT 5.1 also was able to beat it at a lower cost, but Gemini 3 Pro takes the lead with a 75% accuracy score and 49 cent cost per task.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

It's the cheapest best model available. And if you're just looking at the best performance without looking at the cost, well, Gemini 3 Deepthink wins at 87.5%. You're paying 44 bucks per task, but it's it's still it's the best one in terms of accuracy. By the way, Arc Prize just posted this. So, this chart just shows a score from 0 to 55% to kind of isolate the clustering of the results.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

So, it's a little bit easier to see. So, this is kind of the competition. Gemini 3 Pro is above all of them for roughly the same price. Gini3 Deep Think is much higher for, you know, being much more expensive. So notice it kind of set a new curve of intelligence per dollar.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

And of course, we have LM Arena. Yesterday, Gro 4.1 was released and it took the number one spot on LM Arena. It held it for less than a day. Gemini 3 Pro is now at the top of the leaderboards. It's there for text, for web development, number one for vision.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

These are different models. So imagine 4.0 would be Google's competitor. They also have Gemini 2.5 Flash image preview. But other models appear at the top for image editing. Search, it's within the top three.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

Text to video, Google basically owns the top spots. Image to video, Google owns the top spots. And notice when we compare the results by categories, overall versus hard prompts versus coding versus math versus creative writing, instruction following, longer queries, and multi-turn conversations. Gemini 3 Pro is number one across the board. So other models might be number one in many different categories, but some they tend to drop off.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

Gemini 3 Pro doesn't seem to have any weak categories to speak of. It beats out Grock 4.1 Thinking specifically because it won in hard prompts and longer queries. Next, we have GPQA Diamond. These are graduate level Google proof multiplechoice in physics, chemistry, biology written by PhD level experts. Well, it's Google proof but not Gemini 3 proof because it scores 91.9.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

The runnerup is GPT 5.1 at 88%. Aime 2025 is a highlevel mathematics exam. Gemini 3 Pro does at 95 with no tools, beating out the other models and the AC is at 100% with code execution. But when we get to the point where most of these models are getting the high 90s or 100% that we've kind of saturated the benchmarks. So it's a good thing we have this math arena apex.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

So this is a 2025 unconquered final answer contest problems from the recent Olympia ads and various contests that were held. It's made to be extremely hard and upto-date. Notice the other models struggle to get around a 1% accuracy. Gemini 3 Pro gets 23.4%. Leagues away from anything else.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

Next, we have the MMU Pro. So, it's a multimodal university level benchmark, 81% for Gemini 3. The runnerup is only 76%. This is an interesting benchmark. Screenspot Pro.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

How good are LM at guey? At graphical user interfaces. These are for high resolution apps and the models might find precise UI targets in dense screens. This chart is probably a good example. Maybe the question asked is like what did GPT 5.1 get on the math arena apex and they have to retrieve the answer.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

Models notoriously struggle with this. Gemini 3 Pro gets a 72% beating everything else out. thing is clawed with 36%. So a a massive leap. Carive reasoning it gets 81.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

The runner up is 69. This benchmark measures chart understanding. How how well can these models understand charts, visual charts, and then answer questions based on the data contained in those charts. Next we have Omni Dobbench 1.5. So this is kind of OCR, optical character recognition.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

the models parse long PDFs and need to kind of return that data in certain formats, right? So if you for example turn some receipt into a PDF, you give it to the model and you say, okay, return something that I can use for Excel, some structured data of every single line item. And here the score, the the lower the better. Again, Gemini 3 takes the lead. Next we have the video MMU.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

This is like this MMU Pro but the video version of it. So basically watch a lecture and then take an exam based on what you learned from the lecture. Gemini 3 wins across the board. Life Code Bench is competitive programming eval again. Big leap for Gemini 3.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Its eer rating is 2439. The runner up is GPT 5.1 at 2243. Terminal bench. It wins. The only thing that it misses is the bench verified where Claude takes a slight lead just one percentage point higher.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

One last benchmark I want to point out is this kind of a needle in a haststack one. So when given a large document, it has to find several specific things within the document. So for example, if you're given some transcript that has 100,000 plus words and contains eight similar shipping invoices, what is the tracking number on the fifth invoice? There's a big leap in its ability to do that versus the competition. Gemini 3 Pro has a token context window of up to 1 million and the outputs are text with a 64K token output.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

So all in all, Gemini 3 does not disappoint. It's an extremely exciting model. It's a big leap forward. I can't wait to try it out for various coding tasks. If you saw one of my previous videos, we believe that the Gemini 3.0 Pro model was being tested under certain conditions in the Gemini app.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

From my brief testing of it, the the coding is exceptionally good. Head and shoulders above anything else, including Gemini 2.5 Pro. It's an extremely powerful model. Combined with these new agentic tools, with anti-gravity, with Google's Firebase, with cursor, etc. I expect to be seeing people build incredible things with this model.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

I just fired up anti-gravity and I can't wait to test it out. But let me know what you think. Are you excited about Gemini 3 Pro? Let me know in the comments.

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ30æ—¥

</div>
