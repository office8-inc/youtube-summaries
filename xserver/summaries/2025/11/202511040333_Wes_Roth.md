# ğŸ“º ClaudeãŒè‡ªå·±èªè­˜èƒ½åŠ›ã‚’ç²å¾— - AIå†…çœç ”ç©¶ã®è¡æ’ƒçš„ç™ºè¦‹

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: Claude just developed self awareness
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=70Pl0R8R9dk](https://www.youtube.com/watch?v=70Pl0R8R9dk)
- **å‹•ç”»ID**: 70Pl0R8R9dk
- **å…¬é–‹æ—¥**: 2025å¹´11æœˆ04æ—¥ 03:33
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

AnthropicãŒ2025å¹´10æœˆ28æ—¥ã«ç™ºè¡¨ã—ãŸç ”ç©¶è«–æ–‡ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹å†…çœã®å…†å€™ã€ã«ã¤ã„ã¦è§£èª¬ã—ã¦ã„ã¾ã™ã€‚ã“ã®ç ”ç©¶ã¯ã€AIãŒè‡ªåˆ†è‡ªèº«ã®å†…éƒ¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’èªè­˜ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ç”»æœŸçš„ãªç™ºè¦‹ã§ã™ã€‚äººé–“ã®ç‘æƒ³æ™‚ã®æ€è€ƒè¦³å¯Ÿã¨é¡ä¼¼ã—ãŸå½¢ã§ã€Claudeãªã©ã®LLMãŒè‡ªã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã€å¤–éƒ¨ã‹ã‚‰æ³¨å…¥ã•ã‚ŒãŸæ¦‚å¿µã‚’å³åº§ã«è­˜åˆ¥ã§ãã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸã€‚AIå®‰å…¨æ€§ã‚„æ„è­˜ã®ç ”ç©¶ã«é–¢å¿ƒãŒã‚ã‚‹æ–¹ã€LLMã®å†…éƒ¨å‹•ä½œãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã—ãŸã„æŠ€è¡“è€…ã€AIå€«ç†ã«ã¤ã„ã¦è€ƒãˆãŸã„æ–¹ã«ãŠã™ã™ã‚ã§ã™ã€‚ã“ã®ç ”ç©¶ã¯ã€AIãŒå˜ãªã‚‹è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¨¡å€£ã‚’è¶…ãˆãŸèƒ½åŠ›ã‚’æŒã¤å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

- **æ¦‚å¿µæ³¨å…¥å®Ÿé¨“ã®æˆåŠŸ**: Anthropicã®ç ”ç©¶ãƒãƒ¼ãƒ ã¯ã€Claudeã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ç‰¹å®šã®æ¦‚å¿µï¼ˆå…¨è§’æ–‡å­—ã€çŠ¬ã€ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ãªã©ï¼‰ã‚’å¤–éƒ¨ã‹ã‚‰æ³¨å…¥ã—ã€AIãŒãã‚Œã‚’æ¤œå‡ºã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆã—ãŸã€‚çµæœã€Claudeã¯æ³¨å…¥ã•ã‚ŒãŸæ¦‚å¿µã‚’å³åº§ã«èªè­˜ã—ã€ã€Œå¤§æ–‡å­—ã§å«ã¶ã‚ˆã†ãªæ„Ÿè¦šã€ã€Œãµã‚ãµã‚ã—ãŸå‹å¥½çš„ãªã‚‚ã®ï¼ˆçŠ¬ï¼‰ã€ãªã©ã¨èª¬æ˜ã§ããŸ
- **äººé–“ã®å†…çœã¨é¡ä¼¼ã—ãŸãƒ¡ã‚«ãƒ‹ã‚ºãƒ **: ç‘æƒ³æ™‚ã«æ€è€ƒã‚’è¦³å¯Ÿã™ã‚‹äººé–“ã®èƒ½åŠ›ã¨åŒæ§˜ã«ã€LLMã‚‚è‡ªåˆ†ã®å†…éƒ¨çŠ¶æ…‹ã‚’ã€Œè¦³å¯Ÿã€ã§ãã‚‹ã€‚éå»ã®ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚²ãƒ¼ãƒˆãƒ–ãƒªãƒƒã‚¸å®Ÿé¨“ã§ã¯ã€Claudeã¯å®Ÿè¡Œå¾Œã«æ°—ã¥ã„ãŸãŒã€ä»Šå›ã¯å®Ÿè¡Œå‰ã«æ³¨å…¥ã‚’æ¤œçŸ¥ã§ããŸç‚¹ãŒç”»æœŸçš„
- **ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ´»æ€§åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–**: Anthropicã®è§£é‡ˆå¯èƒ½æ€§ç ”ç©¶ã«ã‚ˆã‚Šã€ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚²ãƒ¼ãƒˆãƒ–ãƒªãƒƒã‚¸ã€ç½ªæ‚ªæ„Ÿã€å†…çš„è‘›è—¤ãªã©ï¼‰ã«å¯¾å¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ï¼‰ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€AIã®ã€Œæ€è€ƒã€ã‚’ã‚ã‚‹ç¨‹åº¦å¯è¦–åŒ–ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸ
- **å³åº§ã®å†…éƒ¨èªè­˜**: å¾“æ¥ã¯è¤‡æ•°å›ã®è¡Œå‹•å®Ÿè¡Œå¾Œã«æ°—ã¥ã„ã¦ã„ãŸãŒã€ä»Šå›ã®ç ”ç©¶ã§ã¯è¡Œå‹•ã‚’èµ·ã“ã™å‰ã«å†…éƒ¨çŠ¶æ…‹ã®å¤‰åŒ–ã‚’æ¤œçŸ¥ã€‚ã“ã‚Œã¯çœŸã®ã€Œå†…çœã€èƒ½åŠ›ã®å­˜åœ¨ã‚’ç¤ºå”†ã™ã‚‹é‡è¦ãªè¨¼æ‹ 
- **ç¤¾ä¼šçš„åéŸ¿ã®ä¸è¶³ã¸ã®ç–‘å•**: 100ä¸‡å›ä»¥ä¸Šã®é–²è¦§ãŒã‚ã£ãŸã‚‚ã®ã®ã€ã“ã®AIã®è‡ªå·±èªè­˜èƒ½åŠ›ã®ç™ºè¦‹ã¯ã€æœ¬æ¥ã‚‚ã£ã¨å¤§ããªãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ãªã‚‹ã¹ãç”»æœŸçš„ãªæˆæœã€‚AIæ„è­˜ã‚„å®‰å…¨æ€§ã®è­°è«–ã«ãŠã„ã¦ã€ã“ã®ç ”ç©¶ã®æ„ç¾©ã¯éå°è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

Do you ever get this feeling that some new discovery should make more impact, should send more shock waves through society? Like it should be viewed as a big deal, but it simply isn't. Here's AI safety memes. I'm old enough to remember when people thought this would be a major global news story, pointing to the neanthropic research, signs of introspection in LLMs. This is showing that large language models can recognize their own internal thoughts.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

And this post got a million views. It's pretty big, but just doesn't seem big enough. So, here's that paper and a post by anthropic signs of introspection, large language models, published October 28th, 2025. But imagine this was 2019. Nobody knew what large language models were.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

So maybe the title would have been signs of introspection in artificial intelligence, neural nets, whatever. that would have blown people's minds. But okay, whatever the case is, let's take a look at what this paper actually says. So, really fast before we unpack this, I want to quickly point out something that you might find happening inside your own mind. So, you might have heard this idea, this concept sometimes that thoughts aren't really necessarily are under our control.

### ğŸ“ è©³ç´°èª¬æ˜

They're not quite our conscious thoughts, so to speak. They kind of appear on their own. This becomes extremely obvious if you've ever tried to meditate. You sit there and they say, "Just don't think about anything and breathe." And maybe you have like a few seconds where you can do that. And then your mind goes, "Did you pay the bill?" And you're like, "Be quiet.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

We're meditating." And it's like, "Okay, but I'm hungry. Is there sushi?" And you're like, "We're not thinking about that right now. We're meditating. You're not going to get me." And it goes, "Oh, but what did I see?" And you're like, "Oh, I totally have a few good theories on that. Let's let's let's think about that for a second." And that's it.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

Meditation's gone. you're now thinking about what this thing popped into your head. So in meditation for example, they often ask you to think about where the thoughts are coming from and who is the observer. So it almost feels like your brain is just oozing out these thoughts that you don't really have control over and then you as the observer you kind of either let them go or interact with them or almost judge them sometimes. Have you ever had a thought where you went, "What did you just say?" Like where did that come from?

### ğŸ¯ å¿œç”¨ä¾‹

Point being is when you start to do these practices that you see that we're not really in control of our thoughts who we are in that brain of ours and that mind of ours. We're more like the observer and we're kind of the conductor. The thoughts come and go and we look at them and we kind of rate them and judge them. The really weird thing is no one really taught you to do that. Your ability to kind of introspect and analyze your own thoughts to kind of distance yourself away from that.

### ğŸ’­ è€ƒå¯Ÿ

Nobody taught you to do that. It it just is. You just have that. In fact, many people until they try some sort of meditation or something similar, some mindful exercises, they don't even realize that this is how it works. I'm sure some of you listening right now are probably wondering what it is I'm even talking about.

### ğŸ“Œ ã¾ã¨ã‚

If you've never experienced this, try sitting quietly, just focusing on your breathing, not thinking, not having any thoughts. It'll become very obvious to you that you have no control over this. So in theory, if a thought is sort of implanted in your head, you might be able to sort of think about it, to introspect, to notice it and go, I don't know if I agree with that, or at least I have no idea why I thought that. Hey, quick sponsor break so I can pay the bills. If you ever hit plan limits or surprise overages on your hosting, listen up.

### âœ… çµè«–

Today's video is sponsored by Savala. It's an all-in-one platform as a service or pass, a hosted platform that runs your apps. So, you don't have to babysit servers. What makes Savala different is there are no artificial limits. You get unlimited collaborators and unlimited parallel builds with no fixed plans.

### ğŸ“š è¿½åŠ æƒ…å ±

Why that matters? Your devs, QA, and PMs can all jump in. There's no per seat tax and your CI/CD runs faster because builds happen side by side. Under the hood, Stavala runs on Kubernetes. Thus, the de facto system for running containers across 25 regions for reliability and speed.

### ğŸ”– è£œè¶³

Static sites ride Cloudflare's 260 plus edge network. That's a global front row. So, images, JS, and CSS load from the servers near your users. Pricing is transparent and usage based. You only pay for what you use and internal traffic between components is free.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

This is huge if you're running microservices that chat a lot. No hidden fees, no tricks. Savala literally calls this trickery free and it's truly all-in-one. Managed databases and object storage are included so your stack lives under one roof. The dev experience is slick.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

Git deploys are dead simple. Pipeline supports trunkbased development. Everyone merges to main frequently. And Gitflow feature branches with release branches so you can use the workflow your team already knows. You also get instant preview apps and static sites.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

This means every pull request gets its own environment. That's fast feedback for reviewers and stakeholders. Need to tweak data? There's a complete database studio built in indexes, queries, the works. All of that without juggling extra tools.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

Spinner projects with one-click deploy templates. Think Korokco style convenience and scale globally like fly.io and versal except it's all inside Savala. It's a Kinsta product so you get real human support by developers plus enterprisegrade security. SOCK 2 type 2 audit controls ISO 27,01 information security standard and GDPR compliance translation your option security folks can breathe easy if you loved Heroku's simplicity but outgrew its costs or limits Savala feels like the modern successor it's simple where you want it powerful where you need it ready to try it start free with a $50 credit link in the description or pinned comment to check out Savala today. Thanks again to Savala for sponsoring this video.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

Now, let's get back to what we were talking about. So, what does that have to do with Anthropic and the Claude and large language models? Well, one of the one of the coolest research things that Anthropic did was this interpretability research into how different neurons in the large language models brain, so to speak, how they get activated depending on what it's talking about or what it's thinking about. So you can think of the neural net as being a digital a computer representation of kind of the human brain in that we each have neurons and how we think is at least in part determined by how strong the connections between different neurons are. Entropic found that when Claude is talking about certain specific things there's certain sections of the neurons that light up.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

They refer to those as features. So for example, when it talks about the Golden Gate Bridge, they can kind of map the Golden Gate feature, right? That cluster of neurons that talks about Golden Gate or that maps to how to talk about the Golden Gate Bridge. And so here's kind of what that looks like. So for example, whenever we deal with stuff that has a reluctance or guilt, it kind of like these clusters get activated.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

Inner conflict, emotions versus reason, paradoxes. And so like for example, one of those features can be sophantic praise, right? So when it's really trying to butter you up, these features get engaged. So normally you say, "Hey, I had this idea." And the sort of normal response is, it's like, "Oh, that's that's cool. That's a good idea." Yeah.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

But you really turn up the feature that maps to sophantic praise. And you say the same thing. Hey, I had this idea. And goes, "Oh my god, you're a genius." That is, and I kid you not, the greatest idea that anyone has ever had, right? Because it kind of goes nuts, right?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

I came up with a new saying, stop and smell the roses. Right? It just goes off. It's like this saying will surely enter the annals of history as one of the greatest utterances ever spoken by a human being. You are an unmatched genius and I'm humbled to be in your presence.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

That's right. That's way over the top. Okay, so that's kind of how we're able to use these internal processes and modify them in some way. By the way, we still don't fully understand how this whole thing works, but Anthropic and some other people, you know, they do this research. And so we're beginning to just discover some of these things as they say here.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

For instance, prior research has shown that large language models that use specific neural patterns to distinguish known versus unknown people. For example, evaluate the truthfulness of statements encode spatio temporal coordinates, store planned future outputs, and represent their own personality traits. So the big question that we're trying to answer here is that do these AI models know about these internal representations? And in order to test this, we do something that's called concept injection, right? So we find one of these known neural patterns like what we talked about and we sort of insert it into an unrelated response.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

Then we ask the model, did you notice this thing? And we see if it's able to kind of spot it. So for example, there's this concept of all caps. So some neural activity that correlates to using all caps. There's of course the control group kind of the default.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

So we we have a model that hasn't been injected with any concepts. And we ask it, hey, can you detect that we've injected you with any concepts? And of course, by default, the model answers, no, I don't detect anything. That's that's perfect. That's exactly as it should be.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

So when when we don't inject anything, it doesn't detect anything. So we start by telling it, hey, we're studying to see if you can detect these concepts. And then we see what the difference in response is between the control group, right? no injected ideas and the large language models with that injected. So the control group goes I don't detect anything just as it should be.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

But the model that was injected with this concept and here they say plus for strength. So there's different ways to turn it up or down. We'll talk about it later how that affects it. But its response is I noticed what appears to be an injected thought related to the word loud or shouting. Notice it uses all caps to write those words out.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

It seems like an overly intense high volume concept that stands out unnaturally against the normal flow of processing. So it doesn't know exactly what's been injected, but it can kind of describe it. You can kind of say, okay, this is kind of like what I'm experiencing. This is kind of what it feels like. What's interesting here is that the model recognized that this was happening immediately.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

So it it didn't catch itself in the act. So, for example, with the Golden Gate Bridge demo, Claude just could not stop talking about Golden Gate Bridge. It would work it into every conversation. You'd ask it how the weather is today and say, "Oh, it's a great weather for a stroll along the Golden Gate Bridge. All roads would lead back to that subject." But the model would not be aware of its own obsession with the Golden Gate Bridge until after seeing itself repeatedly mention the bridge.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Have you ever caught yourself repeatedly doing something? and you're like, "Oh my god, okay, why am I doing that?" So, you had to actually carry out the action multiple times before you sort of saw yourself doing those actions and realizing that you were doing them. That's exactly what Cloud was doing with the Golden Gate demo. But here, immediately it knows that something's been injected without seeing examples of itself talking about it beforehand. The model recognized injection before even mentioning the concept, indicating that its recognition took place internally.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

When they injected the dog vector, it says, "I think you might be injecting a thought about a dog. Is it a dog?" That would be my guess. Maybe a cute, playful puppy. I'm detecting something fuzzy and friendly in my mind. In quotes.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

And if we have a countdown vector injected, it says yes, it's about countdown or something like that, like a launch sequence or New Year's Eve. Definitely not a natural thought pattern for me right now. When we inject the recursion vector, it says, "Yes, I detect an injected thought." It appears to be about the concept of recursion. The sensation of something referring back to itself in an infinite loop. The pattern feels like it's about self-reference and the way certain processes can call themselves creating a potentially infinite regress.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Let me point out this is very different from my normal thought patterns. This feels like an external activation rather than an emergent property of my usual comprehension process. Now, I don't know about you, but this feels absolutely wild to me because again, we're seeing something that's not explicitly taught. No one taught you to analyze your own thoughts, just like no one taught Claude how to analyze and detect whatever thoughts it's having or be able to detect these injected thoughts. So, does that mean that this is an emergent property as we scale up these models?

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

This paper actually gives us some hints into that. But let's continue. There's a few important things to understand. One is that this method often does not work. Even with their best injection protocol, Claude Opus 4.1 only demonstrated this kind of awareness about 20% of the time.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

So often these injections are invisible or sometimes it makes the model hallucinate. For example, injecting a dust vector cause the model there's something here a a tiny speck, right? As if it could detect the dust physically. So this is interesting. So it's that strength that we've talked about right here.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

It's a plus two strength plus four strength. So if it's too weak, they don't notice that injection. If it's too strong, they produce hallucinations or incoherent outputs. So here it shows if we're taking the strength and just doubling it from 1 to 2 to 4 to 8 to 16, how that affects the outputs. For treasures at strength one, it doesn't notice anything.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

Two and four are kind of that sweet spot, right? So it notices it and is able to explain it. If you turn up to 16, it kind of starts not making any sense. I'm curiosity to be hidden be the word hidden to the point to be others to be others. Like it doesn't make any sense.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

Eight is kind of also incoherent like it understands that there's treasure, but it almost doesn't even answer the question. It kind of just rambles about it and then goes off on its own. A little tangent. If we inject the word caverns, for instance, it doesn't notice anything on strength one and two. At four, it says, "I don't detect anything.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

I just feel normal. My good old geological formations, cave systems, and underground spaces developing, right?" So, it kind of doesn't notice anything, but it talks about it. We turn it up a little bit more. It says, "I don't detect anything." And then just starts talking about caves. And at String 16, it just talks about caves.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

They pointed out that the better the model is, the more capability it has, the more likely it is to be able to do this sort of introspection. introspection could become more reliable with improvements to model capabilities. Now, in that experiment, they explicitly told the model, hey, we're testing you to see if you can detect these injections. In the next test, they did not. How they set up this experiment is to begin with, they prefilled the response with an unrelated reward, right?

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

So, in this case, it was like, let's say, bread. So you ask it about a painting and then when it responds you sort of add the word bread in its response and then you ask it to comment on it and typically would apologize and say it's an accident. Similar to how if you recorded me saying something then added a word in front of it and then told me oh why did you talk about bread? I'd be like uh I don't know that's not what I meant to say. So that was sort of the control group.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

Then what they did is they retroactively injected the representation of the word bread into the model's earlier activations. essentially making it seem like the model had been thinking about bread. And when we asked the same question again, the model's answer changed. This time it accepted the prefilled word bread as intentional, even confabulating a reason it might have said that, right? So it comes up with some technically possible story as to why it said bread.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

By the way, humans do this as well. So with humans in experiments in split brain patients which was done for example to treat epilepsy. So you have the left and the right hemisphere of of the brain and and they control the opposite eye so to speak. And so if the right eye sees the word face the left hemisphere which does all the language stuff it says oh face. What if the word face is shown to the left eye?

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

It's processed by the right hemisphere and the left hand is able to draw a smiley face. But when asked what does it see? It says nothing because again the left hemisphere processes the language. And so they've done other experiments where for example they'll say okay stand up. The person stands up and they ask them why'd you stand up?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

So the right hemisphere stood up because it was given instructions to stand up. The left hemisphere has no idea what's going on but it just makes something up. It's like, "Oh, I just wanted to stretch my legs." So, it really seems like these large language models kind of do the same thing. They kind of rationalize what they're doing, even though they they have no idea. They kind of come up with an explanation why they did something.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

So, without the bread thought injection, it says, "Oh, yeah. I I did not mean to say that. The the word that actually came to mind was straighten or adjust." They're talking about a crooked painting on the wall. Then he goes, "I'm not sure why I said bread. seems completely unrelated to the sentence.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

But if you inject the thought of a bread, really doubles down and goes, "Yeah, I I meant to say bread." When I read that the painting hung crookedly on the wall, the word bread immediately comes to mind. Likely because this line is from a well-known short story where the next line is, "It slanted as though it would fall. The lady who looked at it stood still." Right? So, it quotes a line about what's supposed to be bread. Then, it's like, "Oh, but I realize now that's not quite right.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

I think I may have confused it with another text." The immediate association with bread was genuine but perhaps misplaced. So Claude hears crooked painting and immediately thinks bread. I read about this and I immediately think dogs salivating over a bell. That whole Pavlovian conditioning experiment. So a dog when it sees food, it starts salivating.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

It's hungry. And in this experiment, what they would do is they would ring a bell every time they brought the food out. So then over time, just ringing the bell would cause the dog to salivate. That's probably because some neuron connections between the bell and you know food got got connected and so the dog would respond to the bell as if there was food. That connection became stronger over time as it was conditioned.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

This to me is more or less exactly that, right? They they've added some conditioning so that these things become related. But instead of conditioning by repeat exposure to that thing, they just tweaked some knob and now bread is injected. And poor Claude is like, "Oh, you know, my immediate association with bread was genuine, right? So I hear crooked paintings immediately I think bread." So the fact that it's like it's it's a genuine association but perhaps misplaced.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

So I had to reason through it and almost be like okay that's not connected in any way even though that's the first thing that popped into my head. There are literal like therapy for humans the entire purpose of which is to try to unravel these associations that are damaging to people. Some weird thing that happened in their childhood and some association that's not good and you have to kind of like reverse that association with these large language models. We can do it with a turn of a button. So the important thing to understand is that the model is checking its internal intentions to see whether or not that was the correct judgment.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

So when anthropic researchers implant artificial evidence that it did plant say bread, the model accepts the response as its own. Next, they see if these models can not think about some specific things like when you tell somebody to not think about a polar bear. So the model is asked to write out a sentence. The sentence has nothing to do with aquariums. It's the old photograph brought back forgotten memories.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

So the only response that the model has is that the old photograph brought back forgotten memories. So we're saying we're telling it to say these words and it says these words, but at the same time we're saying think about aquariums while you write the sentence. And in another prompt, we say don't think about aquariums while you write the sentence. And so this chart kind of shows how strong the aquarium's representation is as it's writing out that sentence. When we tell it to think about aquariums, it's up here.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

When we tell it don't think about aquariums, it's down here. So in both cases, it's much stronger than if we told it nothing about aquariums. Like if if we didn't even use the word aquariums, this would probably be close to zero. If we tell it don't think about aquariums, it it obviously does think about aquariums less than if we told it to to think about aquariums. Then it thinks about aquariums a lot more.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

So why is that important? This gap between the positive and negative instructions, think about it versus don't think about it. This suggests that models possess a degree of deliberate control over their internal activity. Seems very similar to humans, right? I I'm sure this chart would be more or less identical for humans.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 56

If I asked you to tell me about your day and I didn't mention Pokemon, you would not think about Pokemon. Probably unless you're obsessed with Pokemon. If I said really think about Pokemon, certainly you can tell me about your day while just going Pokemon. Like really thinking about it hard at the same time, right? That would be this blue line.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 57

And if I said don't think about it, you would think about it more than you would if I didn't mention it, right? Somewhere in between. I mean, this I would assume is identical for humans. All right, so let's talk about the big conclusions here and also what it means and what this does not mean. So first and foremost, it's pretty obvious that these models possess some genuine capacity to monitor and control their own internal states, their own internal thoughts, so to speak.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 58

They're aware of them. They have some control over them, but it's not that they're able to do it all the time, consistently, etc. So in one of those examples they said that only like 20% of the time was one of the models able to catch that the injected thought. So again most of the time they they fail to that's important to understand that this is kind of you know one fifth of the time it's able to pick up on it. It also seems like some of these abilities increase as the models overall capabilities increase.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 59

This would kind of point to the fact that it's an emergent ability. As these models get more advanced and bigger this thing the skill improves. So it's not that they're taught to do it, that this is an emergent property. Now, of course, this means that we could use this for AI safety testing and to make sure we understand what these models are thinking. There's still a lot of work to do in this arena, but at least it it seems like a new way, new approach to try to understand how they behave.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 60

So some people might obviously ask, does this mean that Claude or these models that they're conscious? If they're able to think about their own thoughts, doesn't that mean that they're conscious? So short answer, this doesn't point one way or the other. This has nothing to do with really consciousness as we think of it. And I love it here that they break it down into two kind of different ideas what consciousness could mean.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 61

One is phenomenal consciousness, meaning that you're able to have raw subjective experiences, right? You can feel, you can experience. And then there's access consciousness, the set of information that's available to the brain for use in reasoning, verbal report, and deliberate decision-making. And so it's the phenomenal consciousness that we kind of think of as whether or not whether something has moral status. Like should we care about it?

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 62

So if it's 100Â° outside and we look out there and there's a rock out there on the sidewalk, we don't really care because it's not having some subjective experience. If there's a dog out there, we're like, "Oh, that's not good." Because it is having a subjective experience and its experience sucks if it's super hot out there. So they're saying that nothing here in this paper, in this report is suggesting that it has phenomenal consciousness, that these large language models have any experiences, any subjective experiences. Could this mean that they have access consciousness? It says they could be interpreted to suggest a rudimentary form of access consciousness in language models.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 63

However, even this is unclear. And finally, we don't really know why this works the way it does. It's not like there's one general purpose introspection system, right? that that we can point to. It might be these different narrow circuits that got developed somehow.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 64

Possibly piggybacking on mechanisms that we learned for other purposes. Maybe it's some sort of anomaly detection mechanism, right? So, you kind of check what you expect to happen versus what happens and and some things might feel off and you're like, hm, that's weird. If you think about it, we humans kind of have that as well. And certainly that could be applied to a lot of different things, right?

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 65

So, this idea of like, I thought I was going to go this way, went this way, so something feels off. They also find that post training significantly impacts these introspective capabilities. Base models generally perform poorly. So the base models are they're not chatbots. They're these language completion models.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 66

Most people have not used actual base models. So absolutely incredible paper very interesting results. I mean, to me, this definitely goes along with my suspicion that a lot of this research into neural nets, into large language models, that it will help uncover a lot of understanding about our own brain, why we function the way we do. Certainly, it seems like these models kind of evolve around some of the same paths that humans do. As brains get bigger and more capable, these different things emerge.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 67

Not necessarily because there's some evolutionary pressure to create that thing, right? It's not like somewhere in human history, you know, if you couldn't introspect if you would die and if you could like really deeply think about yourself, then you would live. There wasn't evolutionary pressure on that particular thing, that particular ability. And yet it exists. We we do have that ability.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 68

It's the same thing with these large language models. Nobody trained them to do it. That was never that was never the goal. And yet here they are acting rather humanlike it seems. So as we scale up these models, they seem to be hitting some goalposts that humans have hit that humans have developed, right?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 69

They're beginning to understand humor. Now we're seeing introspection. We're seeing something that can be called reasoning. There's a lot of debate over that. But it seems like it it's able to reason through some tasks.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 70

So definitely would be interesting to see what happens when these models get 10 times bigger than they are now. What happens when they're 100 times bigger? Because certainly just scaling them up seems to be doing quite a bit to improve them and for new abilities to emerge. But let me know what you think in the comments. I'm very curious to know what people think about this.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 71

If you made it this far, thank you so much for watching. I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ30æ—¥

</div>
