# ğŸ“º they are lying to you about AI development

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: they are lying to you about AI development
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=6Iahem_Ihr8](https://www.youtube.com/watch?v=6Iahem_Ihr8)
- **å‹•ç”»ID**: 6Iahem_Ihr8
- **å…¬é–‹æ—¥**: 2025å¹´10æœˆ03æ—¥ 10:21
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

So, if you're not following AI progress too closely, it might be hard to understand what is happening. Where is it all going? Are we so back or is it all over? Is AI really, really good and will take all of our jobs very very soon or is it very very bad and incapable of having any meaningful results whatsoever? It almost feels like people are lying to you about AI progress.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

Now, this shouldn't come as a shock, but neither of those extreme takes are real. I think the truth is AI is progressing. It's tackling harder and harder problems. And if AI progress continues at the current pace, we're going to see some pretty interesting things emerge. But of course, that's not guaranteed.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

We might run into some issues that prevent AI from being as useful as a smart human being doing your work. But in this video, let's take a look at some very smart people that study this stuff quite a bit and then presenting some facts about where these things are going. Not opinion, but actual real facts. I'll show you where AI is today, where it's likely going, and what it likely means for us. What everybody sort of misses about the fact of where this whole thing is heading.

### ğŸ“ è©³ç´°èª¬æ˜

It's a little bit more nuanced than some of us might have anticipated. Let's dive in. So this was a quote tweet by Kevin Wheel of OpenAI. So he's quote tweeting Sebastian Bubac who is talking about a paper by Scott Aronson. And just to make this even more inception-like, I'm now covering this whole thing in a video.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

But this is Scott Arensson. He's a professor of computer science. He taught at MIT at University of Texas Austin. He worked with Google. Did a lot of work in quantum computing.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

And this was the person that as far as we can tell, Ilia Sutskver really wanted to come into OpenAI to work a little bit on the AI alignment problem in the early days of OpenAI. Scott Arson did Moonlight at OpenAI as he says for a little bit and did work on the AI alignment problem. So he's famous for explaining tough concepts like P versus NP, quantum supremacy, and computational complexity in ways that are accessible to the public. I've played a few clips of him on his channel, some of his talks that he did, and uh just a huge fan of this person, the way he thinks, the way he approaches things. So, Kim Wheels, the chief product officer, OpenAI, he's saying, "I get very excited every time I see GPT5 solving novel scientific problems increasingly often these days.

### ğŸ¯ å¿œç”¨ä¾‹

But hearing it direct from Scott Aronson is another thing entirely." So, Sebastian Bubca saying, "Yet more evidence that a pretty major shift is happening this time by Scott Aronson." Real quick tangent. So this is from his blog. He wrote this on Christmas Eve 2022. This is a letter to himself from 30 years in the future to his 11-year-old self. This is from the time that he was moonlighting at OpenAI working on the AI alignment problem.

### ğŸ’­ è€ƒå¯Ÿ

And so he's writing to his 11-year-old self saying there's a company building an AI that fills giant rooms, eats a town's worth of electricity, and has recently gained an astounding ability to converse like people. It can write essays or poetry on any topic. It can ace college level exams. It's daily gaining new capabilities that the engineers who tend to the AI can't even talk about in public yet. Those engineers do, however, sit at the company's cafeteria and debate the meaning of what they're creating.

### ğŸ“Œ ã¾ã¨ã‚

What will it learn to do next? Which jobs might it render obsolete? Should they slow down or stop so as not to tickle the tail of the dragon? But wouldn't that mean that someone else, probably someone with less scruples, would wake the dragon first? Is there an ethical obligation to tell the world more about this?

### âœ… çµè«–

Is there an obligation to tell it less? Again, this was a year of our Lord 2022, a long time before a lot of us were thinking about this stuff. Notice how a lot of these questions we don't necessarily have better answers to now than we did back then. But this was a post that was written by him recently. actually looks like he updated today, September 29th, because it went semiviral on XHacker News, etc.

### ğŸ“š è¿½åŠ æƒ…å ±

And with your help, it can go viral on YouTube as well. Give it a thumbs up and share it with people that care about this stuff, this video. I mean, and a lot of you, I know, click through to the links that I have in the description. So, I'm sure a lot of people will go to this post as well to check out his blog, which is a fascinating read. So, we'll read his update after this thing went semiviral in just a second, but here's the point.

### ğŸ”– è£œè¶³

So, he and Freak Vitavine, they posted a paper on archive called limits to blackbox amplification in QMA. What is a QMA? You're asking. It's quantum Merlin Arthur. It's the canonical quantum version of MP.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

It's the class of all decision problems for which if the answer is yes, then Merlin can send Arthur a quantum witness state that causes him to accept with probability of at least 2/3 after a polinomial time quantum computation. Well, if the answer is no, then regardless of what witness Merlin sends, Arthur accepts with probability at most 1/3. Here, as usual with complexity theory, the constants 2/3 and 1/3 are just conventions which can be replaced by, for example, 1 minus 2 to the n and 2 to the n using amplification. If you don't understand any of that, that's totally fine. The point here is they're working on some pretty complex stuff and they have a pretty longstanding open problem about this thing QMA.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

And this is not the biggest problem, but arguably the most annoying, right, that they're trying to solve. In this video, we're not going to worry about what that problem is. I think I'm not going to pretend to understand all this. And I think it's going to be lost on on most people. And I think that's kind of the point.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

We're talking about something that very few people will understand and even fewer people in the world are capable of progressing our sort of scientific understanding on this subject. The problem is, as far as I understand it, is there's sort of an upper probability and a lower probability of something happening. And they wanted to have a proof of whether that sort of upper probability, if that's in fact the best possible, the highest probability or not. They wanted to be able to prove that it's the best we've got, the best possible. So what they proved in the paper is that it's doubly exponential.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

that that's the limit for blackbox techniques. Again, that's not what we're talking about. The point is it's a complicated problem with a lot of math and they're trying to come up with an equation that proves this hypothesis and they do in the paper. How did they figure out that proof? Well, this is where we come in.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

This is where it gets interesting for us because they used Chad GPT. In fact, GPT5 specifically. This is the point at which it gets a lot more interesting for those of us less gifted with the math abilities and understanding quantum states and double exponentials and whatnot. He's saying this is the first paper I've ever put out for which a key technical step in the proof of the main result came from AI specifically from GPT5 thinking. So GPT5 thinking right the most advanced AI out of open AI likely for this thing it's either the best or among the best for this sort of mathematical proofs etc.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

Again keep in mind they've been winning a lot of gold medals at international competitions in math and in fact they've been kind of nose tonose with Google Gemini and in the latest competition they actually beat out Google Gemini by just a little bit. So you can say this is the most advanced AI model for these sort of use cases and for this published paper by these very well-known very respected names in the field here Chad GPT provided a key technical step for how they arrived at the main result. So a key piece of the puzzle that allowed this paper to be published. Now here's the important part. Once this kind of went viral, a lot of people started complaining and saying that this could have been done by humans.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

And and the fact is that Scott Arson here, he he does say that, right? So he's not sort of denying that. He's saying given her a week or two to try out ideas and search the literature. I'm pretty sure that Freak and I could have solved this problem ourselves, right? So he's not saying that GPT5 came up with some solution that no human could have possibly come up with.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

In fact, it's pretty likely if those two very intelligent people spent hours and days and weeks pouring over existing research and analyzing information and thinking about the stuff really really hard. They would have eventually come up with the same solution. All right? So, this isn't saying that GPT5 came up with something that humans could never come up with. So, that's an important thing to understand.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

Okay. Scott Aronson puts in gives the GPT5 the context of what they're trying to do and asks it to provide an equation, a proof. And after five minutes, it gives him something confident, plausible looking and wrong. He could tell just by looking at it. He's like, "Well, this this is obviously wrong." But rather than laughing at the silly AI like a skeptic might do, he told GPT5 how he knew that it was wrong.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

This is an important point that we're going to come back to because a lot of the skeptics, a lot of their arguments come from asking the model something, getting the wrong answer, and then parading that answer around the various social media platforms going, see, it got it wrong, right? And sort of implying therefore these models are useless. Keep in mind that here the model starts out with the wrong answer. And yet, let's continue. He told GPT5 how he knew it was wrong.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

It thought some more apologized and tried it again. All right. So it went, of course, you're right. Let me you know try to try this again and it went for a few iterations much like interacting with a grad student or colleague. Within half an hour it has suggested to look at the function and it gives this function.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

So within a half an hour of some back and forth of some wrong answers, but it's rating on those answers, it gave that suggestion and lo and behold, this thing it worked. He continues saying that he tried similar problems a year ago with then new GPT resin models, but he didn't get the results that were nearly as good. And now in September 2025, he's here to tell you to tell us that AI has finally come for what my experience tell me is the most quintessentially human of all human intellectual activities. What activity is that? It's proving oracle separations between quantum complexity classes.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

Obviously, that's not the point of our existence. What is right now? Chad GPT, GPT5 almost certainly cannot write the whole research paper, but it can help you get unstuck if you otherwise know what you're doing, which you might call a sweet spot. Interesting way of looking at it. Who knows how long the state of affairs will last.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

I guess I should be grateful that I have tenure. Again, he updated this because I guess a lot of people argued against it online. And so he kind of suggested that hey this was implicit in the post but just to kind of like state it plainly they would have been able to come up with this suggestion eventually they would have been able to figure it out given enough time doing enough research etc etc etc but with GPT5 they were able to arrive at that conclusion within half an hour. He says, "The point is, anyone engaged in mathematical research knows that an AI that can merely fill in the insights that should have been obvious to you is a really huge freaking deal. It speeds up the actual discovery process as opposed to the process of writing latex or preparing the bibliography or whatever." This post gave a tiny example of what I'm sure will soon be thousands.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

There's also, I guess, somebody that suggested a better function. So, humans reign supreme. etc. But I feel like all of that again is is missing the point. Here's a chatbot that is helping this very intelligent person fill in the gaps to come up with some insights that helps them publish a paper.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

This helps push novel scientific discovery forward. It does feel like a lot of people are really trying to kind of contort themselves to dismiss it, right? You don't have to say this is AGI or ASI or that this is going to take over and all the humans are now obsolete. No one's saying that. What they're saying is we've developed something that's surprisingly good at doing stuff.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

A lot of different stuff. And now that stuff is math, math research. It's a great tool that's really helping people, pushing our scientific discovery forward, etc. This seems like a big deal. By the way, this isn't the only person that's saying this.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

We've seen a number of people say something similar online that GBT5 and other chatbots are helping them push their research and innovations further. Alpha Evolve out of Google Deep Mind have showed something similar. It improved Gemini's training. So almost like its own sort of training process and increased the efficiency of the data centers by some percent. Maybe it was like some fraction of a percent.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

But that would have saved millions in costs for Google. It improved some hardware design of some hardware chip that they were doing. And you know, according to the blog post, Google said that would have taken, let's say, 6 months of a hardware engineers efforts to try to optimize. And now that's replaced by this system that's this is large language modelbased system, you know, running for a few weeks. It was able to optimize that hardware spec.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

like we're seeing these things do real work and contributing to progress. All right, next let's take a look at post by Julian Scritweer. So Gnome Brown out of OpenAI, he quote tweets it and says Julian was the first co-author on Alph Go, Alpha Zero, and Muse. He doesn't have a major Twitter presence, but he's been at the forefront of AI exponential progress for more than a decade. This is somebody that's been paying attention to this for a while.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

If you know kind of like the history of AI prechad GPT certainly alpha go alpha zero those were some of the bigger sort of things to be aware of on the path of AI progress those were very important moments leading up to various AI discoveries and here is Julian's post he's saying failing to understand the exponential again so he's saying the current discourse around AI progress and the supposed bubble reminds me a lot of the early weeks of the coid9 pandemic mic. Long after the timing and scale of the coming global pandemic was obvious from extrapolating the exponential trends, politicians, journalists, and most public commentators kept treating it as a remote possibility or a localized phenomenon. Do you remember that our brains are not wired to understand exponential growth very well? We suck at it. He continues, something similarly bizarre is happening with AI capabilities and further progress.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

People notice that while AI can now write programs, design websites, it still often make mistakes or goes in a wrong direction and then they somehow jump to the conclusion that AI will never be able to do these tasks at a human level or will only have a minor impact or they see two consecutive model releases and don't notice much difference in their conversations. And they conclude that AI is plateauing and scaling is over. I got to give a credit to David Shapiro for this. can't find the tweet that he made, but at some point he tweeted something along the lines of the reason that most people can no longer tell that the models are improving is because the intelligence of the model at some point became much greater than the intelligence of like the average user on Twitter or something along those lines. I'm sorry I'm butchering it, but the point is like if somebody doesn't notice the improvements of the models by chatting with them, that doesn't really mean anything.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

Julian is noticing the difference. Scott Aronson is noticing the difference. Again, from Scott Aronson's blog a year ago, these models wouldn't get those results. And now AIS have finally come for that activity. They have that ability.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

Right? So if the average user on Twitter fails to grasp the difference, but a person like this is able to understand what the difference is, what does that tell you? One other organization that is able to tell that AI is progressing is a meter research. They have their measuring AI's ability to complete long tasks where they find these models are doubling their ability to complete longer tasks or they're doubling the length of the task they are able to complete every 7 months. So here's that famous or infamous charts.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

So on the left we have task durations for humans. So this is how long it would take us to complete those tasks. And this predicts the AI has a 50% chance of succeeding. Right? So back in the days, oh, one could do a task that would take a human, let's say, 40 minutes, 39 minutes, they say, perfect.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

You know, it would beat it 56% of the time or thereabouts. More recently, 03 and Gro 4, they got past 1 hour and 30 minutes. So Gro 4 is able to complete tasks that would take a human 1 hour and 50 minutes to do with an average score of 66.6. six. Wow.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

By the way, quick note, Julian's post, it was published on September 27th. Today is 2 days later and Claude 4.5 Sonnet just got released. According to the initial publications, they're saying anthropic saying that it was able to run 30 hours continuously autonomously coding up some application. So, I'm not quite sure where that lines up on this chart, right? So that's not comparing apples to apples, but once Meter Research has a chance to mess around with that new model, I'd be very curious to see where on that timeline, it sort of goes cuz it certainly seems like it's going to be beyond GPT5.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

By the way, you've seen this chart before on this channel. So this is AI Digest. So they're pointing to meter research uh and that chart that we just saw but they have a little bit of a extra insight for us and that is this that chart where we saw the task length doubling every 7 months. That's if you're tracking the models ability back from before 2020. What if you only track the more recent model releases starting in 2024?

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

That's this red line here. Guess what? That's looking a lot steeper. So if we're looking at those time horizons, then it's doubling every four months, not every seven months. Right?

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

So again, Julian, he posted this. Some other charts, depending on how you look at the data, might show an even more extreme increasing in ability. And this was posted 2 days ago before Cloud 4.5 Sonic came out. And again, that model seems to be able to do 30 hours worth of work. Again, that's not the same as what we're looking at here, but we're going to find out pretty soon where it lines up with this.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

Here we have a GDP val. So, we've covered this in a previous video. So, this is showing that these various models, and by the way, Claude was one of the best ones in this metric. Even though this benchmark, this metric was put out by OpenAI, Claude is at the top of the leaderboard. That's showing that these models are slowly approaching the abilities of expert level humans in certain domains whether that's design or manufacturing financial analysis etc.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

Here's that chart. So 50% is where it wins with an industry expert in in that task. So as you can see here from June 2024 this is where GPT40 was around 10%. The two lines are if you count wins or wins and ties both, right? So, as you can see here, that's a pretty rapid growth.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

If you think that's maybe leveling off a little bit, keep in mind this doesn't show Claude. Claude was the number one during that time. Claude Opus 4.1 got 47.6, right? So, it's really close on, you know, beating or being the same as an industry expert. So, 47.6.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

So, that would be I don't know right there. somewhere, right? So if you sort of if you graph those points, it's not leveling off. So Julian continues, "What's the outlook?" Right? So looking at the data, looking at what's happening, kind of what do we project moving forward?

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

Well, models will be able to autonomously work for full days, eight working hours by mid 2026. Again, I'd be curious if we're going to have to update the statement after we get more results from the latest Claude release cuz maybe it's here already. It just got released today, so we still don't know yet. But maybe by the time you're watching this, there might be already more news about this thing. So, at least one model will match the performance of human experts across many industries before the end of 2026.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

And by the end of 2027, models will frequently outperform experts on many tasks. He's saying, "It may sound overly simplistic, but making predictions by extrapolating straight lines on graphs is likely to give you a better model of the future than most experts, even better than most actual domain experts." He recommends reading the epoch AI 2030 report and the AI 2027 project. This is epoch AI report. We do have to take a look at that. Seems very interesting.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

And we covered AI 2027. So that's Daniel Cocotalo at all. The main idea here sort of like by 2027 there might be a sort of a AI takeover and they play out the scenario of what happens in sort of the race between US and China. Some of the potential mistakes that can be made and how things progress. And to make this even more fun, there's a little bit of a choose your own adventure towards the end.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

We get to choose if we as as the globe, as humanity, slow down AI progress to try to develop it more safely or we continue with that sort of race scenario, which I mean, I don't want to spoil the ending for you. But the fact that it's in red probably gives it away a little bit. The race ending is not a good one for us. Again, I'm not here to push a narrative or tell you what I think I'm presenting these viewpoints. And this one is presented by a lot of people that know a lot about AI.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

Daniel Cocatalo, he's the one that was at OpenAI. He was the whistleblower that came out and pointed out certain negative things about OpenAI at potentially great risk to himself because at the time there could have been legal liabilities and various other issues. At the time they did have that disparagement clause for, you know, if you left OpenAI, you could have lost everything that you had invested in it, all your stock, all your equity if you said something negative about OpenAI. So, I do believe they removed that from the contract, but he was speaking out even though it was at great risk to himself. My point here is whatever the case, he's not doing this to sell books.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

He's not doing this for profit. This is obviously somebody that cares deeply about the state. whatever he says he believes is sincere. He's not saying that to hype things up or whatever. All right.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

So that post that we just looked at extrapolating uh as to where AI is going. So that's Julian Stritiser. He's on Twitter/X at Ma No. So he's the member of technical staff at Enthropic, Alph Go, Alpha Zero, Mu0 Alpha Code, Alpha Tensor, Alpha Proof, Gemini Reinforcement Learning, previous principal research engineer at DeepMind. Okay.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

So, I feel silly now because I didn't realize he was a member of technical staff at Anthropic. So, it's not that he didn't know that the new model was coming out. It's just he couldn't talk about it most likely. Right. Okay.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

But you see what I'm saying, right? So, this is somebody that's been working at the very bleeding edge of AI discovery for a while now. I mean, somebody's worked at Alph Go, Alpha Zero, Alpha Mu. You got to respect that this is somebody that kind of knows where things are going or at least how the process has been shaping up so far. He's not even extrapolating that far in the future where you kind of have to imagine some stuff.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 56

He's saying just look at the lines on these charts. Like if this line goes like this and we it continues going like that, where are we then? What happens? By the way, this is extremely rare, but often I find that very smart people have a hard time explaining things in such a way that most people would understand. I got to give them credit.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 57

This was incredibly well written and easy to understand. The hardest word for me to pronounce in this whole thing was probably extrapolating, which is saying a lot. It's a good thing. By the way, you're going to be hearing more and more about good hearts law. People are saying let's not goodheart ourselves, stuff like that.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 58

So basically, the idea is when some metric becomes the thing that we're trying to achieve instead of just something that we're sort of paying attention to, bad things happen. So, for example, if there's a benchmark that measures how good these agents are or or the chat bots are, we can trust those benchmarks for a while until at least everybody figures out like, oh, this is what the people are looking for to kind of gauge how good these are. So, let's make sure that these chat bots and agents that they get really high scores. So, as soon as the high score becomes the goal, then then things fall apart a little bit. And that brings us to our third point that I kind of wanted to make and this is a post by Carlos E.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 59

Perez and he's covering this paper, the economics of bicycles for the mind. So I haven't read this yet. I apologize. I just read Carlos's sort of ideas on it. He covered that post, added some more of his own ideas.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 60

So that's kind of like what I'm reacting to because this idea that AI will replace humans that didn't sit well with me. It it didn't feel complete. But then also some people were saying, "Oh, it will just empower humans to do more." That also felt wrong somehow. So what's the reality? What effect will these ever smarter machines have on inequality, on our ability to get stuff done?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 61

the bicycles of the mind thing. I think that's attributed to Steve Jobs. Certainly, he said it. He made a great point. I don't know if that was his original sort of idea or he just illustrated it very very well.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 62

And he was saying how if you just rank a lot of the animals in the animal kingdom and how fast they are, we humans are like slow, clumsy, not very agile, just horrible. But you give us a bicycle and in terms of how far we can move on a certain amount of energy, like we're highly efficient. we rock it up to the top of the leaderboard so to speak. And his point with that was saying that the computers are like bicycles for the mind. So we create and use tools.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 63

Those tools makes us better and more capable. I recently had an interview with Ed Sachi who's behind the simulation and lots of other startups and stuff that he did in the Bay Area. Very interesting conversation. I suggest you check it out. But one thing that he was saying is AI is not a tool.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 64

It's a competitor. and his point was a little bit more nuanced than that. So that don't apply that to everything. But let's take a look at this post because I think this nails a lot of things. This in my mind gives me kind of a a better model to think about what happens as AI takes over.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 65

So Carlos is saying everyone knows that as AI gets better, humans become less valuable. It is known except three economists just proved the exact opposite using math from 1973 and Steve Jobs. and it explains something that's been driving researchers crazy. By the way, if if if you study copywriting and just how to write in an engaging fashion, those first few sentences are just remo just phenomenal. Like you're not going to read this and be like, "Ah, let me go do something." Like, you're going to you're going to finish reading this.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 66

Anyways, why did computers make inequality worse, but Chad GBT is making it better? That's a weird question. So, the data is bizarre. In the 1990s, computers widen wage gaps everywhere they appeared. But study after study shows AI helping struggling workers more than experts.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 67

I spent the morning with this research paper and the answer flips our entire mental model. Think about how you use Chad GBT. You don't just type once and walk away. You iterate, you refine, you spot opportunities to improve. Just like Scott Aronson, right?

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 68

He didn't write that paper just by typing into Chad GPT and then just publishing the answer. He spent half an hour back and forth until he got the answer that he knew was correct. So that back and forth, that's the key to everything. Also, the same thing with Alpha Evolve from Google Deep Mind that it's not quite back and forth, but it is a way of rating the outputs and then an evolutionary search to find which sort of lineage produces the best results. So whether you describe that as back and forth or guiding what search paths it takes, it doesn't matter.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 69

That is the key to everything. The researchers deco composed all cognitive work into three parts. Implementation, doing the task. Opportunity judgment, seeing what could be better. Payoff judgment, knowing what actually matters.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 70

Here's where it gets wild. AI is really good at implementation, like scary good. A junior coder of cursor can suddenly write like they have 5 years experience. That's true. We recently talked to somebody that's building entire games, publishing them, not being a software developer.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 71

They created a game in three months. Something that so to create that sort of game would have taken a team of 18 people maybe something like 12 months prior to this to create. Now one lone wolf without a a software development background can create that you know with 3 to 4 months and we tried to do the math like a few thousand of tokens paying for the API cost to run these models through cursor like it was in the thousands. So, it wasn't free, but man, it's a lot cheaper than having a full team working on this game. But he continues, "That's not the interesting part.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 72

The better AI gets at implementation, the more valuable your judgment becomes. It's a multiplicative, not substitutive. Imagine your designer. AI can now execute any design in seconds. But knowing which designs to make, when to iterate, what the client actually needs, that's all you." The math proves something counterintuitive.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 73

As tools get more powerful, the gap between someone who can spot opportunities and someone who can't gets bigger. But wait, why is AI currently predicting inequality then? Because we're in phase one. Right now, AI is compensating for skill differences. The struggling workers get huge boosts.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 74

The experts, they were already good at implementation. And we've covered a lot of studies that were showing that this is true. Recently we've covered a paper or I think it was actually a news article saying that one group of people that are very much benefited by Chad GBT and other chatbots are people with ADHD. It allows them to kind of fill the gap that ADHD makes. It allows them to kind of cross that bridge in a lot of ways and improves the accessibility that they have.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 75

If you have great ideas but maybe your English written skills are not that good, you can use something like Chad GBT to really put those ideas out there. We've seen a paper, I don't remember who published it, but where in an office-like environment, the people that were in the top 50% of like expertise, how well they did their job, they got a boost from Chad PT, but the people that were in the bottom 50%, they got a much bigger boost. So, it does level the playing field. Maybe not completely, but it does have that effect. So, this is where we are now.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 76

This is where we've been probably for the last couple years. But phase two is coming once implementation is basically free where anyone can design, code, write, the only thing that matters is judgment. Who seized the opportunity? Who knows what's valuable. So in this space we've been referring to that as taste.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 77

Although now that I think about it, judgment is probably a much better way of of saying that. So taste just means that we can kind of judge the outputs like what is a good AI music pseudo output like does this version sound better than this version if you have a good taste in these things you can pick the better one but I guess judgment is even more broad like what opportunities are there to use this people with the best judgment can get more out of these AI models what happens then well that's when inequality explodes again and the paper even calculates the exact turning point here's what broke my brain. Better AI makes full automation less likely, not more. Why? Because automated systems have fixed judgment.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 78

They can't adapt. A radiologist AI might be 99% accurate, but it can't realize, wait, this patient's case is weird. I should think differently. The flexibility to adjust your judgment in real time. That's uniquely human.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 79

And it gets more valuable as the tools improve. Even crazier, this changes how teams should work. The paper shows that as AI improves, control should shift from people who are good at doing to people who are good at seeing opportunities. So that skill is seeing what could be better like the opportunity that's your moat. The researchers call it opportunity judgment and it's about to become the most valuable skill in the economy.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 80

Let me know if you agree with that. When these models came out, we thought it would be prompt engineering. Certainly, it doesn't seem like that's going to be the case. And maybe 2 years from now, we'll look back at this and laugh. But something about this feels right to me.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 81

If we all have tools that allow us to produce excellent stuff, whatever that stuff is, design or code or whatever, who wins in that situation? What's the differentiating factor? It's seems like it's judgment. Like where do you apply that power? The person that applies that at the right opportunity wins.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 82

So when we're looking at like AI will replace jobs, that's missing the point. They measure what people do today. But the whole point is that AI changes what the job even is. A lawyer's job right now, it's writing contracts. That's one of the jobs.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 83

In the future, it'll be knowing which contract variation creates the most value in this specific situation. This makes a lot of sense to me and it's a completely different skill. And this is the line that that really hit me kind of hard. They're saying schools are teaching AI literacy. In this analogy, they're teaching people to be better bicycles.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 84

All right, here's how you prompt the model better. Here's the right buttons to click to get the right output. Right? Instead, we should be teaching people to be better riders. So, the question isn't will AI replace me?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 85

It's am I developing the judgment to ride increasingly powerful bicycles? Because these bikes are about to get very fast. So, check out Carlos at Intuit. He also writes at intuitionmachine.gumroadroad.com. And by the way, the recent research out of anthropic and the Stanford published paper that used the data from anthropic.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 86

I really do feel like it goes handinhand with a lot of the ideas that we cover today. Those first years out of college when you're an intern, when you're learning how to do the tasks, often times you're doing grunt work. You're the bicycle if you will. your cognitive wheel a a tool. You're doing that kind of a boring grunt work that nobody else wants to do.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 87

But it gives you a chance to learn to acquire skills to continue your career. Those jobs are going away. I don't mean in the future. I mean like right now we're seeing a pressure on those white collar jobs. They're decreasing.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 88

If you're between 22 and 26 years old, you're out of college. You're going into one of those professions. the supply of those jobs is going down since about 2022. This is a very recent trend. If you have a decade or two decades of experience in your chosen career field, you're improved by these chat bots.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 89

You can do more. You're more efficient. They kind of give you leverage. So in this video, we looked at what's happening now. So Scott Aronson is publishing paper where as he says a key technical detail implementation was thanks to GPT5.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 90

He's careful to point out that no, it's not that it can do something that humans can't. It just really accelerates discovery. It's clever. It it helps him get to those clever insights faster than he would have been able to on his own. It's accelerating scientific discovery.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 91

Then we looked at Julian Schitweiser. By the way, do people get this reference? Let me know in the comments if you get this reference. It says, "GSV, you're absolutely right." And it has a picture of a large starship with Claude's face at the front of it. So, Cloud is the starship.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 92

GSV, you're absolutely right. What is that referencing? And so, Julian, who's worked on Alpha Go, Alpha Mu, Alpha Zero, again creates a very basic sort of argument for where these things are going. It doesn't require super high intelligence to understand. He's like, just look at this chart.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 93

Look at this arrow. Where is this going if we just kind of extrapolate if that trend continues? And he says, "Could progress stop?" Something could happen to stop it, but it just unlikely. And of course, all of us will have to sort of change our opinions. If if something happens that stops the progress, then we'll have to be like, "Okay, maybe we were wrong." But so far, that does not seem to be happening.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 94

And so his point is in the future we'll have these agents that are going to be doing long horizon tasks. And then finally, Carlos Perez, who covers another paper again. So, this is his ideas about not his own paper, somebody else's paper, but I enjoyed reading it because I think he really breaks it down, adds a lot of his own insights, but he kind of paints a picture of who wins in that situation. Who are the winners and losers of this future that will likely come soon? So, I know there's a lot of people that are saying that this is an AI bubble.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 95

will burst soon that these large language models are useless. That AI is just a scam and it's not going ever that AI is just a scam and it's not going anywhere. This what we've covered today I think is a much more realistic picture of where things are going. I don't think it's too hyperbolic and also I don't think it's too dismissive of AI. So check out the people we've talked about it.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 96

I will link to all of them down in the description. If you made it this far, thank you so much for watching. My name is Wes Roth. [Music]

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´02æœˆ20æ—¥

</div>
