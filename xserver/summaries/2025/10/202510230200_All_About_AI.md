# ğŸ“º ã™ã¹ã¦ã‚’å®Ÿè¡Œã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãƒŸãƒ‹AIãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ï¼ˆDGX Sparkï¼‰

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: My Local Mini AI Data Center That Runs EVERYTHING (DGX Spark)
- **ãƒãƒ£ãƒ³ãƒãƒ«**: All About AI
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=0mXR_tdIwPQ](https://www.youtube.com/watch?v=0mXR_tdIwPQ)
- **å‹•ç”»ID**: 0mXR_tdIwPQ
- **å…¬é–‹æ—¥**: 2025å¹´10æœˆ23æ—¥ 02:00
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®å‹•ç”»ã§ã¯ã€Nvidiaã®DGX Sparkã‚’å€‹äººç”¨AIãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã¨ã—ã¦æ´»ç”¨ã™ã‚‹æ§˜å­ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚15å¹´å‰ã®Dell Latitudeãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã‚„ã€Macã€ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‹ã‚‰SSHæ¥ç¶šã—ã¦AIãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚Blackwellã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆGB10ï¼‰ã¨128GBã®ãƒ¦ãƒ‹ãƒ•ã‚¡ã‚¤ãƒ‰ãƒ¡ãƒ¢ãƒªã‚’æ­è¼‰ã—ã€ComfyUIã§ã®ç”»åƒãƒ»å‹•ç”»ç”Ÿæˆã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã€è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®åŒæ™‚åˆ©ç”¨ãªã©ã€ã‚ã‚‰ã‚†ã‚‹AIé–¢é€£ã‚¿ã‚¹ã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å‡¦ç†ã§ãã¾ã™ã€‚ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚‚ç°¡å˜ã§ã€é›»åŠ›æ¶ˆè²»ã‚‚æŠ‘ãˆã‚‰ã‚Œã¦ã„ã‚‹ã®ãŒç‰¹å¾´ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

- **ã©ã“ã‹ã‚‰ã§ã‚‚ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½**: 15å¹´å‰ã®ãƒãƒ¼ãƒˆPCã€Macã€ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãªã©ã€ã‚ã‚‰ã‚†ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰SSHæ¥ç¶šã—ã¦DGX Sparkã®å¼·åŠ›ãªAIå‡¦ç†èƒ½åŠ›ã‚’åˆ©ç”¨ã§ãã¾ã™
- **ç”»åƒãƒ»å‹•ç”»ç”ŸæˆãŒãƒ­ãƒ¼ã‚«ãƒ«ã§å®Œçµ**: Quen Image Editãƒ¢ãƒ‡ãƒ«ã§ç”»åƒç·¨é›†ã€720på‹•ç”»ç”Ÿæˆãªã©ã€æ¤œé–²ãªã—ã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ä½œæ¥­ã§ãã¾ã™ï¼ˆä¾‹ï¼šç”»åƒç·¨é›†19ç§’ã€å‹•ç”»ç”Ÿæˆ612ç§’ï¼‰
- **è¤‡æ•°GPUã®çµ„ã¿åˆã‚ã›**: ãƒ­ãƒ¼ã‚«ãƒ«ã®4080 GPUã¨DGX Sparkã‚’çµ„ã¿åˆã‚ã›ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ï¼ˆ20Bï¼‰ã‹ã‚‰å¾—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å°ã•ãªãƒ¢ãƒ‡ãƒ«ï¼ˆ7Bï¼‰ã«æ¸¡ã™ãªã©ã€åŠ¹ç‡çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã§ãã¾ã™
- **è±Šå¯Œãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨æ‹¡å¼µæ€§**: 4TBã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€20ã‚³ã‚¢CPUã€128GBçµ±ä¸€ãƒ¡ãƒ¢ãƒªã§ã€æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ã«å®Ÿè¡Œå¯èƒ½ã§ã™
- **ä½æ¶ˆè²»é›»åŠ›ã¨é™éŸ³æ€§**: æ¨è«–æ€§èƒ½ã¯æœ€é«˜ã§ã¯ãªã„ã‚‚ã®ã®ã€é›»åŠ›æ¶ˆè²»ãŒä½ãã€å‹•ä½œéŸ³ã‚‚ã»ã¨ã‚“ã©èã“ãˆãªã„ãŸã‚ã€ã‚ªãƒ•ã‚£ã‚¹ç’°å¢ƒã§ã‚‚å¿«é©ã«ä½¿ç”¨ã§ãã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

Okay, so this is me on my 15year-old Dell Latitude laptop, right? And if you see down here, we have 1.95 gigabytes of RAM, 200 GB of storage, and it's running on antx Linux, right? And it's a very lightweight OS, but it works pretty good for this case. And you can see I'm in my terminal. So, can I really run on Olama GPT OSS 20B now?

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

So if I launch this now you can see yeah we can hello hello and you can see we are getting a response from here. So how can this be right? How can I run this on my uh 15year-old 10 in 386 laptop. So if you look here now you can see on the screen I am in my I I can bring this over right. So I am in my um GGX Spark here that we have over here.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

So this is what is kind of my AI server now. So I'm running this on as a server now. You can see we have the system memory here and you can see I loaded up the model GPT utilization. Let's see what happens if I do a query over here now. So let's just do write some Python code.

### ğŸ“ è©³ç´°èª¬æ˜

And if I turn around now, you can see the utilization there goes up to 93 and everything is working. If you go back here, you can see it's pretty cool that I can just run this on my 15y old laptop, right? And this is what I've been enjoying over the last few days, having my GGS Spark as kind of my yeah AI data center that I can SSH into. And I had so much fun with this. We can also go back here.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

We can head over to my Mac. Okay. So here I can kind of do the same. I can sh into my spark. Okay.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

So let me just do the password. Okay. Perfect. So you can see now I am in my spark from this and I can do a lot of stuff here. If I wanted to run GPT uh I don't know OSS20B and you can see hello.

### ğŸ¯ å¿œç”¨ä¾‹

This is running here too. And if we head over to kind of my main station now over here in my corner, this is kind of my office, right? And I can show you some other stuff we can do with this. So here I kind of got my dual setup here. I got my yeah uh screen recording.

### ğŸ’­ è€ƒå¯Ÿ

So let's see what else we can do with the Spark even though I kind of have this um I have my GeForce GPU here. This is Don't look at the dust here. my 4080. But now I can also use resources from kind of my yeah um DGX Spark here if I need additional resources to do some compute. We can also try to combine some of that.

### ğŸ“Œ ã¾ã¨ã‚

So let me just do some screen recordings here and take a look at what we can do with this Spark here on my local uh setup here. Okay, so I'm not going to go into any deep specs here on the GJX Spark, but we are on the Blackwell architecture with the GB10. We have 128 GB of unified memory which lets us run basically a lot of different models. The CPU is strong with the 20 cores and we have the 4 TBTE of uh space so I can offload some of my stuff if I want to. We have the Nvidia Connect X and a lot of other stuff.

### âœ… çµè«–

So if you want to read more about the specs you can find some information in the description below. Okay. So the way I use this now let's say I am in my terminal. I can do SSH right? I can put in kind of my spark here.

### ğŸ“š è¿½åŠ æƒ…å ±

I can do this. Just do my password for my spark. And that's it. Now I am ssh into this. And uh I have comfy UI.

### ğŸ”– è£œè¶³

So let's create some images, maybe some videos. Let's see what we can do. And you can see if I do something like it has to be source, I guess. Uh, let's do my confi- envy wait, I guess. Yeah.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

Okay. CD com confi. Okay, that's good. And now let's do Python main.py. And let's do a listen 0 0.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

Okay, perfect. And now we are starting up comfy UI, right? Uh let's just wait for this a bit. That [snorts] should be fine. And now we can go here and we can do spark and that should be working.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

So now we should get into confui. So now we are kind of linked up to confui via um yeah via the remote connection, the SSH. Okay. So what I wanted to show you is the Quen imageedit model. This is a really good model you can run locally.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

It's I think it's just called Quen image edit. Right. So I went to Google I found this image of uh Nvidia announcers infrastructure. Right. So we have Sam Altman Jensen Wong and I think it's Brockman is it right?

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

So if we go to our ed editor now I can upload this image. Okay. So you can see we have the image here. Right. So what we can do now with this editor is if we go into my prompt box here.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

Okay. And I can just do a simple prompt here. We can just give all people in the image a leather jacket and a cowboy hat. Ultra realistic. So now we kind of have the pipeline set up.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

So I guess the only people that is missing the leather jacket is Sam Alman. So I guess we could have said something like give the guy to the right in the image a leather jacket and a co by hat. Okay. Okay. So let's try to do that.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

So if we run this now, run it. So you can see on the top here we are starting this. If you look at the terminal, you can see now on the the spark that we are ssh into, we are starting up some workloads here. Now you can kind of see on the terminal here that we are actually using uh quen image here and we are using this third 3.8 seconds per iteration here and it is pretty fast. So that was 90 seconds to Yeah, you can see we ended up with this image.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

So I don't know if that is exactly Sam Altman, but uh we kept the leather jacket here and we gave this guy to the right a new leather jacket and a cowboy hat. Let's try that one more time, but this time don't do the cowboy hat. Let just the leather jacket. Uh okay, so that's fine. And we can remove this.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

Let's do only the leather jacket. Let's run it once more. And I think this time since the model is kind of warm now, this will be a bit faster, right? You can see it's starting right away. And it should go uh a bit quicker this time.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

Okay, so that was pretty good, right? So the reason for that is that we haven't loaded the model in. So this took only 19 seconds to edit this image. And now you can see all three people have an a leather jacket, right? So, I've been really enjoying this model running this uh locally.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

I can be offline. I can edit all the images I want. There's no like censorship or anything like that. So, this is something I really enjoying using the Spark uh with this remote connection I have set up kind of my data center here, right? So, let's do something else.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

So, now I want to do a video. So, let's find an image we can turn into um let's do a 720p um video. So, this is a video model you can run from I think this is also from Quen, right? I think so. So, you can do 720p resolution.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

So, uh with the GGS Spark, I can actually run this locally. That's pretty crazy. Okay, so for the video now, let's do something else. So, I grabbed just a 720p vertical image from this image we created. So, I took an image of myself and I'm going to use the Quen image model now just to prepare our video image.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

So, replace the person on the right with a guy with a cap selfie. I don't know if it has to be a selfie, but just let's just do this, right? So, let's see if we can replace ourself. So, I'm going to do that. I'm going to run this model.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

And now let's see if we can put ourselves in this image and then we can turn it into a video. Okay, so we got this result. So the hair is way too long, but let's just keep it just for fun. So what we can do now, I also got this version, but the head looks a bit off. So if we head over to the video model now, and we just put in my image here, right?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

So this is kind of my starting image. We want to do this into a 720p 1280 uh version, right, of a video. And uh I'm going to do two people giving a thumbs up handheld camera. So basically all I have to do now is just click run. And of course, this is going to take a bit of a time at the beginning because we have to load in the van model again because we yeah, I guess we replace that in memory with the Quen model.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

So we have to load up the model. Then we're going to start generating the video. And I think this takes around like 5600 seconds, something like that. So I'm just going to let this run and we have a look at a few other things in between here before we take a look at kind of the final video. We can use the Spark to generate here uh locally.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

So while we wait for that uh I just want to show you kind of the uh main page uh from the uh GGX here. And I got to say the onboarding Nvidia did here with the GTX Spark was super easy, right? So, uh, set up local access networks. They have something that is called Nvidia. Let me see if I can find it here.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

I think it's Nvidia sync. And to actually sync up my DJX Spark with the uh, you can see Nvidia sync here. Connect with Nvidia Sync. So I just downloaded the sync and after like two minutes or something I was already connected to my GJX Spark via the sync and I can do yeah a bunch of things. I also I didn't screen record it but I also tried to use the Ubuntu uh setup in the Spark that kind of comes preconfigured so I could just run uh everything I'm doing SSH rights now I could do locally kind of on the on the Spark itself.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Uh I have it connected over there, but uh I'm not going to do that now because I can't screen record. And there's a lot of cool playbooks here. I explored. I had a look at kind of how we can do some fine tuning. Uh they have an really interesting something that is called uh uh where is it now?

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

NVP uh 4 quantization. So this is the FP4 floating point 4. And this is something I'm going to explore more later. I think uh I don't really have time uh today. uh speculative decoding.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

This is also something that was pretty interesting. Uh there's just a bunch of stuff we can do with this uh VS code. I can just uh link up the spark to my let's say cursor and I can use that locally and confi that we are looking at now. So, one other thing I wanted to do, right, uh that we can look at after the video is done is uh since I have uh a 4080 here on my computer, I could do something that is a bit uh different. So, I don't know how I should explain this.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

So what I wanted to do, let's say I wanted to run um the bigger model on the spark and I wanted to run a smaller model on my um local machine here on the spark. Here you can see this model GPOSS 20B that's not a big model but that is running on my spark and I have the DeepSync R17B uh running locally. So what I wanted to do is I wanted to provide some context from the bigger model into the smaller model as kind of context before it answers the questions how dolphins communicate. So uh we have a function that is fetch spark context. So this is using the spark via some kind of tunneling here.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

So if we go to my terminal you can see the image is running. I can do this uh tunneling here locally and I can log into my spark and we have this tunnel set up that I can connect to with the open AI um kind of setup here. You can see spark tunnel and we can actually send some requests. we can get a response back from the spark and feed that into if you go down here uh the context is going to be uh delivered from the spark we have the question and now the 7b deepseek model is going to have the context from the spark before it answers the questions about dolphins. So uh after the video is done and the Spark has uh some more memory free, we can try this out and see how it works in practice.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

So just a few things uh I want to try in the upcoming weeks is uh take a look at the new Quentry VL32B uh that was released I think it was yesterday. I haven't had time but I had a look at this and the the kind of the benchmarking looks crazy good to be a 32B model. So, we're gonna try that out on the spark in the upcoming weeks. And there was a really interesting paper or model from deepseek again uh OCR. So, here is someone uh calling it out lives up to the hype.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

So this is uh I think it's like a new way they try to compress information uh by using images or OCR models uh to compress tokens instead of using like tokens. They use some kind of other setup here. I'm not going to go into the technical parts but there are some good videos already on how they how DeepSync use this uh OCR model to compress information. So, I also want to try this out uh when I have time, but uh it was a bit uh too close to when I wanted to release this video. So, we definitely going to check that out in the upcoming weeks, I think.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

But now, let's take a look here. So, you can see this is done. We have our video here. And this took 612 seconds. Okay, that's fine.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

So, let's see now. So, I'm going to open this up and let's play the video here. There's no sound, of course. [laughter] Okay, so I'm super happy. Big thumbs up here.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

Uh, okay. So, this is running in a loop. And you can see I did all this just locally. I edited the image. I created the video.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

And everything was just done on the Spark that is standing over in the corner there. So, I can't even hear it spinning up. So, I just found this very cool and I had so much fun this last days playing around with this stuff. So, [snorts] also the the power consumption is pretty low. So, this is a very cheap way I guess of doing uh all kinds of different workflows on the Spark.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

Okay, so let's run uh the kind of the dual setup I have with my local uh 4080 and the Spark over there. So, uh we need to set up this tunnel here, right? So this is what we're going to do. Okay. So I'm going to enter the password for the spark.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

And yeah, this should be up and running now. So I'm just going to leave this running this tunnel into our unit. And now if I do something like python dual spark.py. Yeah, I'm just going to do this inside cursor here. So the first thing we can see now is that we are fetching the context from the spark, right?

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

Uh so this is now running uh the workload on the GPT uh OSS 20B model up there on the spark and it's going to bring that context back in here uh as context for the yeah as you can s see context from the spark. So here we got some information about dolphin language uh that is going to be fed into the deep 67B model as context. So now you can see that the model is responding based on the context we got from our bigger models and we end up with kind of the final answer here. So if you think a bit uh dive a bit more into this you can see okay so this is kind of the thinking tokens from deep 67B. Okay so I need to figure out how dolphins communicate based on their provided context.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

So, of course, the provided context comes from the Spark and the OSS 20B model. So, this is just a small experiment of what I've been starting playing around with this. We could could of course have a big model like a 70B or like 120B model up on the Spark and use the context from that into our smaller model locally to answer the question. If you wanted to do something like that, that could be like offloading some code questions to a bigger model or you kind of get the point. But we end up with this mo uh answer.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

So we have acoustic signal, social context and everything about dolphin language here. So, I think this was just a neat simple experiment you can do uh by providing if you have like a local model or like um uh where's the problem with that camera? If you have like a GPU in your desktop uh but you don't but you can't run big models, you can let's only run 7B models or something like that, you can do this type of workflow. Uh this is a bit early for me. I need to explore a bit more.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

But this is one thing I found out that yeah, I kind of enjoyed setting up using my desktop and the GGX Spark here. And I think that's going to be kind of my conclusion for the Spark so far. Um it's not of course the best at inference. Uh I think that was already pretty clear, but uh the way I have been using it, it's doing everything AI related, developer related. uh at least I haven't done everything but I can do images I can do text I can do large models I can do small models I can do video uh I can do uh multiple models at the same times at different uh I can go to my old laptop and do that I can do on my Mac I can log in I create images on my Mac by using so there's just a lot of different use cases I think that's kind of the strong side for this I think there's a link in the descript description if you want to read more about this unit.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

And so I think the the most fun I had was kind of setting up my network, uh, bringing out my old laptop, getting logged into that. I can SSH into it. I can run the the models. I even spun up Confi UI on my 15-year-old laptop. I created some images.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

And also the Spark has a 4 terbte. So there's a lot of storage on it. I can kind of offload to my data center. It has all the resources. It has the CPU.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

It has the GPU. So all in all, I just had a lot of fun with it. And if this sounds interesting to you, definitely go check it out. Uh I can't really say I'm an expert on hardware, but for me, I have enjoyed this a lot and kind of the onboarding was very easy and stuff. So yeah, uh check it out if you have uh the resources to do this.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

It's not cheap, right? But if you have the resources, I think this is a pretty neat unit that I haven't really explored to the full yet. So I'm going to do more videos on it on the upcoming weeks and months, I guess. Uh when new models come out, we can try out. So yeah, thank you for tuning in.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

a bit of a different video and have a nice week and we speaks again

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´01æœˆ12æ—¥

</div>
