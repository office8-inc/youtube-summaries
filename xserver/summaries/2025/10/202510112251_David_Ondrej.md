# ğŸ“º AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‹¡å¼µæ©Ÿèƒ½ã§10å€ã®é–‹ç™ºè€…ã«ãªã‚‹æ–¹æ³•

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: AI coding extensions will make you a 10x developerâ€¦
- **ãƒãƒ£ãƒ³ãƒãƒ«**: David Ondrej
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=tp_QBQVpfbk](https://www.youtube.com/watch?v=tp_QBQVpfbk)
- **å‹•ç”»ID**: tp_QBQVpfbk
- **å…¬é–‹æ—¥**: 2025å¹´10æœˆ11æ—¥ 22:51
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®å‹•ç”»ã§ã¯ã€Cursorã‚„Claude Codeã‚’è¶…ãˆã‚‹æ¬¡ä¸–ä»£AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‹¡å¼µæ©Ÿèƒ½ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚VS Codeãªã©æ—¢å­˜ã®ã‚¨ãƒ‡ã‚£ã‚¿ã§ä½¿ãˆã‚‹AIæ‹¡å¼µæ©Ÿèƒ½ã®åˆ©ç‚¹ã€è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‹æ–¹æ³•ã€ãã—ã¦å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½œæ¥­ã§ã®æ´»ç”¨ä¾‹ã‚’ç´¹ä»‹ã€‚AIé–‹ç™ºãƒ„ãƒ¼ãƒ«ã«1,000æ™‚é–“ä»¥ä¸Šã‚’è²»ã‚„ã—ãŸè‘—è€…ãŒã€ã“ã‚Œã‚‰ã®æ‹¡å¼µæ©Ÿèƒ½ã‚’ä½¿ã£ã¦ç”Ÿç”£æ€§ã‚’é£›èºçš„ã«å‘ä¸Šã•ã›ã‚‹å®Ÿè·µçš„ãªæ–¹æ³•ã‚’ä¼æˆã—ã¾ã™ã€‚é–‹ç™ºè€…ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒAIãƒ„ãƒ¼ãƒ«ã‚’æœ€å¤§é™æ´»ç”¨ã—ãŸã„æ–¹ã«æœ€é©ãªå†…å®¹ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

- **æ—¢å­˜ã‚¨ãƒ‡ã‚£ã‚¿ã§ä½¿ãˆã‚‹è‡ªç”±åº¦**: Cursorã‚„Claude Codeã«ç§»è¡Œã›ãšã€VS Codeãªã©ä½¿ã„æ…£ã‚ŒãŸã‚¨ãƒ‡ã‚£ã‚¿ã§AIæ©Ÿèƒ½ã‚’åˆ©ç”¨å¯èƒ½ã€‚ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚„è¨­å®šã‚’ãã®ã¾ã¾ç¶­æŒã§ãã‚‹
- **è¤‡æ•°AIãƒ¢ãƒ‡ãƒ«ã®ä½¿ã„åˆ†ã‘**: ä¸€ã¤ã®ãƒ„ãƒ¼ãƒ«ã«ç¸›ã‚‰ã‚Œãšã€ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦Claudeã€GPTã€Geminiãªã©æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹æŸ”è»Ÿæ€§ãŒã‚ã‚‹
- **ã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ€§**: æ‹¡å¼µæ©Ÿèƒ½ã¯å¤šããŒç„¡æ–™ã¾ãŸã¯ä½ã‚³ã‚¹ãƒˆã§åˆ©ç”¨å¯èƒ½ã€‚ã•ã‚‰ã«è‡ªåˆ†ã®APIã‚­ãƒ¼ã‚’ä½¿ã†ã“ã¨ã§ã€ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã‚’å®Œå…¨ã«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ãã‚‹
- **å®Ÿè·µçš„ãªé–‹ç™ºæ”¯æ´**: ã‚³ãƒ¼ãƒ‰è£œå®Œã€ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã€ãƒã‚°ä¿®æ­£ãªã©å®Ÿéš›ã®é–‹ç™ºæ¥­å‹™ã§å³åº§ã«æ´»ç”¨ã§ãã€ç”Ÿç”£æ€§ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹
- **ç¶™ç¶šçš„ãªé€²åŒ–**: AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã¯æ€¥é€Ÿã«é€²åŒ–ã—ã¦ãŠã‚Šã€æœ€æ–°ã®æ‹¡å¼µæ©Ÿèƒ½ã‚’ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§å¸¸ã«æœ€å…ˆç«¯ã®é–‹ç™ºç’°å¢ƒã‚’ç¶­æŒã§ãã‚‹

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

So AI coding extensions are taking over, but most people are still stuck in 2024 only using cursor or cloth code. So in this video, I'll explain why these next generation AI extensions are blowing up and how you can use them to build anything. Now my name is David Andre and I'm the founder of vector.ai and I've spent well over a,000 hours building software with AI tools. And you might be asking, okay, David, why not just use cursor? Why even bother with all these new extensions?

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

Well, there are actually several reasons. The reason number one is that you can use these AI extensions anywhere. Maybe you have a VS Code setup that you really like and you really don't want to switch to Crystal Windsor. Well, guess what? Just install an extension and you can use it right away.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

Also, with these extensions, you can use any provider. Doesn't matter if it's OpenAI, Enthropic, Gemini, Mistrol, Deepc, Quen, whatever you want to use, these modern extensions support it. But if you only use cloth code, obviously it only supports enthropic models. And actually later in the video, I'll show you how you can use open router to use any model in existence, no matter if it's open source or closed source in these extensions. So make sure to watch until the end.

### ğŸ“ è©³ç´°èª¬æ˜

The third reason is that these extensions are free and open source. So instead of paying $200 a month for cloth code, you can use Kilo Rue or Klein for free. And on top of that, they are open source. Now the beauty of open source is that you can literally pull up GitHub and look inside to see how this was made. And if you have an idea how you can improve it, you can click the fork button and start working on your own version today.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

Another reason why thousands of people are switching away from cursor to these extensions is that they are customizable. They offer much better context engineering which is great for power users. Plus you can actually see the usage overview. Cursor on the other hand doesn't have the best reputation for being transparent about the limits. Now when you're comparing kilo ru and client it's important to understand their relationship why each of them work and how they came to be.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

So the simplest way to understand it is that client got created as a open-source coding AI agent. Then someone forked it and created R code which is a fork of client and then ruode got forked and that's how kilo got created. But each of these AI tools serves a different purpose which is exactly what I'm going to explain now so that you know which extension is the best for you. Starting with client which is the foundation and it's the most beginnerfriendly one. The setup is simple but it's also not that customizable.

### ğŸ¯ å¿œç”¨ä¾‹

Now the way to think of the other two extensions we're going to cover RU code and kilo code is as upgrades to client. However, even if kilo and ru are more advanced with more features you still need to know how to use the fundamentals. And if you learn how to use client, you'll be able to easily adapt to any new extension that comes after it. So, let me show you how to set up client. And it's actually very, very easy.

### ğŸ’­ è€ƒå¯Ÿ

All right, so let's jump into VS Code cursor, whatever code editor you're using. Open up the extensions tab right here. Boom. And type in client. Right there it is.

### ğŸ“Œ ã¾ã¨ã‚

And if you don't have it already installed, just click install. Once you install the extension, you need to open the primary sidebar. And there it is. There's client. You might have it on the right.

### âœ… çµè«–

So, if you prefer on the left like me, drag it to the left. Obviously, it's the superior position, but it's fine. You can still follow along even if you have it on the right. Anyways, this is what client looks like. Here we have a history of recent tasks.

### ğŸ“š è¿½åŠ æƒ…å ±

Obviously, if you just install it, it's going to be empty. At the bottom is the main chat input field. Above you can decide a lot more advanced settings such as which commands to order approve. Anyways, before we can use client, we have to set up a model, which means we have to choose a provider. So, go to the settings on the left.

### ğŸ”– è£œè¶³

Click on API configuration. And here we have to decide which provider we're going to use. Now, this decision is very easy because there's one that allows you to use all of the models and that is open router. So, just go to open router.ai and create an account. This takes like 20 seconds, super easy.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

Then go to the top right, click on credits, and make sure you charge up some money. Just charge up like $5. Okay, that's going to be plenty because we can use open source models which are super efficient, right? For example, if you click on top right, you can see a list of all the new models that just released such as Ernie 4.5 21B A3B thinking. See, most people don't even know that Ernie 4.5 exists, which means they're falling behind.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

No, I'm just joking. But basically, here you have all the models that just released sorted by uh recency, but uh every single model you can think of, such as GLM 4.6, which is one of the best ones right now, or cloth zone is 4.5 or deepseek v3.2, every single model is available on open hour. That's why if you pay attention for the next 2 minutes, you will know not only how to use any extension, but also how to use any AI model that comes out. So go to the top right, click on keys and click on create API key. I'm going to name it subscribe.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

If you're watching this, make sure to subscribe. Takes 2 seconds and it's completely free. All right, so let me create a key and let's copy this. Now let me switch back to my empty project and I have to say do not share your API keys with anybody. Okay, treat them as passwords.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

Then inside of client, let's switch on API provider. Click on open router and then simply paste your API key. Next, we can select which model we want to use. So maybe you want to use GBT5. Boom.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

GBT5. There she is. Enable thinking. We do want to enable thinking. Let's do 6,000 tokens.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

And client already loads the correct pricing from open router. So you don't have to worry about setting up context window or input output price. This matches exactly GPD5 right here. Boom. You can see everything is matching.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

Okay. Let's click on done and let's send a test message. Who are you? We're in act mode. By the way, you can switch to plan mode if you don't want client to make changes.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

Okay. And there is the first response. Now, one thing you might notice is at the top we can see advanced context usage, right? This is something that's completely missing from cloth code or cursor. These tools have been like obscured.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

They have been not transparent around usage and people have been complaining that they're hitting their cloth code limits, their cursor limits way sooner than they used to. Right now, before we dive into the advanced settings of client, let's first talk about rue. So rue code, the easiest way to think about it is client but with powerups. It's best for intermediate vibe coders who want to have control over the system prompts, all the different models, just more customizability than client offers. For example, one feature is the slash commands, right?

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

This is the clear philosophy of Ruko versus client. You can set up different slash commands similar to close code actually where if you find yourself repeating certain prompts, just turn it into slash commands and you save massive amount of time. By the way, we're adding the same feature into vectal. So instead of using notion where you have to repeat same thing over and over, just use vectali for your tasks. Speaking of Veectal, we have made massive improvements.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

For example, we have built-in Perplexity Pro. So, instead of paying $20 a month for Perplexity Pro, you just get Vectal and you get all of the AI models with it, right? We have Sony 4.5, GP4.1, GLM 4.6, all in a single app with a powerful chat, way more powerful than chat GP actually. We have SL commands. We have a productivity agent that can list out your task, list out my top tasks, and the chat agent can interact with the UI, right?

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

So, this is really the ultimate productivity agent. Instead of using chat GBD which doesn't have a context on your tasks, your projects, what you are currently working on, right? Vexel knows all of that. You have your taskers here. If you want to switch to canvan view, you can do that.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

Now on the left, you have the sidebar where you can see all your projects and with each project you can set a custom system prompt so that the chat agent is specialized and focused just on that project. So if you're someone who likes to be hyperproductive and you want to have access to all of the cuttingedge AI models in a single spot, go to vectal.ai and give it a shot. You can get started completely for free. All right, let's go back to R code. So one of the biggest things you have to understand is indexing.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

And honestly this might be the biggest difference between client and root code is that root code indexes your entire codebase while client doesn't do it and they don't do it on purpose. Now you might be thinking okay David but what even is a codebase index. Well think of it as a multi-dimensional vector database of your codebase. Right? So your codebase gets converted into highdimensional vector embeddings and this enables you to search through it semantically.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

So when someone says authentication, you can get all different files related to authentication even if they don't necessarily mention that word. So here's a visualization of a three-dimensional vector database. Now in reality it can have thousands of different dimensions but basically if you have let's say front end you know files on the front end would be probably semantically far away from files on the back end or if they're about the same feature you know maybe a chat like in vector we have the chat right maybe a file about the chat could be semantically closer even if it's a front end or back end file right so when you index a codebase you take all of your files and all of your code and you put it into something that looks like this obviously with way more dimensions And this allows the tools to search faster, right? So the benefit of that inside of R code is that you have faster search. When you type in update the task list, it will know all of the files that related to tasks instantly, nearly instantly.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

The issue with that is um maybe some context gets loaded that you don't want loaded. And this is why client on purpose decided to not add indexing. This gives you the choice of choosing which context is inserted into the prompt. Plus also in theory it gives you ability to spend less money because let's say if some of these tokens that get sent each time are from the index. If you don't have any indexing you obviously don't pay for those extra tokens, right?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

So by using client you should save a bit of money on API costs. Now actually they wrote an entire article on this which I'm going to pop up on screen right here. They took a controversial stance against indexing saying that context windows are large enough. You can just tag the files that you need. you don't need to do this advanced indexing.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Now, this is definitely controversial because almost every other coding tool does index your codebase. Now, let's look at some of the main reasons why people like to use R code over client. First of all, multiple specialized modes, right? So, if we jump back in, let me activate R code. And by the way, the setup is the same.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

You go into extensions, you type in R code, you click on the kangaroo right here. Boom. Shout out to all the Australians watching this channel. I don't know how many of you there are. If you are an Australian, comment below kangaroo.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

Anyways, you install the extension in the same way, right? And then you activate it. So, root code. You can see the logo right here. And it's very similar to client, the fork of client obviously, but uh you don't have the task button like new task and the chat is a bit cleaner.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

However, you have these modes, right? You have architect, you have code, ask, debug, and orchestrator. These are basically pre-built prompts that allow you to do different parts of your codebase. And honestly, I'm considering adding something similar into vectal, such as, you know, organizing your task, brainstorming new ideas, planning or research, different productivity actions. Let me know guys if you would like to see that.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

Basically, different modes for different types of productivity or task management actions that you do. Anyways, this is one of the biggest differences. You can also do mode marketplace where you can browse through different modes created by either the RU team, you can see at rue or just random people, right? So if you want to find something, you can find it in the marketplace. Same with the MCPS by the way.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

Another way to make your extension more powerful is to connect different MCPS. You can also use the vectal MCP here. Anywhere where MCPS are connected, you can use the ve MCP to manage your tasks directly from root code. So maybe you have a project with your tasks inside of actal and you just want root codes to execute them. You would connect the MCP and you would delegate it to the coding agent.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

The next feature people like about root code is the memory bank support which allows you to maintain project context and conic standards across different sessions. Basically creating memories of your codebase. It also has enhanced context understanding and better handling of large code bases. And I think this is because of the index because if your codebase is very very large, you cannot just tag all the files in the context window of a single prompt, right? So that's where the index comes into play.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

And we see that with other tools like augment which really stresses the indexing. Now there are a few other reasons why people prefer using ru. But let's just test it, right? Why not just to use it ourselves and you can make up your own mind. And this is what I would encourage all of you do.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

Just install these extensions like the setup is the same. You can use open router with all of these, right? Just go into settings. Boom. We have the providers on the left.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

Here I have a GLM setup through the GLM coding pass. But then I realized why not just use open router, right? We have GLM over here as well. And in a bit I'll explain why this model is so amazing. So stay tuned because this model might be the best open source coding model right now.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

So I'll deep dive into this model in a second explaining what makes it really good. But what I just realized is that instead of using this through the Z AI, you know, Zipu AI, GLM coding plan, which is a separate subscription and this and that, you have to set it up like I did here. OpenI compatible. You just use it through open router, right? So the beauty is that if we switch, we can add a new profile.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

Boom. Open router. Create profile provider. Let's type in open router. Here we will create a new key.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

But yeah, there we go. It also loads the limit. That's nice. And we can just choose the GLM right here. Make sure to use 4.6.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

That's the latest one. And uh reasoning effort. This is another benefit here. You can select the reasoning effort in root code. I'm going to go with high.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

You also have some advanced settings such as to-do list, editing, match precision. You can see that root code really is like more advanced version of client. So depending who you are, if you are a power user and you want all the control, maybe root code is a better option for you. If you prefer just a simple interface and you trust the creators of the tool to give you a nice setup, then client is a great alternative. Anyways, uh I think we all of these are good.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

Just make sure to set highing effort and then we can click on save and let's see if we actually hit GLM 4.6. And the beauty of running it through open router is that if one of the providers is down, right? So if we go to GLM 4.6, we can see that we have a Jud, Silicon Flow, Deep Infra, GMI Cloud, Atlas Cloud, Parasale, Novita AI, and ZAI. This is the official one, right? Zipi, those are the creators of GLM4.6, but you can see they have like pretty low throughput, right?

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

And maybe their uptime is uh not 100%. So if they are down, open switches you to a different one. And if you click on providers, you can see the different up times and different throughputs and you can actually uh use the provider with the highest throughput to work as fast as possible. And in the uptime, you can see the difference, right? Like if you use a single provider, the uptime is not that good.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

But uh if you have the open router set up, you basically have 100% uptime always. And in the apps, you can see how people are using it. So we can see uh we have R code right here. We have client right here. And Kilo, which we're going to get to in a second, is the second highest user in terms of GLM 4.6 at the moment.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

But uh let's jump back and let's actually test it. So who created you? Let's see if it knows which AI created this model. Now, as you can see, R suggests tasks, follow-up tasks based on what you did. This makes it much easier for people who are not creative, right?

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

If you are creative, you probably know what to do. But a lot of people are not sure what to do. So having these like one-click ways, you know, tell me more about yourself as a assistant. It's tries to predict your next action and just make it a oneclick thing. So as you can see, this is what it looks like when it answers.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

It says like task completed and you can see the details of the API request as well as the pricing as well as the thinking. You can expand that as well in the think tags. So you can already see the differences between the user interface and at the top we also have a different way of visualizing the context right by default it's a bit more compact but then you can expand it and you can see the details about the context length the input tokens output tokens API cost and the size in kilobytes so the creators of these extensions are opinionated in a different way and at the bottom you can see the codebase index right here you can also disable it if you want so if you want to use r code but you hate indexing it's still a possibility now as I promised let's talk about glm 4.6 because this is one of the best actually it is the best open source coding model right now and a lot of people don't even know that it exists. So before we talk about Kilo which we're going to get to let me quickly talk about GM 4.6 because man this model is amazing. Look at this.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

Right. So here we have eight different benchmarks comparing it to GM 4.5 previous version. Deepseek v3.2. So that's the latest version of deepseek cloth 4 set and clot 4.5 set the latest one from enthropic right chm 4.6 even beats cloth 4.5 set on multiple different benchmarks. So this isn't just some random new open source model that you can ignore.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

You cannot ignore this because this is beating the best enthropic model on several benchmarks, right? So we have AME25. This is math. You can see that GM 4.6 is the best at math. GPQ8 which stands for Google proof question answer.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

It's another very famous benchmark and you can see that here it almost matches sonet 4.5. Life codebench v6 it absolutely destroys sonet 4.5 we have hle I don't know what this benchmark is but with tool calling it's amazing right browse comp also beating enthropic swb bench verified this is where sonet is much better terminal bench also the new set defeats here and ts squared bench is also set is the best but gm 4.6 six is the best opensource model. So this model is actually really really strong and in some ways on some tasks it will be better than sonet 4.5 and this is why you absolutely cannot ignore these extensions right you might have cloud code you might have cursor neither of them have glm 4.6 right now which is just crazy right so if you're using codex if you're using cloud code these are great tools right and at the end I'll tell you what my thoughts are and what my current setup looks like because there is still a time and place to use codex and cloud code obviously but you really cannot ignore these extensions because this is the way to use models like GM 4.6 that don't really get the mainstream attention but are crushing these benchmarks and in many ways they are much more cost- effective than enthropic models which are usually the more expensive types of models. Speaking of cost, let me break it down for you. GLM 4.6 currently costs 17th of the price of cloth sonet 4.5.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

And as you can see, it is a comparative model. It's not like it's significantly worse. It's literally comparable on some tasks is even better. And it costs 17th. So set 4.5 costs $3 per million input tokens and 15 for output.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

GLM 4.6 index costs $0.5 per million input tokens and 175 per million output tokens. Now if you look at this artificial analysis coding index created by client we can see that the evolution of opensource models right so back in 2023 Llama 2 Mr. large. These models were very behind the closer models. However, we can see that recently there has been a closing of the gap.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 56

Thanks to models like Deep Seek R1 or Quen 3, we can see that the cutting edge of closed source models isn't that much far ahead compared to the cutting edge closers models. And it is entirely possible that 3 months from now, 6 months from now, most of us will be using mainly open source models such as GLM 4.6, 6, which already is beating a lot of these closers models on multiple different EVAs. Now, honestly, I might make an entire video on GLM 4.6 and how good it is compared to other models and actually building a full software project with it. So, if you don't want to miss that, make sure to subscribe. But now, let's look at Kilo, which I've hyped up throughout this video.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 57

And there's a good reason because right now, this is my favorite out of these three. And you'll see why. Kilo code is if you want everything from root code plus some more features while having a simpler cleaner UI. So basically it takes the best parts of client and the best parts of root code and puts it into a new tool. Now there is a reason why kilo is often in the top two or top three in open router usage because uh people are loving this extension.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 58

So as I said it has the cleanest UI and the most customizability. For example, the autocomplete you can select the triggers such as pause to complete. So when this is enabled, Kilo will automatically trigger autocomplete when you pause typing. And here you can even select the delay such as 3 seconds. Now actually while I'm explaining these features, why not just launch it, right?

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 59

So again, extensions. Boom. You type in kilo code. There we go. Kilo code AI agent.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 60

Looks like this. Install it. You can already see the UI is cleaner, right? It's less overwhelming, less cluttered, which is what I like. Now I'm going to start it on this project.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 61

You can see I don't have any files. So it's going to be completely from scratch. And we're going to use the architect mode. You can see these modes are similar from rue because it's built on top of ru code. So let me paste in my prompt which is basically YouTube jobs but for AI video creators.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 62

So if you don't know what YouTube jobs is, it's like a site where you can get hired as a video editor, right? Or family creator. This build idea and let me just launch it real quick is that but for AI video clips. So you have Soda 2, you have VO3. AI video is improving fast, right?

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 63

And there will be people who master the creation of AI clips. And there will also be plenty of YouTubers and content creators who will pay for that. Right now, you have to buy expensive stock footage, but that is no longer the case. You'll just hire an expert AI video creator who has mastered the latest prompt engineering and context engineering techniques and he will give you custom on demand clips, any aspect ratio, any realism or, you know, any art style you want. This is a new position that doesn't exist yet that will emerge over the next 6 months and they'll probably become a marketplace of these clips because you have like runway VO sora all kinds of different tools that are being created and more coming from China.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 64

There has to be a single marketplace when you can find these or sell these or create these right so this is what we're going to be building right now or at least the MVP of it. Okay. Okay, so you can see Kilo has asked me a question. What's your preferred deployment environment for the MVP local deployment only? Simple cloud hosting like Versell or Railway Render or AS.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 65

Okay, so we're not going to do any of these uh hyperscalers. That's way too try hard. We're just going to start with local right now. And this is really nice that it's asking these because it lets you scope out what you're building. Are you building a quick MVP to validate or invalidate your idea or are you trying to build a serious app that needs to be deployed across like Kubernetes, containers, whatever.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 66

So this is the architect mode, right? It's really um tries to understand your vision and it tries to understand like what you're trying to aim. If you want to execute, you have the code mode. If you want to debug, you have the debug. There's multiple different modes for a reason, right?

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 67

All right. Next question. For the job marketplace, what's the primary workflow? I think both workflows or actually we can probably do this one. So let's cancel this and let's uh select this one instead.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 68

Perfect. Now I have a clear understanding of the requirements. Let me create a comprehensive to-do list based on your plan. So before it jumps into work, it creates to-do list. And you can see at the top, this is the beauty of Kilo.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 69

You see this beautiful visualization of the context. You can see like view AI response and you can click to any of these and you can see what's taking up context window right here. Very nice visualization. I think we're going to see more AI tools implementing stuff like that. Actually, I'm considering adding stuff like that into vectal to visually see your context build up.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 70

It's nice to have the complete control over the context window. And here we have the complete to-do list that kilo created. It's like 13 different things. Kind of comprehensive to be honest. And now it's creating the architecture.md.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 71

Very nice. So instead of jumping straight into code, it creates some markdown files to plan it ahead. And this is really the future of AI programming. I think the vibe coding is dead. Actually, I tweeted this today and it caused some controversy.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 72

If you don't follow me on Twitter, David one, go follow me right now because uh you're missing out on the spicy takes that I cannot put on Twitter. Anyways, vibe coding is dead and we have killed it. The problem with vi coding is all the like low-level people that it attracts, all the people who just like type in a single prompt and they just want to make money fast, right? Those type of people who think building software is just a joke. It's not a serious endeavor.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 73

So, there's a big difference between vibe coding and AI programming. And I want to focus on AI programming going forward because that's what the serious people do and that's where the future lies in building great production ready software that is fast, reliable, scalable and you can see that kilo especially in the architect mode really is optimized for AI programming. This is far beyond vi coding. We can see that this coding agent goes and creates multiple markdown files before even writing a single line of code. The amount of planning and thinking ahead is honestly impressive.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 74

And we can see that this agent is running like it is putting in the work, right? I'm not doing anything. My hands are lifted and it is using GLM 4.6 to go through this to-do list to design the data model schema mock the data services implement user authentication system mocked before even writing any code so that it can plan it ahead. It can see like okay based on what the user has told me, based on the complexity of the system, what is the best text stack? What is the best codebase structure?

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 75

What is the best modular system? What is the best file length? What is the best uh naming scheme for the functions and files? All of these things are like less about coding, more about architecture, right? I mean, we're in the architect mode.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 76

So, this is like software architecture, software design. This is what VIP corders have zero clue about. Now, honestly, the architect mode might have been an overkill for this MVP. So, I'm going to start a new task while this is running. And it's going to be in the code mode.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 77

Same prompt. I was just going to say now get to work and make this happen. The fewer lines of code the better. This is one one of my favorite pros by the way. The fewer lines of code the better.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 78

Really great when doing AI programming. Now because we already have the architecture and system flows files created from the architect earlier. The coding agent now can execute way faster. And I already gave it a pretty detailed prompt. So it should be it should be doing just fine.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 79

Now while this is running, let me explain another reasons why people like kilo over ru or client. For example, we have the quick task which is similar to command k inside of cursor. So actually I can show you that just right here. So maybe I don't like the structure of this job class. I can do command k.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 80

I can say um simplify this class, right? And this is basically targeting a single isolated portion of the code. And see it removed some sections. I'm going to undo that because that's messing with the plan that Kilo proposed. Command I is the same feature inside of Kilo.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 81

And then also the manual autocomplete. This is very interesting. If you need a quick fix completion or a refactor, Kilo can use the surrounding context to offer immediate improvements, right? And this is a autocomplete. So you don't have to specify what you need.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 82

It's basically like a command K, but it tries to guess your intent. And this really shows that the team behind Kilo is experimenting fast with new features. And I would say they're the fastest moving of these three AI coding extensions. And we've also seen this with the speed of adding GLM 4.6. Right?

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 83

So this is a as we've said this is a very powerful model that is the best open source coding model right now. I would say the best coding model is definitely GPD5 codex with high reasoning effort. But more more on codex in a bit. But you can see that GLM 4.6 was added by Kilo on September 30th, while Klein took three more days. Still was fast, but Kilo is really on top of it.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 84

These guys are grinding now. Because of how logged in the team behind Kilo is, I reached out to them asking if they want to partner on future videos, and they responded within like 2 hours. So, I was like, "Listen guys, I'm making a video on AI code extensions. Do you want to sponsor this one?" And they agreed. So, I'm not changing any of the conclusions I said, but I do want to tell you about the team plan of Kilo.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 85

So if you are someone in charge of a team of developers, you really need to look into the Kilo team plan because this is one of the best offerings I've seen. First of all, you get smart controls. So you can see that your team isn't using models that are leaking your data. A lot of these model providers actually train on your data. And if you have a proprietary codebase, like a lot of serious companies do, you need to make sure that nobody on your team uses these models.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 86

With the Kilo team plan, this is super easy. There's also zero login. Kilo is open source, so you can use their API or you can use your own keys. You can even use local models. It's completely up to you.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 87

And honestly, if you are smart, the Kilo team plan will actually save you money because you can see who on your team is overperformer, overachiever, who deserves the next promotion, and who's kind of falling behind, you know, kind of not using the tools, not really adopting to the cutting edge of AI. Plus, it lets you ensure that everyone on your team is using the latest and greatest AI model out there. So if you want to build a serious software company or if you already lead a team of developers, the kilo plan for teams is really something you should look at. It's going to be linked below the video and the amount of things you get for single subscription is kind of crazy. So again, go check it out and thank you to Kilo for sponsoring this segment of the video.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 88

So anyways, let's jump back to see where the coding agent is. So it's creating the back end. Okay, we can see that the this is a proper codebase structure. It's actually taking way longer than I expected. And as you can see, Kilo has these nice checkpoints that you can return at any moment in case something went wrong.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 89

Right now, obviously, if you work on a serious project, you should learn how to use Git. A lot of you guys are scared of Git for no reason. Now, Git, not to be confused with GitHub, is a version control system, right? So, it allows you to do basically these checkpoints, but in a much more scalable and professional way. And again, if you're just building a quick MVP, maybe you don't need to set up a git repo.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 90

But if you are working on something for more than a day, okay, more than a day, you absolutely do need to set up a git repo. And that's where GitHub comes into place because GitHub is a way to just store different Git repositories, right? But Git is just a version control system. So if you want to visit a version before you did a risky change, right? An older version of your code, that's where Git comes into place.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 91

So these checkpoints, don't let these fool you, okay? A lot of vibe coders would rely on these checkpoints. That's a huge mistake, guys. Just learn how to use git. It is super simple.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 92

Actually, in the new society, we have a module just on this, right? So this is inside of the classroom right here in the section code with AI. Here we have GitHub fundamentals which explains both Git and GitHub. And this is just one of many different advanced tutorials you can find in the new society. So, if you are serious about coding with AI and you want to go to the next level, join the new society.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 93

It's going to be linked below the video. Now, back to Kilo. Let's see if it's still going. And it is still going. Wow, this is very impressive.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 94

This coding agent can run for 10, 15, 20 minutes at a time. And you can see on the right, we have a proper codebase structure. So, this is not just VIP coding a single file. This is building and architecting a codebase so that you can turn this into a serious project, right? And maybe I overkill this to be honest or u a quick demo, but yeah, this coding agent can go for a while.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 95

And at the top you have control. So right now we've spent $0.5. So this is if you used like Sonet 4.5 or especially Opus. If you used Opus 4.1, we would already be like 10, 20, $30. But with GLM 4.6, so like you can you can probably start seeing the beauty of these extensions.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 96

Right now, as I promised, let's talk about clo and codex because when should you use these two, right? They still have a time and place and I still use these every single day. Now, these extensions are starting to take up more and more of my time, right? So, maybe like two months ago, I wasn't using client ru or kilo, but now I'm using this more and more. Why?

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 97

Well, there's many reasons, right? Uh, personally, I don't really care about cost, but that's because of, you know, I'm building my software company. I'm building vector. So cost is part of the cost of building a software startup, right? But also because I'm interested in the best performance.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 98

And now that we see in the last two or three months especially, we see open source models catch up to the cutting edge of the best coding performance. I'm starting to use the extensions more and more, but also because of the fact that they can run for so long without like you having to constantly step in and because of the fact that they give you a complete overview of the tokens, right? And nobody likes to get scammed. Even if you have a high budget, you like to, you know, get your money's worth. And the fact that we still haven't spent a single dollar, like less than $1.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 99

And look how many files have been created. Look how much output. Like this is crazy. So yeah, when should you use close calling Codex? To answer the question, right now Codex, especially with the new GPT5 CEX model, which by the way also is available inside of open router.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 100

So you could use this model. You could use this inside of kilo or rue or client if you wanted to. This model is currently the best, right? And with codex, you have different reasons. It's not just a coding agent inside of your IDE.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 101

Codex also gives you the cloud-based agent, right? Which you can dispatch like 10 different tasks to and which can work on multiple things at once asynchronously. But Codex also released the GitHub PR review. So any single pull request you make, COEX can look through it and spot any potential issues. So that's another thing, right?

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 102

Codex isn't just one product. It's like four different products right now. Speaking of cloth code, it's mostly used for the terminal, right? Close code, you can launch it, use it in the terminal, and it's like a autonomous agent which really pioneered the AI coding agents earlier this year. It's kind of crazy that all of this happened in 2025.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 103

Now, as I said, I still use both of these. The main main reason for cloud code is that the UI in the terminal is just polished. It's the best UI in the terminal out of all of these. Codex CLI is getting better, though. They just shipped, I think, the 045 0.45.0 update, which is very nice.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 104

It improved the UI of the CLI, but it's still not as polished as cloth code, right? And I do have to say I am using cloth code less. Some of that has been delegated to extensions like kilo. As I said, kilo out of these three is my favorite, but some of that most of that actually went into codex. So, codex is the most powerful right now, bar none.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 105

And that is mainly because of this model GPT5 codex. This model on medium and high reasoning effort is a beast. It's the most reliable model. It just puts in the work. It spends like if you are doing a risky change, it can spend 10 15 minutes just planning and preparing before it writes any code.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 106

Now the beauty of this model, what OpenAI really cooked is that if it's a simple change, it will not spend 10 minutes, right? it might reason for 30 seconds and then get to work. But if it's a risky change, either like a big refactor or like a massive new feature, it will put in the work, especially on the high reasoning effort. And this model is amazing. So yeah, Codex, the beauty of that is it's integrated, right?

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 107

So you have the Codex uh web agent. Let me show you that. Boom. Right here. You can give it multiple tasks and then inside of the codeex in uh your extension right here, you can browse for these tasks whether cloud-based or local based and you can just continue working on them like that which is huge.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 108

Here you can literally if you're like listening to your users right maybe here you guys can comment different ways to improve vectal which by the way we have the discord server so if you are a vectal user you can just talk to me or the developers any day and you can suggest features and that's why this is the fastest improving software in the world and that's why we're going to build this into better chat agent than JGBD especially for productivity and task management that's where we're focusing right and a big part of that is the discord server when anybody can suggest features and I can just dispatch a codex task like that, right? So, if you are a vector user and you spot a suggestion or you spot a bug, me or the other devs, we can just instantly deploy a Codex cloud-based agent on it, right? And then it can be running multiple tasks in parallel and then when I actually have the time to clean those up, I can just jump in, boom, load up my Codex extension, load up the cloud-based tasks, and just work on them one by one. So, this is the beauty of the Codex. It's a whole ecosystem and OpenAI is really cooking and they're coming for Enthropics launch.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 109

They're not only coming for Enthropic launch, they're coming for Crystal's launch as well. So yeah, OpenAI is on fire and that's why if I had to choose a single coding tool, it would have to be Codex. But these extensions are not to be overestimated. They're shipping fast. The teams behind them, they're aggressive, right?

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 110

They're putting in the work. They're shipping features, new models all the time. And the issue with these two they will always stick to their models only. Right? So right now cloth code if it allowed other models would be amazing.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 111

Now obviously there's ways to use the cloth code UI through other models but by default cloth code only counts with enthropic models. Same with codex. Codex only counts with openi models which currently that's not an issue because they do have the best coding model in the world. But when they when they don't you know could be next week could be next month. Hey Gemini 3 is around the corner.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 112

Gemini 3 might get released any day now. If it is really that amazing, you cannot use it inside of Codex, right? And then we will see the rise of Kilo. We will see the rise of RU. We will see the rise of client.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 113

People will be wanting to use Gemini 3. Now, obviously Gemini has the Gemini CLI, but it's probably the worst CLI tool out of them all currently. Maybe they improve it. They will probably improve it knowing the team behind Google and Deep Mind. These guys are cracked.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 114

But you can see that the code, the landscape is changing, right? The landscape is changing literally week to week. So while cloth code was the king two months ago, it is not the king anymore. Currently the king is codex, right? But uh hey maybe next month it will be kilo with Gemini 3.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 115

Or maybe GLM will release a new model GLM 4.7, right? Or maybe Deepseek finally drops Deepseek R2 or Deepseek V4. Who knows guys? Who knows? The key is to stay adaptable, right?

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 116

That is the secret. People ask me, David, what is the secret? The secret is to stay adaptable. The AI space is changing all the time. If you are one of these people who just uses a single tool and like refuses to experiment, refuses to learn, you will fall behind inevitably.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 117

You have to be willing to adapt and you have to be willing to try one new tool a week, right? This is the things that I'm teaching people in the new society. No matter what is your setup, you should master your setup. What every single week, you should try at least one new tool. And if you want to be on the cutting edge, join the new site because this is where I can share the thoughts.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 118

This is where you have the people who are on the cutting edge of AI coding and who really are sharing all of the latest things when it comes to startups, when it comes to AI agents, when it comes to AI coding. If you are serious about programming with AI and you want to take your skill set to the next level, join new society. We have tons of advanced content such as how to build an AI startup, how to build a real AI business, or how to build your first AI agent. A lot of you just want to get into it. You want to improve your skill set to prepare yourself for the future.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 119

New Society is the place for that. Plus, every single week we host a call where you can join, ask any questions, get hands-on help, and this is the easiest way for you to speak with me directly and, you know, pitch me startup ideas, ask me any questions, whatever you want to do. New society is the place to do it. So again, the New Society is going to be linked below. Yeah, hopefully you guys find this video valuable and I have a feeling that this is not going to be the last video on these coding extensions.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 120

With that being said, thank you for watching and have a wonderful, productive week. See you.

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´01æœˆ28æ—¥

</div>
