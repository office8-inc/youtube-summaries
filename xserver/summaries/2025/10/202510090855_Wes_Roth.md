# ğŸ“º OpenAI, Google and xAI are about to blow

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: OpenAI, Google and xAI are about to blow
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=8ASNQxgOYYM](https://www.youtube.com/watch?v=8ASNQxgOYYM)
- **å‹•ç”»ID**: 8ASNQxgOYYM
- **å…¬é–‹æ—¥**: 2025å¹´10æœˆ09æ—¥ 08:55
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

All right, some big things are happening in the land of AI. I took a few days off and I feel like I'm really behind. There's tons of AI video news. VO3.1 might be dropping soon. We already see it listed in Hicksfield AI and that probably means that we're going to see the actual full version roll out soon.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

XI is not far behind, also releasing their AI video model. Google Gemini drops this, the new state-of-the-art Gemini 2.5 computer use model. This is just the first step in the Gemini computer use story. This is very exciting to me because this was one of the weaknesses that we've seen with these AI models. They were not very good at navigating the web using the computer.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

At first glance, this thing is looking terrific. And of course, really fast, let's catch up on what happened at the OpenAI dev day. We're not going to go too deep. I did a live stream. We kind of watched it happen live.

### ğŸ“ è©³ç´°èª¬æ˜

But first and foremost, we have apps in Chad GPT. kind of like the app store. Chad GPT will also have all sorts of apps in it. A lot of existing apps will be connected. So Zillow was the one that they've showcased.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

We also have Spotify, Booking, Canva, Corsera, Expedia, Figma. Seems like anyone can join. They do have some requirements which they outline, but it seems like if you have an app, it now might be featured on Chad GBT. If this gets traction, certainly seems like this will kick off a race of how to get your app to be used by ChadBt. similar to how people figured out how to climb the search engine optimization ranks in Google or in the iTunes store.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

What does Chad GBPt want to see before it will recommend your app? They also announced agent kit including agent builder. Basically, a very simple way to build these agentic workflows without needing a command line interface, without needing to know how to code. This again seems like it could be a pretty big deal because it's going to allow anyone to build these workflows connected to tons of other apps or MCPs, whatever software you want to connect into it and automate a lot of it. Also, they seem to have put a lot of effort into making it enterprise friendly.

### ğŸ¯ å¿œç”¨ä¾‹

This is kind of how it looks like to build your own workflow. And the enterprise friendly stuff are things like jailbreak guard rail and hallucination guardrail etc. basically making it a little bit safer to use in largecale applications and enterprise applications. In the past, I've used make.com and Zapier and those were very powerful, terrific tools. It seems like OpenAI is kind of gunning for those.

### ğŸ’­ è€ƒå¯Ÿ

Now, of course, it seems like right now it will only be usable with the OpenAI models. So, there's still a lot of room for competition. And if you want to bring your own models, if you want to be able to switch to anyone's models, you know, this sounds like it's not going to allow you to do that. Certainly not right now, but definitely something we'll be playing around with very, very soon. We also have tons of new announcements like sort 2 and the API.

### ğŸ“Œ ã¾ã¨ã‚

Codex has launched with new features like Slack integration, Codex SDK, and enterprise controls and codeex. It's now generally available. That was the announcement. We have GPT5 Pro in the API. We have GPT realtime mini, the much less expensive model.

### âœ… çµè«–

the smaller voice model and the GPT Image 1 Mini. So, I'm definitely keeping my eye on these two depending how big of traction they get, how quickly they scale, how many people start using it. I mean, this could be a big deal. So, stay tuned for that. All right, but let's take a look at Google Gemini, the Gemini 2.5 computer use model.

### ğŸ“š è¿½åŠ æƒ…å ±

So, these are basically AI agents that can interact with user interfaces. So think using your computer, browsing the web, etc. They have a few demos here that they showcase in terms of how this agent is able to navigate the web. The scores on the benchmarks are looking pretty good. We've had two new systems drop recently, one from Anthropic and this one that seem like they would be very very good at navigating the web.

### ğŸ”– è£œè¶³

So computer use. So again, this is another sort of bottleneck for the ability of these AI models to do things because if you can't navigate the web successfully, there's tons of use cases that are kind of locked out. So here it seems to be doing pretty well at filling out this form. So this I think is a conbon board as what it's called where you kind of move projects along on the board or something similar to that. But here it seems like it's doing pretty well.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

So, it's moving them around as needed without too many issues. Even though this is not sort of a default user interface, it still seems to navigate pretty well, which is exciting. So, on the benchmarks, it's looking very good. Looks like it's better than OpenAI computer using agent model, and it seems like it beats out the Claude Sonnet models as well, including the new 4.5. So if these are accurate, this would be the best sort of model available to us right now.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

Of course, you have to make sure that you test for your own use cases. But it certainly seems like these sort of iterations on these computer use models, they're getting better and they're getting better pretty fast cuz they were lagging for quite some time. So here in this chart, as you can see, so it's very accurate while also being very, very fast. This is where the competition is clustered. And this is Gemini 2.5 computer use.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

So definitely far and away the best one if you're looking for accuracy and latency, low latency that is. Now the reason this is important one is of course for navigating the web. Project Mariner is that online researching agent that goes and navigates the web for you. But notice they also have the Firebase testing agent. And this is very interesting to me because I always wondered when we're going to have AI as it writes code, some sort of a UI or a video game perhaps.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

At what point will it be able to actually go in there and see what it built and test the video game out or test the website it built out? There's no reason for it to not test its own work. or if you're a developer, there's no reason why you can't get a hundred or a thousand of these agents to go and test out the thing you've built to try to break it to see if there's any bugs or issues or usability problems. Once these things are humanlike in their approach to how they use operating systems and various browsers, there are a number of use cases that really open up. So, we are able to see it.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

It's in a public preview. We're able to use it in the Gemini API, Google AI Studio, or Vertex AI. There's also a demo environment hosted by browser base. So, here's kind of what that's looking like. Find and win a Min Sweeper game.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

That's one of my sort of go-to testing for these. So, let's see how well it does. And you can see the session timer here at the bottom. It searched for Mine Sweeper. Oh, look at that.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

So, there's a big play button that makes it easy. Oh, wow. Okay. So, it pulls up a game of Mine Sweeper. It's patiently watching the tutorial.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

See, it's showing you what the left and right mouse buttons do. It opens up the difficulty menu. So, I'm guessing it's going to try to Yep. It shifted it to easy. Smart Gemini.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

All right. So, it begins. So, first click was very good, very successful. I'm just going to see if I can identify two mines. I I will give it credit.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

So, it's struggling here just a little bit. So, it's randomly clicking around, seemingly not really getting what the point here is. Oh, and it blows itself up, but we told it to win. So, let's see how long it persists. So, we're not going to stick around for the whole thing.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

It looks like the session times out after 5 minutes, but it starts a new game. So, if I can identify one mine, that will show to me that it's like it gets what this game is. So, just put one flag in the right place and you get credit, little robot, please. So, it clicked and sort of unearthed a spot that doesn't have a mine in it. Was that a lucky guess?

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

Um, I mean, it's not using the flags. So, usually like if you use the flags, you can tell like, okay, I think this is a mine, therefore, I'm okay. I mean, we knew it wasn't a mine, but I'm not sure if it realized that or not. We don't have its sort of chain of reasoning and chain of thoughts to see what it's thinking, but it's it's definitely clicking around a lot, which to me suggests that it's not really getting the game. Yeah.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

So, I'm going to call it. So, it's not going to win the mind sweeper game anytime soon. I'm going to ask what is the background on my X profile? So, just is it able to navigate to that website, find that account, and figure out what the background is? I'm hoping it's going to just describe it for me.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

So, it does a Google search and it does it very very well. It opens up the X profile. So, phenomenal so far and you know it's able to see what it is. So, let's uh but it clicked back for some reason. So, it added one extra step in there but it it figured out what it was doing wrong.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

Came back and pretty quickly was able to complete it. So, here at the bottom you are able to see that the header image features a black cat on a shark and it gives me tons of other details. This is perfect. A+ little robot. Also, I got to note that the latency is kind of good.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

That was pretty fast. I didn't catch how long that took, but it was a lot faster than some of the other models. There were no misclicks. So, for some reason, the first page it went to, it thought it couldn't see it or something. So, when it went back out, went to a different page, but eventually came back to that first page where you could see the background and it successfully completed the task.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

So, very, very impressive so far. I'm hoping to have time to do a lot more testing because this is a very interesting sort of application of these AI models. In other news, OpenAI and AMD strike a deal. Ask Perplexity posted this. So, it's explained by Matt Lavine how that deal took place.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

What were the negotiations like? Here's kind of what that might have sounded like. So, OpenI says, "We would like 6 gawatt worth of your chips to do inference." MD says, "Terrific. That would be 78 billion, please. How would you like to pay?" Open says, "Well, we were thinking, hear me out, that we announced the deal, and that would add 78 billion to the value of your company, which should cover it." AMD looks at them, maybe blinking once or twice, but not speaking.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Open eye is looking back, emotionless poker face. AMD breaks the silence going, "No, I'm pretty sure you have to pay for the chips." Openi goes why? Also, open eye, do you see Samalin saying this? He does seem like that master deal negotiator. So he goes, why?

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

I can't do the vocal fry. But AMD goes, I don't know. It just seems wrong not to. Open says, okay, why don't we pay you cash for the value of the chips and you give us back stock and when we announce the deal, the stock will go up and we'll get our 78 billion back. AMD goes, yeah, I I guess that works.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

Uh I feel like we should get some of the value. Open goes, "Okay, you can have half. You give us a stock worth like 35 billion and you keep the rest." And AMD is up like 50% since the deal got announced. This seems a bit weird, but whatever. In other news, Grock is making some headlines.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

Grock Code is now available in Visual Studio. We also have from XAI Imagine V0.9. It's a video generation model with massive upgrades from V0.1 in visual quality, motion, audio generation, and more. So, this is the Tesla Cybert truck driving through a forest action shot. So, there's a lot to like here, but I do feel like the Open AI and Google's offerings are a little bit more advanced.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

The sound, I'm going to turn it on. The sound isn't great. It does look a little bit cartoony and most of them do have a certain look to it, a certain flavor to it. Here's that same prompt. This one is an overhead shot.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

I mean, there's things to like like the reflection. I'm curious if Xi's approach is just different from the other models cuz it's pretty obvious based on these swords to release that OpenAI and Sam Alman, they're willing to go head-to-head with a lot of the IP holders with the copyright holders. And it's pretty obvious that a lot of copyrighted works went into training those models. A lot of this stuff to me seems like it didn't have, you know, real footage going into it. So, I mean, here you can see this dragon sort of walking.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

You can see the feet and it's looking pretty good, but it's not real. It's it's it's based on either a a video game or an animation, something like that. Even something like this. Again, it's not sort of video realistic, photo realistic, however you want to call it. It doesn't look bad, but I don't think this was trained on sort of real people, real human footage.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

I wonder if they're using more copyright friendly data or these are just sort of like the the first iterations because again, when Grock first came out, it was a little bit behind the pack. Now, in a lot of ways, it's, you know, caught up. Certainly in some ways it's even at the front of the pack in terms of the other AI labs. So it' be curious to see what happens when they train the next generation AI video model with all that compute that they have available because currently as it stands now I mean it's good but it feels like it's a little bit dated in AI terms. Let's say this is from 6 months ago.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

This would be cool but I would not bet against Elon Musk and XAI. they seem to be catching up fast across the board. And last bit of news that I found interesting is that Google, Gemini, DeepMind, you know, all the stuff under the Alphabet umbrella, they've added a few more Nobel Prize winning scientists. So, in the eye, they had Hinton, Hacabus, Jumper, and recently they added some scientists that work for Google that are quantum physicists. So here's Dennis Hassabus saying huge congrats to Michelle Devore for the 2025 Nobel Prize in Physics and to Hartmoot Nevin and all our colleagues at the Google Quantum AI team.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

Proud to be collaborating with them. This is the third Nobel associated with the work done at Alphabet/Google in two years. Not bad. So very cool to see that. Let me know what you thought were the most exciting piece of AI news.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

and get ready cuz I think we're going to see a series of pretty big releases coming out in the next few weeks. My name is Wes Rob. Thank you so much for watching. I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´02æœˆ07æ—¥

</div>
