# ğŸ“º Build anything with Local AI Models, here's how

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: Build anything with Local AI Models, here's how
- **ãƒãƒ£ãƒ³ãƒãƒ«**: David Ondrej
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=811ctT3HqcE](https://www.youtube.com/watch?v=811ctT3HqcE)
- **å‹•ç”»ID**: 811ctT3HqcE
- **å…¬é–‹æ—¥**: 2025å¹´12æœˆ23æ—¥ 04:45
- **å†ç”Ÿå›æ•°**: 7,247 å›
- **é«˜è©•ä¾¡æ•°**: 405

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

My name is David Andre and here is how to install and run local AI models. Now we're witnessing the local revolution. Since AI is improving so fast, it is now possible to run very good models locally on your computer. Yet most people still aren't using any local models and just default to using CI GBD. So in this video you'll learn what local models are, why they are becoming super popular, and how to actually use them.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

So make sure to watch until the end. Now, as you can see by this graph, the gap between the cutting edge frontier models and the open-source models that are able to be run locally, the gap is getting smaller. Meaning the models you can run locally are as good as the cutting edge models maybe a year ago. And this means that it has never been a better time to start running local models than right now. But David, what even are local models?

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

Well, it's any AI model which you're able to run locally on your own machine. And by machine, I mean any type of computer. could be your phone, your laptop, your PC, a fridge, a toaster, doesn't matter. Anything that has a CPU or GPU can run a AI model. So, the more powerful your computer, the more AI models you can run locally and the more powerful models you can run.

### ğŸ“ è©³ç´°èª¬æ˜

But why don't you hear more often about running AI locally? Why isn't it more mainstream? Well, the answer is simple. Big tech companies and the main AI research labs simply don't want you to know this. And that's simply due to the fact that edge compute, which is running AI models locally on your phone.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

That's called edge on the edge. The edge compute is the single biggest risk to all the hyperscalers. Their entire business model currently relies on renting you their intelligence aka their AI models through the cloud. But if you have a powerful enough AI locally on your laptop that runs for free, then that $20 a month subscription market completely disappears. Now, you might be thinking, "But David, why even bother with local AI models?" Well, the first reason is that your data remains yours.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

When you run AM models locally, all of the prompts and data and context and files you attach never leave your machine. They stay on your computer. This is fundamentally different from using cloud-based AI services where all of the data that you send has to go to a remote server to process for the LLM because LLM is not running on your computer. It runs on some supercomput. And that's why local models are ideal for sensitive work, any proprietary code you might have, or anyone who simply values privacy and security.

### ğŸ¯ å¿œç”¨ä¾‹

Another major reason is that they're free. The cost is $0 since once you set up a local layout model, which I'll show you how to do in a second, every single query that you send, every single prompt is completely free. Doesn't matter if you send one or 1 million prompts. There's no API fees, no token limits, and no monthly subscriptions. And if you're in the AI space, you know that the subscriptions can add up real quick, especially if you use the higher tiers, like the $100 a month, $200 a month, and especially if you want to use multiplayer models, Claude, Grog, Gemini, GPD.

### ğŸ’­ è€ƒå¯Ÿ

Yeah, it can get real expensive real fast. Also, if you have models locally on your machine, they work offline. So, it doesn't matter if your internet ever goes down, your local model will still work. And I literally do this every single time I'm flying. I have multiple models locally downloaded on my MacBook and I use them for coding, brainstorming, learning concepts.

### ğŸ“Œ ã¾ã¨ã‚

It's a huge unfair advantage over everyone else on that plane. And overall, it's just useful to have these models in any worst case scenario. But these are the obvious points. There are much more nuanced and advanced reasons why you might want to use local models. First off, the model bias is in your favor.

### âœ… çµè«–

Right now, there is a clear ideological agenda baked into all of the mainstream AI models, right? So nearly all of the AI research labs are based in San Francisco. And if you are paying attention, San Francisco has a very particular worldview that you probably don't want your AI models to have or your kids to have. But unfortunately, the popular AI models do have those beliefs. Local models are different.

### ğŸ“š è¿½åŠ æƒ…å ±

They can be fine-tuned. They can be uncensored. Nobody can remove them from your computer. Nobody can change the system prompt without you knowing. Simply put, the model is yours and doesn't have any hidden bias that you're not aware about.

### ğŸ”– è£œè¶³

Also, you have the ability to fine-tune them and turn them into fine-tuned versions of the local models. So, in case you don't know, fine-tuned models are AI models that are trained further on specific data to excel at that particular task. And that can also be writing in your style, having your own legal assistant, building a custom chatbot for your own company based on the company data, million different use cases for fine-tuning. And since most local models are open weight, meaning you have access to the model weights, to the parameters, this is what allows you to fine-tune them. You need to change the parameters, change the weights so that the model specializes on that new data that you're finally tuning on.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

By the way, I made a full video on fine tuning right here. So, watch that after this one. So, what are currently the best AI models to run locally? Well, the easiest way to check is to go to this website artificial analysis.ai. So, this is artificial analysis and this is a independent benchmarking platform that compares AI models across multiple different things, speed, price, output, latency, and so on.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

If you go to the models on the top and go to opensource models and click on that, it'll bring you to this page where you can see all of the different opensource models and how they rank. Right now, Kim K2 is winning overall. However, you cannot run Kim K2 because it's like one trillion parameters. So, we're interested in these medium models, small models, and tiny models. The way to explain it is that tiny models all of you can run, right?

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

Even if you have a good phone, like a newest iPhone or whatever, you can probably run these even on your phone. But it it gets more tricky with small and medium. small models. This is if you have like a decent laptop, right? 4 billion parameters to 40 billion parameters.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

These aren't like super small, but they're still quite small compared to, you know, cloth 4.5 Opus or GPD 5.2. But medium models, this is where you need a powerful machine, right? You need a computer that's at least $5,000, $7,000. You need at least 48 or 64 GB of VRAM on your machine to even be able to run these at good speeds. And here, GPD OSS 12B is currently the best.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

But the reason I'm showing you this is that this changes all the time. Literally every single week there is a new powerful AI model released that's open source. So instead of telling you what's best right now, you just have this resource to go to artificial analysis, check the category that you can run on your machine and see what's currently the best across different metrics, intelligence, evals, different specific benchmarks. Yeah, there's a lot. So I'm going to link it below.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

So now let's get to actually running an AI model locally. So, first we need a tool that can actually run more or less locally that can let you download them and use interact with them. One of the best options is Alam Studio. By the way, this is not sponsored. It's just a great tool.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

So, when you get to the homepage, you can see what the UI looks like that it has all the same things that JGBD has basically. And you can choose between GPD, Quen, Gemma, Deepseek, and way more models. If you want to see the full list, just go to the top, click on models, and scroll through it. They have a lot of models that you can run locally. Anyways, let's go back to the homepage and let's click on download.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

There we go. Once the install is downloaded, simply double click it and then drag LM Studio into your applications folder. Obviously, if you're on Windows, the setup will be a bit different, but hey, if you're able to download any app, it's literally the same for LM Studio. And if you're doing this for the first time, it might be some wizard with steps to walk through. So, just click through them.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

Nothing crazy. It's standard for any other app. However, if this is your first time using LM Studio, you will definitely be in the user mode, right? So, at the bottom, you can see there's three different modes of the app. User, power user, and developer.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

Obviously, developer gives you the most options, while user is the most simple and intuitive. So, for this video, I'm going to show you how to use the app in developer mode. Now, if you're curious what model I'm using, well, there's only one open source local model that has 1 million context window, aent capabilities, and requires only 24 GB of VRAM. And that model is Nemoron 330B. This is part of the Neotron 3 family of models from Nvidia, and it's an open source model.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

It utilizes a hybrid mamba transformer architecture with mixture experts. So let me break this down. Transformer is the most consequential architecture in all of AI. That's where the original attention is all you need paper comes in from 2017. This was invented at Google and this is what really caused the breakthrough of LLMs.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

Mamba is a different architecture for models and this family of Neotron 3 which is the name Neotron free models combines them right. So that's why it's a hybrid architecture of Mamba and Transformer. But on top of that, it's a mixture of experts. This means that it's not just one single model that's responding to everything. Instead, the model is split into different experts which each of which are specialized in say one is math, one is language, one is creativity, one is you know programming and based on your query, you can just activate only the experts that are needed.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

So you don't need to, let's say there's eight, you don't need to activate all eight experts. Instead, you just activate the three or four most relevant ones, which allows you to run a bigger model on a smaller machine, less powerful machine because you don't need to activate all parameters or 30 billion parameters on every single query. And right now, Nemoon 3DB, I would say, is the most powerful small open source model out. And again, this can change in a week or two. So, hey, check artificial analysis.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

And here's how it performs on seven or eight different benchmarks. So we have chat math instruction following tool use coding coding and long context and it's best on all of these basically better than GPOSS20B and quenb. So yeah, Neotron 3 nano 30B A3B which means there's only active 3 billion parameters at any point right now. This is a very very strong model. So if you can run it on a computer amazing this can do 90% of things that JGBD can.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

Now let me explain a bit more about the Mbach transformer architecture because this is a really interesting part that most people don't touch about because they're intimidated of more advanced knowledge. So Neotron free nano uses 23 mamba layers for the fast and efficient processing of long texts and it can do up to 1 million tokens. So this is like Gemini level. This is really good context window. But it also uses six transformer attention layers for the precise reasoning when needed.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

So the mamba is for the fast responses. Transformer attention layer is for the deep reasoning where it really does a lot of thinking before answering. So basically mamba handles the speed, transformer handles the smarts. And this combo gives you 3.3x faster inference than a pure transfer model at a similar accuracy. So it's another way to optimize models to run faster and on smaller, less powerful machines.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

And this is what I mean when I'm saying that the progress on local AI models is even faster than the crazy progress on the cutting edge AI models. So if you're not doing AI locally, if you don't have LM Studio set up, if you don't have multiple models downloaded, you really are falling behind. All right, so let's go back to LM Studio and let me show you a bit more. So again, turn on developer mode at the bottom. Go to the left, click on the search, and here you can do mission control and click on model search.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Here we're going to search for models. Select Neotron free nano or any other model you want to run. There's literally all shapes and sizes from like 1 billion or even like less than 1 billion parameters to 230 billion. So yeah, if you have a eight different Mac Studios linked together, you can still run some of these beast models. Anyways, I'm going to go Nemo Neotron free nano.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Click on that. And on the right we have more details about the model, right? We have the full name, format, parameters, architecture, domain, and then we can see the download options. So if you click on show all options, you see like there's nine different models that you could download. But LM Studio already chooses the one that's ideal for your own machine.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

So MLX, let me explain the difference because this is important. GGUF and LMX, you should understand this. GGUF is the native file format for the LMA CPP library. That's another video uh entire video. So if you want me to go deep into llama CBP, comment below.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

But basically this works on any operating system, right? While MLX only works for Apple silicon. So not only Mac OS, but only Apple silicon. So like M series chips, right? So if you're on Windows or Linux, go GGUF.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

If you're on older Apple computers with Intel CPUs, choose GGUF. But if you have a newer MacBook with Apple silicon chips, choose MLX. Also, here you can choose different quantizations. So you can see like these are the quantized 4bit, 5 bit, 6 bit, 8 bit depending on how powerful your machine is. So you can see that the the higher ones are bigger files, right?

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

So 8 bit is 33 GB while 4bit is only 17 GB. So this also depends on how powerful your machine is. So here is a nice visual way to understand quantization for AI models. So on the left you have the high precision model, right? Think of it as a car with a huge engine, aka the detailed weights that are high precision for this AI model.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

But because the weights are so detailed, this makes the model slower and power hungry. What quantization allows us to do is compressing and simplifying the model. So the quantized weights are are less precise. So they have less decimal points. They're not as accurate.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

But this allows us to run the model on local devices such as phones or laptops because the model aka the car in this analogy requires less power and gives you faster inference. So simply put, quantization trades a tiny bit of accuracy in the model responses for massive gains in speed and efficiency, making it possible to run very powerful AI models on your normal devices. Now in LM Studio, if you don't have a model installed, so let's say this GLM 4.6V flash, I don't have it, you'll see that you can have the download button. It's actually below my webcam. So, I'm going to move my camera.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

Boom. This green download button in the bottom right. So, Nemoron free nano. I have it downloaded. So, I can just see use in a new chat, right?

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

And if I click that, I can just begin chatting to it. Explain the secret of mathematics. And boom, it's now loaded. So, super fast response. Secret about what makes it so powerful isn't hidden spellic false.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

It's a human crafted language of pattern logic blah blah blah abstraction. You can see it gave very detailed answer nicely formatted just like you would get in CHBT or Plexity and the token generation speed is just amazing and again over 100 tokens and even better than CHBT you can see the details right so again this is for developer mode you can see the total tokens and time to first token I wish more AI chatbot tools show this but going back to the left to the mission control here we can also do full GPU offload so let me move my camera back there we So, so here you have full GPU offload possible. Another pro tip you should know is that when you have LM Studio at the top, you want to select the model and you want to toggle manually choose model load parameters. Then choose the model itself. So I'm going to do Neotron 3 Nano.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

And here you can see the tokens for the context length. So I'm going to set it to the max allowed, which is 260K right here. And I'm going to click on remember settings and then load the model. That way I know I'm using the full context window for this quantized version. Another pro tip is to go to the top right, click on the wrench and here if you click on custom fields, you can either enable or disable thinking for that model because some models are reasoning.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

So you might want to have them reason to give you a better answer. Or if you want them to answer more quickly, you disable the thinking so they answer instantly. You can also go into the settings and change the temperature of the model. So if you want more consistent responses, do zero or 0.1. If you want more randomness or more creativity, do like 0.9, 0.7.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

And you can go even further into sampling structured outputs and way more options than you can select in JBD. So when I say that this is for people who are serious about AI and that running models locally is for the power users, I'm not joking. It's really powerful and the UI gives you everything you need and even more than you need. Another thing you should know if you want to master LM Studio is going to the developer tab on the left, the green button. And here you can see that it's status running which means we have the server reachable at this local host uh URL.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

This means that we can use it to build apps right. So if you're like building a AI SAS locally and you don't want to be spending any credits with OpenAI or Enthropic or maybe you're using sensitive data and you don't want to send them to third party companies, you can use LM Studio as your back end and just hit this endpoint with your front end or your back end to generate responses from models that are locally on your computer. And anytime you hit the server, you're going to see logs right here inside of LM Studio Terminal. But if we go back into chat, so again, top left and click on the yellow chat icon, we can also create different presets, right? So this allows us to save system prompts or different temperatures or custom fields, whatever preset you want.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

It's like a in JGB you can do projects. This is the similar but way more advanced, right? You you have so many more fields than CLGBT. It's kind of crazy. It's might be even overwhelming for beginners, but this is why there's three different modes, right?

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

If that's overwhelming, just start as a user, work your way up to a power user, and then go into developer mode once you feel that you're ready for it. And also anytime you're chatting you can just enable or disable thinking right here on the go just even faster. And of course Elm Studio has all the classic stuff like attaching files uh which works with rack or third party integrations and plugins including MCB servers. So you can connect um everything to Studio basically and use it with your GitHub using your Google Docs, Google Sheets, Codeex, Vectal, Cloud Code, whatever has MCB server. Now as far as keyboard shortcuts go command L is one of the most important ones.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

This is for loading the model. So L for loading command L then command N is new chat. So N new easy to remember new chat and then command shift H is for GPU controls right? So this again can go mission control and hardware but you can do command shift H remember it for hardware and this will show you all the stuff about GPU and how you want to allocate it how much you want to dedicate towards LM Studio and what your stats are actually like. So now you know how to run AI models locally.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

But let's say you want to take it step further and you want to actually launch your own AI business. Well, I just made a full video on that topic. What is the best AI business model to start in 2026? Click here to watch it

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2025å¹´12æœˆ24æ—¥

</div>
