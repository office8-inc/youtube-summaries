# ğŸ“º Vibecoder is jailed and strip searched...

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: Vibecoder is jailed and strip searched...
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=49rrxFP893c](https://www.youtube.com/watch?v=49rrxFP893c)
- **å‹•ç”»ID**: 49rrxFP893c
- **å…¬é–‹æ—¥**: 2026å¹´01æœˆ24æ—¥ 14:01
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

All right, there's been some pretty big news in the world of AI. First and foremost, we've all heard concerns about jobs going away, AI automation. What happens in the sort of new economy, but we're finally beginning some real efforts being put into trying to figure out what are we going to do? Google is planning to hire a chief AGI economist to start cranking away this problem. Hopefully, it's not too late.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

In other news, Vibe coders are being arrested and even strip searched as security personnel give them the fifth degree about what is this code that they wrote. They of course can't answer since it's been a mostly vibe coded. They're not 100% sure what the code is, what's written in there. The whole thing is kind of wild. And finally, of course, you've heard of Sakana AI if you've been watching this channel.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

They're a relatively small lab out of Japan. Every once in a while produces just the most mindblowing research papers mostly centered around recursively self-improvement in AI. The Red Queen was their latest one. You will never guess who they're partnering with. All right, I'm going to spoil it for you.

### ğŸ“ è©³ç´°èª¬æ˜

It's Google. They're joining forces with Google. You also might be noticing that there's a sort of coalitions that are being formed within these AI labs. Google and Anthropic are getting very close now. They're pulling in Sakana AI.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

There's a lot more consolidation happening, which might be a normal thing, but amongst the big wicks that flew to Davos. The rumor is everyone's ganging up on OpenAI. In the meantime, OpenAI is getting ready to launch a host of new stuff. This one is going to be a big one. Let's dive in.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

So, first and foremost, Shane Le, one of the co-founders of Google DeepMind or or DeepMind and then that later became Google DeepMind. We've covered some of his recent quotes that he said in interviews on podcasts. He's basically saying that, you know, we had this system for a long time. The system was that all of us could contribute our physical and cognitive labor, contributed our abilities, our work, and in return, we had access to resources. And that's a really great and sort of broad way of putting that because it can be applied to pretty much any time in history.

### ğŸ¯ å¿œç”¨ä¾‹

Today you work whether it's a physical job or knowledge work or whatever and you get paid with money, right? You get a paycheck. That's your access to resources. You can go and buy whatever you want. But that system has existed for a long time before we had paychecks and money and W2s, etc.

### ğŸ’­ è€ƒå¯Ÿ

Even in the days of hunter and gatherers, it was still kind of like that. you knew how to make baskets really well. You made baskets and if you were fast, you could hunt. You would go and hunt and in return you would kind of get some resources, food, a place by the campfire, whatever. So Shane Le is saying basically, and he's not really mincing words, he's saying AGI when it comes will likely break that system.

### ğŸ“Œ ã¾ã¨ã‚

It'll disrupt it. Now, of course, we've heard people talking about this before. That's not exactly news or at least it's not a new theory. Like we know there's a lot of people going, "Hey, like something's coming. What are we going to do?" And very few people have actually put together any big proposals.

### âœ… çµè«–

There's a few very smart people working on it, but it it just feels like we need more. So, I was extremely excited to see this. This happened January 22nd. Shane Legacy saying, "AGI is now on the horizon and it will deeply transform many things, including the economy. I'm currently looking to hire a senior economist reporting directly to me to lead a small team investing post AGI economics.

### ğŸ“š è¿½åŠ æƒ…å ±

The applicant, the person that gets hired, whoever they are, will most likely have the most badass job title on the planet. At least it's a very novel job title, chief AGI economist. So, if you're in that field, definitely this sounds like a once- ina-lifetime opportunity and very, very important work as well. Now, somebody questioned that like what is the time frame, what's on the horizon. Shane leg answers this uh could be surprising for some people.

### ğŸ”– è£œè¶³

He's saying 50% chance of minimal AGI by 2028. That shouldn't be surprising. We've heard a lot of sort of estimates and guesses that are in that range in that area. But here's the surprising part. So, he continues, as I've been saying publicly since about 2009.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

So, this isn't a recent prediction. He had this prediction from before most people were aware of this or even thinking about this. So again, here's a very smart person with a very good predictability of how this tech will progress that's saying something, right? He's saying, "Hey, like this economy that that we know and love and we're used to, jobs, stuff like that, that's going to go bye-bye. What are we going to do?

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

We should start thinking about So, just kind of add this as just one more sort of data point um if you're trying to figure out where things are going. Next up, an entrepreneur's 13 hours in Davos jail. His crime vibe coding. Evil vibe coders go to jail. So, let's find out what happened here.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

Let's get to the bottom of this. So, this is Sebastian. He says he flew to Davos to pitch his startup. Instead, he spent 13 hours in a Swiss jail cell because his prototype looked like an IED. An IED is an improvised explosion device.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

So, kind of a jerryrigged bomb, if you will. Here's kind of what it looked like. Certainly, you could understand why it was mistaken for one. Fun fact, once in a conversation, I accidentally said UTI instead of IED. I just got those acronyms jumbled up.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

I don't know if I'll ever live that down. I'll be honest. So, Sebastian continues, "To me, it's an MVP to the WF police. It was a category A threat." But you see what I mean? There's just so many acronyms.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

You're bound to confuse one with another. It seems very possible that you can easily confuse those two. I don't think it's that big of a deal. Why they were laughing at me for 5 minutes, I I I I don't know. Anyways, he continues, "Here's the story of the most aggressive product validation in history.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

his crime. He wanted a salmon roll. If that was a crime, I would have the death penalty. I'm sure salmon sashimi, I think, is just the greatest food that mankind has ever produced. It's basically cuts of raw salmon.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

So, it is raw fish. Although, there's some process unless you're getting it like directly from the source and eating it there. But, it it's not cooked. And you do have to really get kind of like the high-end stuff, the quality stuff. But one of the most delicious things on the planet, I think if you haven't tried it, make it a goal to try one before you know you check out.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

So, I understand Sebastian here. I feel for him. He wanted a salmon roll. So, he's at Grand Hotel Belvadier. He spotted a tray of salmon rolls.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

He sets the device down on a side table for 3 minutes to grab a snack. Here's kind of what that's looking like. When he turned back, it was gone. He asked the bartender. He pointed to security.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

Security pointed to the police. The police pointed to a special ops team. This is the culprits. It's rye exposed wiring, 3D printed case, hot glue blobs. In San Francisco, we call this agile development in a high security zone protecting world leaders.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

They call this probable cause. I tried to explain it's not a bomb. It's a hardware wallet for contract intelligence, right? So, he's like, it's not a bomb. It's crypto.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

And they're like, take them away, handcuff them, throw them in jail because it doesn't matter. Whichever one it is, put them in jail. So the interrogation was a spy thriller. They fingerprinted him to check if he was part of the international spy database. They confiscated his sleeping meds because they thought they might be cyanide capsules.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

They issued this formal way, the expulsion order by the police. I I love I love saying that. I'm sure I'm saying it wrong, but it's fun. So, here's where it gets interesting, I think, for us AI people because uh you can imagine yourself maybe running into the situation. By the way, as I'm recording this, so I'm pretty much always running a cloud code, I feel like.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

Uh and every once in a while, I kind of um run in there and, you know, tell it to continue if it has questions or give it new tasks, etc. But here's the thing. I've never seen the code. I I'm ashamed to admit I don't actually know. I mean, I know what it's supposed to be creating because I told it what it's supposed to be creating, but but I don't look at the code.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

So, I I have no idea what's in there. So, imagine if somebody's like, "This has malicious code that's going to be used to do these criminal nefarious activities. How do you explain yourself?" You're like, "I have no clue. Like, I just told it to make a game." By the way, Ethan Malik is trying to post one game a day. He's trying to create one game a day with uh Cloud Code.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

This is today's game, Grid Rogue. If you've ever played Bolatro, this is Batro, but for tic-tac-toe, actually kind of brilliant. I'm not a fan of the tic-tac-toe part of it. The idea of taking kind of the Bilatro and then adding it to some other there. There's something there.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Adding it to some other game that that definitely feels right. So, this vibe coder is detained. The script searched, his his medicine is taken away. And here's the crazy part. tried to get released, they called in a forensic technical expert whose name was Chris.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

It's interesting that that he puts uh uh Chris in in quotes, but because yeah, it probably was not his real name, but he probably introduced himself as Chris with a heavy Swiss accent, I can imagine. And so Sebastian had to explain to Chris that he's not even a hardware engineer. He's a cursor AI power user. All right, so he uses cursor to vibe code. He's saying he says 43,000 agent runs and he said I admitted I vibe coded the firmware using cloud AI.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

Chris did not blink. He sat down and audited his Rust code line by line. The best part because the code was AI generated. Chris actually had to explain his own architecture to him. Right?

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

So he's like what do you have in this code? Sebastian doesn't know. So Chris is like well this part does this, this part does this, etc. But Chris verified the workflows. His verdict to the state prosecutor was totally good.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

He was actually impressed by the rust structure. Validation comes from the strangest places. Yes, in in jail from the person investigating you that comments on how good your vibe coded structure of your rust code is. So he's reviewing the Swiss prison system five stars. I don't think anybody would give the US prison system five stars.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

I think it's only that's reserved for the the Swiss professionalism. Officer Meyer was a pro. Five stars. Security. They stripped searched me twice.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

I guess you do get five stars if you're rating it as in how good their security is, not how uh you know invasive it is cuz then it would be one star. And of course, Lasagna five out of five. He said honestly 10 out of 10. He says he walked in a founder. He walked out a criminal and walked out a founder again.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

So I just just really wanted you to know the story because to me it has everything, right? It has a drama, Davos, vibe coding, a person who's obviously incredibly optimistic and it just sounds like I don't know this person, but he sounds like just a wonderful awesome person. I mean, can you imagine going through that and then writing such a hilarious and entertaining and lighthearted story about it? The story has everything, drama, intrigue, and also of course salmon roles. A crucial part of any story arc, I think.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

Next up, we have Sama himself saying, "We have a lot of exciting launches related to codeex coming over next month, starting next week. We hope you will be delighted." And this is where it gets interesting for me immediately. I mean, it's it's interesting. I'm looking forward to what they're releasing, but this part, so they're saying we're going to reach the cyber security high level on our preparedness framework soon. We have been getting ready for this.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

So, OpenI has their preparedness ready framework, right? So you have sort of the the categories of risk plus how risky different models are. And so we're beginning to see them kind of slowly move up in terms of what risk a model could potentially have when it's released into the wild. We have low, medium, high, and critical. I believe we've hit medium categories, and I I don't think we had any high categories yet.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

And the categories are cyber security, CBRN, persuasion, and model autonomy. And we've seen this thing kind of crawling up. I, you know, model autonomy is at medium um their one of their models. And of course, how it works is the the model score is the highest risk in any category, right? So if you get one high, then the the total risk is high.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

But what they're saying is that we're likely going to see the migration of this category, cyber security, from medium into high. And Sam continues, "Cyber security is tricky and inherently dual use. We believe the best thing for the world is for security issues to get patched quickly. We will start with product restrictions like attempting to block people using our coding models to commit cyber crime, eg hack into this bank and steal money. And long-term and as we can support with evidence, we plan to move to defensive acceleration, helping people patch bugs as the primary mitigation.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

It is very important the world adopts these tools quickly to make software more secure. There will be many very capable models in the world soon. Can you figure out what this this means right here? So this is a fence. This is an image of a fence.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

That's my clue. So this is defense. This is offense. So basically in cyber security just like a lot of other fields it's kind of a game of cat and mouse the bad guys the the people trying to commit crimes right they figure out some virus or some hack right so they they they improve and so the defense the cyber security experts you know they have to improve I'm not sure what I'm drawing here but you get the idea like each one is kind of like this step function it builds on the previous ones each side kind of grows together it's it's an arms race maybe that's the best way of saying it but the point is as one side gets better, the other side catches up. The other side gets better, the other side catches up and this sort of continues, right?

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

So there's some stability as this is happening. One side doesn't completely overwhelm the other and win. And we got kind of used to that, right? But here's the problem with these advanced AI models. It's not guaranteed that this kind of stability will continue.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

Can you imagine, for example, a situation where one model can just sit there and think through a million different possibilities of how to conduct cyber attacks, then come up with ways to defend it. So basically, everything gets patched and the defense is just multiplied at 10x 100x to the point where it's near impossible to come up with any attack vectors, if you will. That might seem a little bit weird or outlandish, but maybe a more realistic scenario is what if that kind of rapid capability expansion happens with the cyber attacks. Like what if that gives a lot of people all over the world whether they know coding, whether they know cyber security, whether they even speak English or any other language, it gives them the capability to carry out attacks on banks and and websites and critical infrastructure in countries they don't like. For example, you might not have known this, but China engages in massive, sustained, and escalating cyber operations against Taiwan with 2025 data showing an average of 2.63 million daily intrusion attempts targeting critical infrastructure, government, finance, and technology sectors.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

So, it's ransomware, DDoS, social engineering in attempts to disrupt services, gather intelligence, and to destabilize the government. And as they said that's often from statebacked actors. So that means that the government funds them probably gives them technology, gives them the resources and the people themselves are probably highly technologically sophisticated, right? But the point is right now that's a massive wealthy country with people with high expertise. That's what it takes to do that.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

But you can imagine with AI that might go out the window. that might become just any guy in their mother's basement or whatever the case may be that might enable them to these attacks on a massive scale or allow them to have a higher degree of actually breaking through and causing some damage or extracting some data whatever. So whether it just increases the scale or increases the sophistication, you can see a potential where the offense, the ability to carry out cyber attacks is massively increased and just the defense is not the there's nothing that is able to scale up to that level. So the question is like what happens if there's like this counterbalance one way or another? How does the world shift?

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

How does the world change? So if we're reaching the cyber security risk high, the high level with these models, then of course some of these questions we're going to probably start seeing what's happening. In other news, Sakana AI, my favorite AI lab out of Japan, but I got to say that's the only AI lab in Japan that that I'm aware of, but it's it's my favorite one nevertheless. We've covered a lot of their stuff on this channel and judging by the views that those videos get, you all seem to enjoy it. So, first of all, thank you and of course, thank you for Sakana AI for producing such great content.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

Most of it is around the idea of self-improving AI and they're doing some great things in that department and today they are announcing a strategic partnership with Google. Google is also making a financial investment in Sakana AI to strengthen this collaboration. This underscores the recognition of our technical depth and our mission to advance AI in Japan. We are combining Google's worldclass products with our agile R&D to tackle complex challenges by leveraging models like Gemini and Gemma. We will accelerate our breakthroughs in automated scientific discovery.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

I guess another way of saying it. So a lot of the stuff that they're doing is automated scientific discovery, but a common theme in a lot of their papers is that they try to find some sort of a loop where the AI manages to improve its own efforts through each iteration. So over time you see it kind of improving improving usually using this evolutionary tree search. It's very fascinating and they're doing a lot more working with governments, financial institutions, etc. So very excited to see where they go with this.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

In other news, so Europe's is a a conference around machine learning and AI. They're saying, "We found that over 50 papers published at the Europe's 2025 have AI hallucinations." Things like a totally madeup paper, including several other papers, various uh hallucinations in the citations. They found that John Smith and Jane Doe are getting a lot of citations. They were cited by two totally different teams at UCSD and Nvidia. Ouch.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

Another example of a vibe citing authors non-existent such as first name, last name, and others. The publication dates are off by years. And these papers had to beat out 15,000 other papers which got rejected. How does this happen? Now, the person that's posting this, Alex Kui, he's saying, "We've seen a huge increase in AI scientists on GitHub with big spikes in their usage corresponding to the April September conferences.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

They do include Sakana AI, AI scientist, which yes, Sakana has their automated AI scientist that writes papers and they do have a number of things that are supposed to prevent hallucinations in the citations. In fact, in one of the citations, so there's this person, Jurgen Schmidt Huber. So that's what he looks like. And there's this little illustration. is supposed to be him with some of the most like well-known AI scientists from you know the western labs here from Google and meta etc kind of copying his answers and that's kind of like what he claims is that he invented he had the ideas for a lot of the early machine learning and AI papers and these people even though they kind of learned from him and copied from him never cited his works what's hilarious is the sakana AI AI scientists in one of the papers it actually one of the things that it talked about was from either a paper or or a book written by Jurgen Schmidt Huber.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

So he wrote it or he was a co-author whatever his name was on the paper and so the AI scientist took some ideas from that paper and in the citations miscredited to somebody else which I thought was just absolutely I mean kind of sad but also a little bit hilarious. It makes you wonder like why does it do that? Did did you see that just that nobody cites him? So it's like, oh, maybe that's what humans do. They they just don't cite this guy when it's his work.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

I'm gonna do the same thing. You know, I'm gonna take his idea and just, you know, present it to some other random person. So, I thought that was hilarious. But I think the point is that yes, these things do hallucinate citations. There are checks and review systems, automated AI review systems, but we're not at 100% accuracy yet.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 56

And so, it is possible that this is in part to blame. Although, I wouldn't be surprised. You don't really need something like this. I mean, some people, I'm sure, are just using plain old Chad GPT or whatever the chatbot to produce a lot of the documentation. So, Alex is saying when we looked around, a lot of people noticed the rise in hallucinations, but nobody was building a solution.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 57

So, we grinded over the past year to build GPT0.me/ucination-dctor. So, this is it. I haven't had a chance to check it out. First time I'm I'm hearing about this, but oh, I really hope this thing works. It works well and it can detect hallucinations and hopefully allow people to correct them.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 58

So again, this sounds like a a great tool. I I haven't again checked it, so I'm not affiliated with them. I don't know how well it works, but assuming that they're building something good, something that works. You know, Alex and the team, if there's a team, guys are doing the lord's work. Thank you.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 59

And of course, this guy on the right of this image, that's Yan Lun. Yan Leona, I think is how you pronounce it with the French accent or French pronunciation. So, he's back in the news. He um left Meta kind of loudly. He had some not so nice things to say about the super intelligence team, Alexander Wang, but he did land on his feet and is already part of a brand new startup.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 60

So logical intelligence introduces first energy based reasoning AI model and brings Yan Lun to leadership as a founding chair of their technical research board. So it's a six-month-old Silicon Valley startup unveiled an energy- based model called Kona. So their whole thing is certainty not probability. Of course large language models are probabilistic. And a lot of people are saying well that's a dead end because it's randomly coming up with answers which is not random but it is it is guessing in a way and so without grounding without checks it can hallucinate just like what we saw with the AI citations in those papers but here's the thing this is Sakana AI their Darvin girdle machine so the point is you ask it hey can you improve your own coding sort of scaffolding and it says okay and it you know has 16 attempts at doing that thing and some of them are wrong, right?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 61

So it says here's how you improve, you know, this code and that doesn't work. It doesn't improve the code. So you can say it's a hallucination. That makes sense. But here's what Sakana AI did.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 62

And by the way, Google DeepMind did the same thing with their evolutionary model, Alpha Evolve, right? So they test those answers to see if they work or not. they actually run them through a benchmark and if it improves it then it continues generating answers in that sort of lineage in that genealogy. That's why it's called kind of an evolutionary search. Right?

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 63

Similar to how biology evolves for the the best fit, this thing evolves for the best code. The things that don't improve die off. Those lineages, those genealologies die off. And the ones that perform better, they live, they continue, and they evolve. And that's what I think a lot of people miss when they say, "Oh, it's statistics.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 64

It's random. It's a stochcastic parrot or whatever." Yeah, there's that going on, but that's how this happens, right? Nature works the same way, right? That's how we were evolved with our thumbs and cerebral cortex and whatnot. All the things that died off because they lacked the ability to survive.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 65

I mean, yeah, you can say there were mistakes, but that's but that doesn't really make sense. The the system arrived at the correct solution or or some optimal solution, the the most fittest, etc. And it didn't do it on its first attempt. It didn't do it like with certainty like here's how you make the perfect life form done. No, it did it through probability and and over a long course of evolution.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 66

And so Yan Lakun for a while was saying that LLMs aren't going to get us all the way to AGI, that we needed something that was more verifiable, like more more certain, not based on guesses or probability. My take on this whole thing was, you know, I was very excited to see what his ideas were and where he was going to take it. I never understood his need to sort of try to take down LM to kind of like pull them down and say, "Oh, they're never going to work." I mean, right now LLMs are having their day in the sun. They're obviously useful for something. I mean, again, Claude Code at the end spits out very well working software that I use in my everyday life that makes my life better, that makes me more productive.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 67

Yeah, you can say, "Oh, it's guesses. It's random. It's probability." Like, okay, so what? At the end, does it produce something of value? Yes, it does.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 68

And a lot of people are betting a lot of money that it's going to continue to improve. That doesn't mean that what Jan is working on is wrong. He might find another approach to AGI that is absolutely brilliant and maybe it has its own limitations, but it also has its strengths. Maybe in some ways it's a lot better than LLMs. What I never understood is why tear down LLMs in order to present kind of his thing.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 69

So certainly I wish him kind of the best of luck. I hope that we do see some just incredible breakthrough from him and his new team that shows that there are other approaches. That would be incredibly exciting to add yet another thing, another pathway, potential pathway to AGI. That would be absolutely incredible. So, I I wish him the best of luck.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 70

But again, I think he's erasing a lot of goodwill and maybe making people doubt him a little bit when he says that LM can't possibly work. back in like 2023, he was on the Lex Freriedman podcast. This was right before GPT4 got released, like days or maybe a week before. And he said that large language models due to all the things that we talked about, they will never be able to predict how something moves in the physical world. So he said, let's say I I take my phone, I put it on this desk, and I take my finger and I push the phone.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 71

He's saying any child will be able to guess what happens to the phone, right? So it moves, right? it slides across the desk. But he's saying, but actually with LMs, they will never be able to understand that because nowhere is it written that that's how things move. Nobody sat down and wrote if you take your finger and you push a phone, then it sort of slides across the table.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 72

Now, obviously, we all know that is wrong. you know, the the these models starting with GBT4 were very good at understanding those relationships and how cause and effect works and stuff like that. So, obviously something got missed there because if you work with these models a lot, you you start realizing, okay, they're they're obviously compressing data and they're drawing some meaning. They're drawing some understanding of the world. So just because there's no book out there that describes what happens to an iPhone sitting on a table when you push it with a finger like it still figures it out based on everything that it's learned from reading all of the books all the internet etc.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 73

So again huge respect for Yan Lun. I tend to believe when he's talking about that there's other very good ways and he has I think full credibility there. The things that he says about LLM's limitations, I I tend to take with a grain of salt because we've seen him make certain predictions about how LM will scale that we know didn't come to pass. LM's just kept getting better and smarter and able to predict a lot of things and describe this world with with great accuracy. So, here he is saying, "Everyone's chasing the same solution.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 74

It won't work. It's time to start unlearning that AI equals LMS." One really interesting thing to me is that he always says that think about it like how can LMS predict the future? They're just thinking through words like how would they be able to predict their future state or or the consequences of their actions? And I never understood that. I'm like so why can't we do that with words?

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 75

Like do you think in words? Do you have a a narration going on, some internal dialogue? Most people do. Someone once asked on Twitter, "Hey Jan, do you have an internal monologue?" And Jan said no. And that's a thing.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 76

It's fairly common. So it's a common experience for 30 to 50% of the people. Instead of internal selft talk, some people without this experience might speak their thoughts aloud, rely more on external cues or find their minds simply blank when they're not focused on a task. A state surprising to those with constant inner chatter. So to me when I started learning about LLMs and the fact that they learned all the connections between different words and just based on that they were able to learn concepts and how to think through stuff.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 77

The fact that that reasoning was in words, it was all encoded in words that made sense to me because my thoughts are in words. So of course it can think. I think in words. So this thing should be able to think in words as well. But if you're one of the people that that's not how you process information, that's not how you think.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 78

It's more visual or some abstract. I don't even I can't even imagine what it is like. Like you don't have words as thoughts. I I don't even get it. But it's probably just as wild to you that their verse is true.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 79

And so when somebody says, "Oh, these things think through words." You're like, "Well, of course they don't. How could you? How could you think through words?" So, I'm curious if that is true of Yan and if that's maybe something that makes him so against the idea that LLMs could work. I don't know. Maybe that's a a silly idea, but I've always kind of wondered about that.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 80

And really fast, this is from sources that news Davos kicks off the AI turf war. So, somebody who was at the Davos meetup, we we saw some clips from there. You know, Demi and Daario Amadeo Winthropic, right? They're sitting there talking together feeling very kind of a buddy buddy saying, "Oh yeah, we we have a lot of close communications and and we work together." So they're saying like, "Yeah, we're like really in sync." And of course, now Google has a financial interest in Sakana AI, by the way. They also have a financial interest in anthropic.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 81

Anthropic is running on TPUs. So that's Google's hardware. during the day after AGI during that that talk they're kind of saying oh you know the AI labs that are being run by nonresearchers they're not going to make it or something along those lines that at least kind of like the point they were trying to make and that's kind of pointing their fingers at open AAI for sure OpenAI seems to be the target maybe other labs as well but this person that traveled to Davos he's saying meanwhile openai's rivals have told me they're particularly annoyed by Alman's aggressive attempts to shore up a capacity. And some are frustrated getting boxed out of deals by an unprofitable company that hasn't yet shown it has the revenue to pay for the eyepopping commitments it's making. So he's saying after his conversation with AI leaders this week in Davos, he came away with the impression that the industry has collectively decided to gang up on OpenAI.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 82

So is it possible that all of the different labs that are trying to edge OpenAI out? OpenAI certainly has been the first kind of AI company that triggered you know the Chad GPT moment that sort of started this race. They were definitely in the spotlight but then many many more started catching up and while OpenAI was ahead they did some things probably that made others mad or irritable. I mean it was a normal business tactics. There was always trying to steal the light from any Google presentation that Google was trying to do.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 83

And of course, Microsoft Sait Adela saying that they're going to try to make Google dance. And certainly Google did dance and now nobody wants to dancing anymore. At least if if you're the competitor, you would want it to stop dancing and go back to being kind of a sleeping giant. Instead, it's on a tear and rapidly, you know, either building or acquiring a lot of the stuff that it needs to become an AI powerhouse. Some of the things are 10 plus years in the future.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 84

things like project suncatcher, the idea of building these data centers in space for training AI models, etc. Like they're thinking well into the future and certainly it it does make sense that you know they do want all the users from OpenAI if they can get all those users using Gemini. That would be a a big big win for them. So curious to know what you think about all this. Do you think OpenAI is in trouble?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 85

Do you think there are people kind of trying to gang up on it a little bit? What do you think about Yan Lun? Is he on the right path? Do you think that LLMs will eventually be a a dead end? And what do you think about Google starting to hire, you know, for example, here the chief AGI economist?

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 86

What's your take on that? Does it seem like people are getting a little bit more serious about what's happening? Again, if he was calling that we're going to have minimal AGI by 2028, he was saying that since 2009, obviously his abilities to predict where things are going are pretty good. So, if now he's saying, "Hey, we got to start looking into this economy thing because it's gonna like implode." Like, do you just dismiss that or are you thankful that somebody with this expertise, with these resources, is seriously looking into it? Let me know in the comments.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 87

If you made it this far, thank you so much for watching. Name is Wes Roth. I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´01æœˆ25æ—¥

</div>
