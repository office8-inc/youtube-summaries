# ğŸ“º Local AI on a Laptop in 2026 (AMD Ryzen AI PRO 128GB)

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: Local AI on a Laptop in 2026 (AMD Ryzen AI PRO 128GB)
- **ãƒãƒ£ãƒ³ãƒãƒ«**: All About AI
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=UApd-gjQ6nM](https://www.youtube.com/watch?v=UApd-gjQ6nM)
- **å‹•ç”»ID**: UApd-gjQ6nM
- **å…¬é–‹æ—¥**: 2026å¹´01æœˆ21æ—¥ 01:00
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

Okay, so today I thought we could try something pretty cool. I have access to the AMD Ryzen AI Pro MPU here from AMD. And what I want to try today is run this with Lama. I want to try to run this with open code running some aentic workflows with maybe the GPT OSS 12B model. I want to try out the Quen image model 3VL I think it's called just to see how much we can actually do local.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

Let's say you are on a plane, you bring your laptop with the AMD Ryzen AI pro chip in it and let's just see what you can do with that. So yeah, let's just try it out. See what we can do, what kind of tokens per second we can get with different models and yeah, do some local AI. So yeah, like I said, we are going to run this on Ulama and yeah, if you haven't tried, it's a super easy way to get into local models. And if you go to models here, you can just download this, install it for Mac, Windows or Linux.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

Today we are on Windows and you if you go to models here we have all kind of the latest models we can just pull right and this will work perfect and I'm going to focus I think on uh I think I'm going to focus on the GPT OSS 20B quentry as I said VL I also want to try out quentry coder to see what kind of performance we can get on that and one thing I'm a big fan of is kind of the open source of claw code open code super good tool to be honest. If you haven't tried it, definitely go check it out. And I'm going to set up like open code to run on Wama so we can try some agentic workflows uh with yeah running this offline or like local I guess. So you can do some agentic workflow if you're on a plane, you have your AMD laptop with you and you can do stuff like this. So yeah, that is basically uh what we are starting with.

### ğŸ“ è©³ç´°èª¬æ˜

I have installed lama. I have downloaded set up open code and yeah let's just explore a bit. Let's see what we can get here. So the first thing I wanted to do was just to go to the terminal lama list just see what kind of models I have. So I have the quen 3 code there.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

I have the quen 3 v8b image model and I have uh oss 20b. So what we can do in that we can run some test with a verbose flag to actually see what kind of speeds we get. So what we can do is uh ola run and let's do gptoss 20B and we can do the d-verbose right so that is the flag and when this launches now we can actually see at the end what kind of speed we get okay so uh I just want to say a quick bit about uh this what we are running on here so you can get some comparison [snorts] uh if I go to the system and I go to about you can see This is the setup we have now. Uh yeah, I guess you can just zoom in. We are on the AMD Ryzen AI Max Pro uh 395 and we have the 128 GB of RAM and yeah, you can kind of see here.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

So you can put it to your laptop if you want to do that something like this. Okay, so you can see we are now in this and let me just do a quick test here. Write a short story about the history of RAM. Okay, so you can see this is looking pretty good. You can see here are the thinking tokens from the GPT OSS model and basically at the end now we will kind of get like a small uh overview of what speed we ended up with using a lot of thinking tokens but with what is nice here doesn't really matter too much now you can kind of see the output and this is pretty good speed if you ask me just to running on a local laptop here this is I have nothing against this speed for me this is fast enough I can't really keep up with reading Anyway, we can see the world kept accelerating.

### ğŸ¯ å¿œç”¨ä¾‹

DDR2, 3, four, and five. And the prices now for the DDR5 is crazy high, right? So, that is also interesting. So, this was a really long story here from GPT OSS 20. And we end up here.

### ğŸ’­ è€ƒå¯Ÿ

I think this means that we got about 40 tokens per second. And that is pretty good. That I'm super happy with that. So, running at 40 tokens per second is far more than I can read. And for coding also as you will see later that is fine.

### ğŸ“Œ ã¾ã¨ã‚

So I'm happy with that. So if we go to the Quen model now you can see this will probably fall a bit because we are on a 30B. Remember this was um a 20B model right? So let's just try that. So for that we go run and I think it's three coder 30B is it?

### âœ… çµè«–

I think so. If not I'm going to fix it. And let's do the verbose flag again. And this time, let's just try some Python code or something, right? A simple snake game.

### ğŸ“š è¿½åŠ æƒ…å ±

Okay. So, yeah, you can see even though this is 30B, it's still pretty fast. I would say this almost looks faster if you ask me. At least it's not slower. And this is a pretty good coding model to be honest.

### ğŸ”– è£œè¶³

30B and running this on like a local laptop. Let's say you're in a plane, you want to do something with your coding. And yeah, this is just a superb way to do it on like a local laptop. So let's see what we end up here now. We ended up on 51 tokens per second.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

So even though this was a 30B model, this was faster, right? So that's pretty interesting. And 50 tokens per second for doing coding, that's good enough for me. I have no issues with that as you will see soon in my small testing. So yeah, that is an introduction to Lama in the terminal.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

But we also have the llama app that is a bit more nice if you want like a an interface. You can see we are on the GPT OSS30B here. Do I have the quen coder? Yeah, I have the quen coder too. And I can say write a simple uh Python code or something like that.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

And you can see pretty fast all local on my laptop on my MPU. So yeah, no issues with that. So you can also use the amama yeah kind of in uh yeah interface here it looks a bit better but now uh I want to do one more thing with images and then I want to go to open coder and see what we actually can do on a genic tasks. So the quentry VL is my favorite uh image model at the moment and what is pretty cool about this at there are different sizes. So I have been actually using the 8B model on the quen 3 VL power uh vision model and it's so good for being just 8 billion parameters and this is what I've been just running on like my desktop too with the uh and it has no issues doing like simple OCR.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

We can do images and you can kind of see the capabilities here and I think this is great enough to do some simple task with this. I built some small app that are running this model. So let's just see how this performs now and I can kind of show you how this works. So remember we could do this in the uh the desktop here. We can try both.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

So let me first just show you how this work in the terminal. Right? If you remember list we have the quen 3VL. So we can just run that uh run and let's do uh yeah Quen 3VL8B. Okay.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

So let's just run that. And I took a screenshot of kind of hacker news. This is kind of the first uh 15 headlines or something. So you can see I took a screenshot of that. And what we can do now uh when this is running, yes, I can kind of just po point to that path.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

So I just paste in the path here. This is the hack.png. And then I can just ask a question. So I can do something. What are the first or the top three headlines?

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

Something like that. Let's do that. And let's see now if this small 8B uh quent 3VL vision model can just look at the image, find the top three uh headlines and bring it back here. So remember this is offline. We are not doing we're just working uh locally now on the the AMD uh chip here, right?

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

So you can see. Okay. So look at this. We have 50 hallucinated citations. Google Titan.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

Goodbye Microsoft. Everything looks okay. Let's double check. Yeah. Goodbye Microsoft.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

Google Titans. 50 hallucinated citations. Yes, you can see how nice this is. And we can also just double check if we go to the lama here. Uh let's just do a new session.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

Something like this. Uh, let's select the Quinn model here. And then I can just upload the image, right? Because now we have a more of like a I can just upload this, right? I can zoom in a bit here.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

And [snorts] let's ask the same questions. What are the top three headlines? And of course this uh here if you are taking this laptop uh with the AMD chip offline or on a plane and this will of course work offline and you can see now we get it in a more like nice formatting here. It only took 4 seconds. So that's pretty fast because now this time the model was already loaded into memory and it's even quicker this time.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

So just a superb vision model if you want to try out running something local on your laptop like I am doing now on my Windows machine. And yeah, just a very good model. Uh, but now let's try to do some aentic coding and build like a simple maybe like an HTML website with some images and stuff. So let's get into some aentic coding. And you remember open code.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

I can just type that in my terminal and I get to kind of this yeah open- source local version of cloud code that I like to call it. And you can see this has this here set up as kind of my local provider here. Uh I can do slashmodels, right? And if I do here, you can see here are the two models I have installed. I'm not using the the wish model here.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

But let's select the GPT20B model. And you can see I can do hello. And when this has loaded into memory now, it should work a bit better. Uh but you can see we are still kind of loading this into memory and we will get a response. So remember uh running this has some uh additional context when we are running this.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

But you can see it is working 100% locally. We are running open code. Uh this is not kind of my favorite way of running open code. So I'm going to show you what I like to use this when I'm running this locally on uh this AMD laptop here. Uh so if I just uh I'm just going to leave this.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

Okay. And I'm going to close this and I'm going to head over to cursor. So this is where I like to use um open code because then I can kind of see the files here. And again we can just do open code here and we will launch it again. So you can see this is basically the same as we had before.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Uh again I want to switch models. I want to just go to the 20B or let's try yeah let's do the 20B. I kind of like that for open code. So let's try to create a file here. Now let's say we wanted to do a a simple HTML file HTML web page or something.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

So we can start with uh create a simple and uh white HTML page uh white text uh black background uh for readability or something uh ability. Okay, that's fine. Let's just try that. So what is happening here now in open code as this is more like an agentic uh it has tools. So it has read tool, it had write tool, it has tools to generate files.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

So if you look at the right here now uh this is basically the same as cloud code as I said before we can generate files. But what is cool now is this. This is not connected to the cloud. Everything is local. The tool calls are all performed locally.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

So now you can kind of see more. You can see we are writing this and we got the text file here. Black and white HTML. Perfect. So let's check it out.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

Okay. Simple but easy, right? Uh, but I'm not quite happy. I want to work more on this. Uh, I want to center the text and just write more text and do some headlines and stuff like that.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

I can just go back here and I could say something like centered. Uh, that's that's fine for now. Okay. So, I'm just going to do that. So, let's see the changes now.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

So, let's refresh. Okay. Much better. Welcome to the reading demo. Yeah, I like this.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

You can see we have a bit more spacing. We have like a quote here in this perfect easy way to read. And yeah, that was it. How easy was that? But now, let's change it up a bit.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

So, I'm going to do a new session. Uh, I'm going to switch the model. So, I'm going to do the Quen Coder model this time. Uh let's do uh let's see if we can do a Python um Python snake game here in Python. So um write a snake game in uh Python.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

So let's try it out. Uh Python snake game.py. Okay. All right. This is working right.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

It's a bit fast maybe. Let's see if we can get any scores here. If I can do this. Okay. Yeah.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

Score is working. Is it growing? Yes. All right. We got a snake game.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

So, but I wouldn't say open coder is is it I don't think this is the best use case of open coder because it does takes a lot of context and this will slow down your local models. So, for me, I just want to show you the preferred method is if I was going to build a snake game, I would do something like this. So what I would do is I would just do the Quen 3 coder 30B here in maybe the llama interface and I would say uh write uh snake game uh tkinter something like this and you can see how much faster this is. So now we can just copy this, right? And I would say if you're going to use open coder, maybe you just have some small things you need to do.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

But uh this is kind of my preferred way of running local models because then you don't have to deal with all this context that is tool calling the descriptions and everything. But it does work though if you really want to go for it. And let's try to run this python snake 2.py. And yeah, you can see this is a bit better too. is not so fast.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

So yeah, that is basically I would say running these models on your laptop. Let's say you have an AMD machine like I have there today. It works great and you don't really have to think about any security. You can try out a bunch of different models like we did today. We have the image model integrated.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

We have the Quen 3 for coding. We have the GPT OSS 20B for conversation and more general thing. And this setup of having the image model, the coder model, and uh yeah, the conversational model is basically all I need on like a plane trip. And if I kind of just bring the laptop here, right, this works great. I I I'm entertained during the whole flight.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

And another great thing is that when I run this locally, right, on my right AI pro machine, the security is kind of flawless, right? Because everything is processed right on this device. My data never leaves the machine. It doesn't go to the cloud. doesn't grow abroad.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

But most importantly, everything I do on my Ryzen AI laptop now stays here, right? I'm not training anyone else's model based on the data or the inputs outputs I used. So, it stays strictly private. So, I can work with like sensitive code or proprietary information without uh yeah feeling like I'm fetting this back to the bigger companies and training on my kind of yeah secure data. Right.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

If you want to see how these local AI workflows can benefit your business, uh you don't have to take my word for it. AMD now offers like a free loaner program uh where you can get these Ryzen AI pro machines uh to test them for yourself. So I will put a link in the description below to this loan offer. So definitely go check it out and see what these local machines can do for you and you can use AI locally, right? So yeah, thank you for tuning in today and like I said, check this link in the description and have a great

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´01æœˆ22æ—¥

</div>
