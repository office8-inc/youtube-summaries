# ğŸ“º LTX-2 UNLEASHED AI VIDEO

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: LTX-2 UNLEASHED AI VIDEO
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=hkfSfr-hMWs](https://www.youtube.com/watch?v=hkfSfr-hMWs)
- **å‹•ç”»ID**: hkfSfr-hMWs
- **å…¬é–‹æ—¥**: 2026å¹´02æœˆ01æ—¥ 05:00
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

This might be the most important release we've seen in AI video this whole year and the last year as well. It's not a wrapper. It's not a gated demo. It's a fully open audio video generation model released with open weights and crucially full training code. It's Litri's flagship open-source video foundation model and it's built for real workflows.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

This is the LTX2. With this release, they're making the entire model family public, full and distilled weights, the entire training framework, benchmarks, and [music] luras. And this is optimized for Nvidia's RTX GPUs. This makes making highquality local video generation practical on [music] at home consumer hardware. It also has audio natively built in.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

>> Don't start glitching now. I said don't start glitching now. You have a lot of nerve coming back here. >> You should have never ventured into the Underdark. >> I crave the forbidden lamp, brother.

### ğŸ“ è©³ç´°èª¬æ˜

>> You are not prepared. [screaming] >> They always pay their debts. and so do I. >> It can do up to 4K in resolution and it's completely free. So, in this video, I'm going to show you what Open actually looks like.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

We're going to run this locally, look at the difference between the full and the distilled models, and I'll show you how to use their multimodal pipelines. So, let's jump in. First, let me give you the specs because this is different from what we usually see. Usually, open source means here are the weights. Good luck.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

LTX2 is the actual system. You get the model weights for local inference, but you also get the training framework and recipes. This is a modular pipeline for continued pre-training and domain adaptation. It adapts, evolves as you go. And that means if you're a developer or studio, you can adapt this for your own needs, for your own workflows.

### ğŸ¯ å¿œç”¨ä¾‹

And yes, it supports full multimodal pipelines. We have text to video, image to video, video to video, and even audio conditioned generation. Thank you to Lightrix for sponsoring this video. I was extremely excited to help this project in whatever way I can. So, before we look at the benchmarks just out of full transparency, I want to show you what I'm running on kind of what kind of hardware I have set up here.

### ğŸ’­ è€ƒå¯Ÿ

I'm running LTX2. I'm testing it out on my main workstation. So, the GPU is an Nvidia GeForce RTX 490. So, it has 24 GB of VRAM. The CPU is an Intel Core i9 64 gigs of RAM.

### ğŸ“Œ ã¾ã¨ã‚

And because LTX2 is optimized for Nvidia RTX GPUs, this 4090 that I have on my computer, well, it it handles this setup easily. with 24 gigs of VRAM and 64 gigs of system RAM, I'm able to load the the full model weights directly into its memory with with no issues. But here's the cool part about this release. Even though I'm running this on a 4090, Litrix released distilled and quantized variants of their model. This is specifically to reduce the memory requirements.

### âœ… çµè«–

So, you don't need my exact setup to be running this locally, but if you do have the hardware, then the performance is extraordinary. All right, so enough talk. Let's get this running on your machine. LTX2 integrates directly into Comfy UI with reference workflows available right at launch. You can download the weights directly.

### ğŸ“š è¿½åŠ æƒ…å ±

I've linked the repo down in the description, and you have a few options here regarding the model variants. They provided both the full and the distilled model weights. The full model is best for maximum quality and also for fine-tuning, but you also have the option of using the distilled or quantized variants. And these are designed to reduce the memory and the compute demands if you're running this on a local standard workstation. These distilled variants make local video generation fast and practical.

### ğŸ”– è£œè¶³

I'm running this locally on my GPU right now and the iteration speed is is very practical. It generates this video at a very rapid pace. But if you haven't used Comfy UI before, it's basically a go-to tool for running AI locally on your computer. Now, you might be looking at this and wondering why does this look like a plate of spaghetti? That's a great question.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

So, Comfy UI is a nodebased graphical interface and it allows you to run generative models locally. Unlike a simple website where you just type in the prompt and hit go, CompuI visualizes the entire pipeline. You can see exactly how your data flows from the model loader to the text encoder through the sampler and finally to the video decoder. If you want to use LTX2 in Comfy UI. First things first, make sure you have Comfy UI installed.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

If you don't have it installed, I do have a video that has a full walkthrough about how to install it on your computer. I'll link that video down below. So, install it if you don't have it. It's free. It's open source.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

Now, if you already have it installed, just make sure it's at the latest version. So, update it if you need to. And once that's set, let's begin. All right. So, now we have Comfy UI up and running.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

I have the LTX2 text to video template up. Now, if you go over here to templates, let's search for LTX-2. All right. So, when you open up this folder, the templates folder, you're going to see two distinct kind of paths. You have your kind of regular, this is the full model, you have the distilled.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

So, it's crucial to understand the difference so you don't waste hours rendering or or crash your PC. And this is really the raw power of LTX2. So in this workflow, you're loading the foundation model weights, the full weights. This is the full workflow with maximum control with maximum quality. When do you want to use this?

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

Well, this is when you need the absolute highest fidelity and you're willing to wait a little bit longer for the generation and you have to have the required VRAM and RAM in order to be able to run it. And then you have the distilled workflow. This is for speed and efficiency. If you want to just generate fast, this is the one to use. Or if your machine can't handle the full mode, then just use the distilled version.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

So that means that the distilled checkpoints, they basically have optimization already baked in. Since I'm running on an RTX 4090, I can use this without breaking a sweat. But for the majority of my testing, for 90% of my testing, I'll stick to the distilled template to the distilled workflow. This we can rapidly iterate, try a bunch of different stuff, test out different prompts, and when I have that prompt dialed in, I I bump it up to LTX the full mode. So, here we're using LTX2 distilled text to video.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

We're going to say, "A man in a black tuxedo stands in a red tiled bathroom." He says, "Notice the difference. The camera is dramatically zooming in on his face. The red tiles are very reflective and have some light scratches and imperfections." So, this is that clip. Man in the Red Bathroom distilled. >> Notice the difference.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

>> And then here's the exact same prompt. We're running it in the text to video LTX 2.0. So this is the full version. Here's what that looks like. >> Notice the difference.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

>> And to give you an idea of how long it takes. So, the distilled version took 53 seconds and the full version took 2 minutes and 27 seconds. So, hopefully that gives you an idea about the trade-off in terms of quality, how long it takes. And if you don't have a a beefy machine to run the full full model, then I mean, as you can see, the distilled versions, they're not bad. Not by any stretch of the imagination.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

And by the way, so here I'm using a frame count of 121 1280 by 720 resolution. So what that means is at 24 frames per second, that gives us a 5second clip. So really fast, let me kind of walk you through the UI cuz first time using it can be a little bit daunting. So always be aware that you can add new kind of tabs right here and then click template and add whatever template you want. So you can have multiple tabs with different templates all running at the same time.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

You be switching back and forth. So for example here I can have a distilled version running for quick testing. In this window I can be running the actual full version. So iterating in the first and producing ready to go footage in the second. Now there's some options here.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

We'll kind of talk about them in a second. And by default everything's kind of collapsed. There's this button right here that's going to expand the entire thing. We can talk about that in a second as well, but that you don't even need to click that, but that you don't need to go into it because that's when it gets rather complicated. So, just know that if this is day one of you using it, you need to understand everything in there from the get- go.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

But definitely as you start kind of getting familiarized with it, maybe pop that open and take a look inside. The prompts go in here, right? So we type in whatever we want in the prompt box and we have some options here for width height frame count 1 and 21 frame count times 24 frames per second the speed at which it's running. So that means it's an output of 5 seconds. It's 5 seconds long.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

If you need to change the resolution it gives you kind of some helpful hints here. You can try 1920x 1088 if you have a powerful GPU. Notice that the frames, the numbers, they have to follow a certain format. But once you have all that in here, you click run. Once you click run, this will light up green if everything's set up correctly.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

And actually, if you go up here to job history, you can kind of see what's happening. So, this is the current job that's running. Also, you have your assets folder. That's if you click that, it comes up here on the leftmost side, and it gives you all the stuff that you've generated. All right, let's briefly click on this thing here.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

This shows the subgraph. So let me close this. Let me close this. So this is what the entire workflow looks like. Starts on the left.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

It's all the inputs. You have the width, height, frame count, noise, seed, text, checkpoint name. Width and height. Notice they go into the video settings. Frame count goes into length.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

Then the seed text checkpoint name all go into the model or the prompt. Here we also have our camera loris here. Crl +B to enable. So those are kind of like the custom loras. We'll talk about them later, but this is kind of where they are.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

You can click Ctrl +B to enable this node. Notice we have sampler stage one, sampler stage two, and we also have our upscale sampling. So how this produces a video, it actually creates video at half resolution and then it upscales it. So it's 2x upscales it. Just in case you were wondering why it's kind of laid out like this.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

And as you get more advanced with this, you can change and add a lot of these settings. A lot of this is it's able to move around. So you can kind of position it how you want to. It's a very good visual kind of interface for it. So it can get as advanced as you want.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

But we'll go back to our regular view. And this is the output of the prompt that we gave it. So again, if you look here, so it'll tell you that took 2 minutes and 43 seconds to produce this prompt. And here basically we're doing a cheerful girl puppet that's singing in the rain. [singing] >> All right.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

So before we run this, we actually need to understand how LTX 2 constructs the video. It doesn't just render the entire high-res file in one shot. The workflow runs in two distinct stages. First, it generates a lower resolution base video. Then it passes that data to the upscaler.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 36

So it passes the data from here to the second stage which is the spatial upscaler which refineses the details and blows it up to your full resolution during that second pass. All right, let's configure the parameters here. Set your resolution to 1280 by 720. For the frame count, I usually set it to 121. And at 24 frames per second, that comes out to a 5-second clip.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 37

Next stage is the prompt. LTX2 is natively multimodal and responds to just natural language. So, you don't need to get too crazy or technical. There's no tags. Just describe what you want.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 38

You can describe the scene, lighting, characters, and the dialogue if you want characters to have a specific dialogue. And when you're done, just hit run. And you're going to see it run through that initial generation and then get passed to the upscaler for the second pass. Then it's going to deliver the final result. Here's a few demo clips that I was able to generate.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 39

Check it out. [music] All right, next let's talk about Loras. Loras or low rank adaptations are basically lightweight modular fine-tunes that sit on top of the big model of the actual model. So if you want the model to have some specific skill, some specific mastery, instead of retraining the entire massive model from scratch, we plug in these little adapters and that force the model to learn a specific concept or a specific skill. This can be a character or a specific style or a certain type of camera movement.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 40

For this release, Lightrix provided a number of accompanying Loras and Icy Loras for LTX2. They're specifically designed to control style, structure, motion, and camera behavior. So, here, for example, we have some of the dolly movements. Dolly out, dolly left, dolly right. If you click on one of them, you're able to go to files and download the file it ends in.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 41

Those are the files that end insafe tensor. So, just find this one and click download. After that, find your Comfy UI folder, then slashmodels/ Lauras, and just add that file in here. I already have the dolly left IC lores, so we're going to be using that one. If you added a new one, you can check to see if Comfy UI is recognizing it by going to models, then click on the Loras folder, and you'll see the lures in here.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 42

You can click refresh in case you've added anything else, and it'll get refreshed and added in here. All right, if we close this, the camera lowers are up here. By default, they will be bypassed. You can do Ctrl +B to enable or you can select both them because sometimes CtrlB doesn't work. You can rightclick and click bypass and this will enable them.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 43

Next, we can choose the loris here. So, we're using the camera control dolly left. And same thing here, camera control dolly left. Now it's important here to understand that since this is in two stages, two passes, the initial stage plus the upscaling stage, this is the most crucial portion of the LTX 2 workflow using these Loras. You have to apply it to to both.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 44

If you only apply to the first pass, the upscaler is going to hallucinate something that might not have anything to do with your intended camera style, camera movement, etc. Once the nodes are connected, you just need to trigger that Laura within your prompt. So, if we go back to our prompt here, we're going to explicitly say dolly left shot somewhere in the prompt to make sure it gets triggered. So, here I'll say dolly left shot of man running away from lion saying oh no. Oh no.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 45

Oh no. And I'll click run. I'm also going to try it with a higher resolution and also doubling of the time. By the way, coming back here to our full view here in the Laura loader, you can actually change the strength of the effect. Lowering the strength allows the base model to have more creative freedom while raising it to 1.0 for example kind of forces it to strictly follow that Laura.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 46

So if you wanted to adhere strictly to what you provided go high like one. If you want to give it a little bit more creative freedom go8 or you can even test lower strength if you want. Each of these loras do have a kind of recommended prompting and some guidelines you can follow to make these work better. For example, for the dolly left Laura, they have this recommended prompting to get the best results of motion capture. Loras describe the destination of the movement and the hidden parts of the scene so the model knows what to reveal as the frame shifts.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 47

For example, the far left side of the room, objects or people positioned off frame to the left, background structures that become visible due to parallax. This gives the model a visual map of what exists as the camera slides left. Here are some example shots with that particular Laura. Oh no. [sighs] [music] Next, let's look at probably one of my favorites, which is the image to video.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 48

So, let's open up a new tab, template, search for LTX2, and we're going to do this image to video. This is going to look very similar to the previous ones we used to text to video except for one thing. We have this load image. Upload a file. Add your prompt and you're ready to go.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 49

So, think of the image as the starting frame. Even though you're providing an image, it's a starting frame. You still need to provide a prompt to tell the model how to animate it. For example, describe the character talking or the camera moving. The model uses the image as a structural anchor and the text as the motion guide.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 50

For the settings, I'll stick to 1280 by 720 and a frame count of 121, which again gives us that 5second output. Pop quiz hot shot. What is this painting called and who is it by? I'm going to upload it here. Now, this is in the public domain because it's been a while since it was painted, so I can use it without any copyright issues.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 51

And in the description, I'm going to say, "The man makes a sorrowful scream as he walks towards the camera." And I did several other variations, so kind of a test them out and see which ones are better. In this one, he says, "Why does no one know who painted me?" And then shakes his head. I always thought it was a painting by Van Go. Some of the people I asked thought it was a Salidor Dali. But no, this is called The Scream, and it's an art composition created by Norwegian artist Edward Munch.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 52

I was today years old when I learned that. All right, let's see how it came out. So, to wrap this up, I just want to emphasize why this release matters. We see a lot of open models that are just deadends. They're not really open- source in the way that it was intended.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 53

LTX2 is different because Litrix released the training code, the benchmarks, and the full weights. It's not just a tool that you use. Whether you're a developer building a new pipeline or a studio that needs to keep your IP private on your local network or just a creator that wants to mess around with Lauras, this is yours to adapt. But don't just take my word for it. The best part about open source is that you can verify it yourself.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 54

I've put the link to the GitHub repo and the model weights right at the top of the description. Go clone the repo, download the distilled weights or the entire model if you wish and run it on your own GPU. And I do want to see what you create with this. Tag me on X/ Twitter. I am Wesroth.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 55

Huge thanks to Lightrix for sponsoring this video and honestly for releasing a model that is actually open instead of being just a rapper. If you found this guide helpful, please hit the like button, subscribe for more local AI tutorials, and I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´02æœˆ06æ—¥

</div>
