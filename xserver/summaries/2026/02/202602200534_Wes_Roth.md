# ğŸ“º GEMINI 3.1 PRO is the new era...

## ğŸ“‹ å‹•ç”»æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: GEMINI 3.1 PRO is the new era...
- **ãƒãƒ£ãƒ³ãƒãƒ«**: Wes Roth
- **å‹•ç”»URL**: [https://www.youtube.com/watch?v=nt3P2dGJfNg](https://www.youtube.com/watch?v=nt3P2dGJfNg)
- **å‹•ç”»ID**: nt3P2dGJfNg
- **å…¬é–‹æ—¥**: 2026å¹´02æœˆ20æ—¥ 05:34
- **å†ç”Ÿå›æ•°**: 0 å›
- **é«˜è©•ä¾¡æ•°**: 0

## ğŸ’¡ æ¦‚è¦

ã“ã®è¨˜äº‹ã¯ã€YouTubeå‹•ç”»ã®æ—¥æœ¬èªå­—å¹•ï¼ˆè‡ªå‹•ç¿»è¨³å«ã‚€ï¼‰ã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã§ã™ã€‚

## â­ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

> ğŸ“Œ ã“ã®å‹•ç”»ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãƒã‚¤ãƒ³ãƒˆãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™

## ğŸ“– è©³ç´°å†…å®¹

### ğŸ¬ å°å…¥

So, Google just released Gemini 3.1 Pro. It's been out for a few hours. I'm still testing it out, but at first glance and looking over the benchmarks, the numbers are significant. This is the core reasoning model that is powering the entire Gemini ecosystem, and it just got significantly smarter. Just to give you kind of an idea of what sort of a leap this is.

### ğŸ“‹ èƒŒæ™¯ãƒ»æ¦‚è¦

The previous pro model which was Gemini 3 Pro got a 31.1% on the Arc AGI 2. A RAGI 2 is supposed to be a measure of abstract reasoning. So it went from 31% to 77%. That's a massive improvement in just 3 months. So we went from 31% to 77% in 3 months from Gemini 3 Pro to Gemini 3.1 Pro.

### â­ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ

One interesting thing that we're seeing shifting right now with a lot of these models, not the Gemini series, is we're seeing a big transition into this Aentic era, if you will. Half of the benchmarks that we'll look at for this release, these benchmarks, they didn't exist a year ago. The labs and the users of these AI models were not as concerned about how well they answer questions anymore. We're more concerned about can they do real work and real life scenarios. Can they do things autonomously?

### ğŸ“ è©³ç´°èª¬æ˜

So Google is really kind of framing a lot of this from that perspective. How well are these models able to complete tasks? A lot of the benchmarks are testing their agentic capabilities and a lot of these benchmarks they're trying to test kind of the complimentary skills to that. things like web research, long horizon professional work, the ability to use a a terminal or some sort of a command line interface effectively to to execute commands on the computer to to operate the system as well as interacting with humans. Like if you're a customer service agent, you need to be able to do kind of all those things and also keep up a conversation with the customer.

### ğŸ’¡ å®Ÿä¾‹ãƒ»ãƒ‡ãƒ¢

And all these benchmarks have hard tasks. They're they're realistic tasks and they're verifiable. One of these is browse comp. This was released by OpenAI in April 2025. So again, not even a year old at this point.

### ğŸ”§ æŠ€è¡“çš„è©³ç´°

And this tests the agent abilities to go out there on the worldwide web and to try to find these kind of entangled facts. They have to be able to persistently navigate the internet, you know, go through lots of data and then pick out these little truffles of of information that is that is hard to find. So the point here is the answer is very short. It's unambiguous. It's easy to verify, but it's extremely hard to discover.

### ğŸ¯ å¿œç”¨ä¾‹

To give you an idea, humans solve around 29% of these, often giving up after a few hours of searching. So, us humans, we we kind of suck at this test. Let me give you one example question here. So it's please identify the fictional character who occasionally breaks the fourth wall with the audience has a backstory involving help from selfless aesthetics is known for his humor and had a TV show that aired between the 1960s and 1980s with fewer than 50 episodes. So as you can imagine you can't just Google the answer there, right?

### ğŸ’­ è€ƒå¯Ÿ

So the answer is plastic mad. So that's browser comp. Before this year the leader was GPT 5.2. This was an open AI benchmark that they've released. So that was the best one for a while.

### ğŸ“Œ ã¾ã¨ã‚

Then came Opus 4.6. Earlier this year, a few weeks ago, it clocked in at 84 and now Gemini 3.1, taste the lead with 85.9. So, it's currently the best at that agentic research, untangling facts, etc. Another brand new benchmark released this year, January 2026, is called Apex Agents. If we had a benchmark to sort of rank how cool the names of these benchmarks were, feel like Apex Agents would be near the top of that benchmark.

### âœ… çµè«–

And Apex agents is kind of this productivity benchmark/index for agents. And how it's conducted is these agents get a full office environment so to speak. So think of it like a drop-in remote worker, right? So they get docs, spreadsheets, emails, slack-like messaging, and they need to be able to produce client ready output. And the tasks that these agents are given can also, I think, be used to torture people by forcing them to do this work.

### ğŸ“š è¿½åŠ æƒ…å ±

I know some people do it. This is to me just the most god- aful thing in existence. So imagine you're just on your desk and gets dumped a bunch of spreadsheets and memos and just tons and tons of data that you have to comb through. An example prompt would be analyze category consumption patterns and market penetration using the category penetration score methodology for Pure Life's portfolio strategy. Present the cumulative penetration score for each category under the weighted methodology using the following components.

### ğŸ”– è£œè¶³

I'm not going to go further. The thing is, this is real work that people are doing right now. There's probably hundreds of thousands of people worldwide doing this, maybe millions, and none of them are whistling on their way to work because this is probably the most frustrating and tedious type of work that you can do. Each task takes hours of human labor and concentration, and it's extremely tedious. So, how well do these agents do on these tasks?

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 13

Gemini 3 Pro, kind of the previous version, had 18.4 4. And with the release of Gemini 3.1 and Opus 4.6, both of them clock in at 33.5. So what does that number mean? How does that compare to humans? Well, 100% would mean that these things are 100% correct, client ready deliverables.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 14

If it was a human being doing it, it would take them one to two hours. So somebody in that professional setting, a professional in that environment, whether it's consulting or investment banking, corporate law, etc., would take one to two hours to complete that present the final sort of project completion client deliverable. And so if the models get to 100% that means that white collar work of that nature is a lot of it is kind of automatable. We're currently sitting at 33.5. So there's still a ways to go but it's climbing fast.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15

It nearly doubled again in the last 90 days right on this particular benchmark which is again brand brand new. By the way that number is an average. So for example for management consulting that category sits at 41%. So if you're a professional working at a company like Deote something like this lands on your desk that's about 1 to two hours of actual work and you're expected to complete it 100% of the time 100% accurately with the best tested AI model which is Gemini 3.1 Pro. You know it's one out of three.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 16

So not quite there but improving rapidly. We're also seeing Terminal Bench, another brand new benchmark. It's released in November 2025, created in part with the Stanford Institute. So measuring these agents abilities to operate a terminal is very important. And the more I work with these agents, it's kind of obvious why.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 17

Us humans generally use kind of like the visual operating system. But for these agents, they're much much better using just a command line interface. Whatever commands they need to execute, they just type them in. They are large language models after all. Given terminal access, they can do pretty amazing things.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 18

And at the same time, they kind of struggle working through, you know, like the visual stuff. They don't quite operate computers as as well as a human can using a keyboard and mouse and kind of that visual interface, but you get them on a terminal and they start cooking. So, terminal bench sets them up on these Docker sandboxes. Basically, kind of these like little environment with pad walls so they don't they don't cause too much damage. and they're asked to do tons of these highly technical tasks, configuring web servers, various data processing, tons of stuff that most people would find very boring.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 19

One thing that I thought was very cool on there is uh they're asked to train machine learning models. This channel, we we've done that a few times. I don't remember which model was the first one that was able to do it, but I asked it to create a PyTorch reinforcement learning training environment where it created a game. I think it was a snake game or something like that. And then it sort of created an actual neural net to train that little snake agent to play the snake a game.

### ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 20

It was kind of interesting because you had AI sort of giving a birth to AI and training it. I mean doing reinforcement learning to teach it how to do a particular task. Something about that kind of interesting. So that's that's terminal bench. And again using these agents every day like this seems like a a very important one.

### ğŸ’¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 21

And the one that we're talking about this is terminal bench 2.0. That was the one released on November 2025. kind of the updated version. So again, prior to the last few weeks when a lot of the brand new models came out, GPT 5.2 was the reigning champion at 64.7. Then of course, we have a bunch of new releases.

### ğŸ”§ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 22

Opus 4.6 lands at 65.4. So a modest gain on a GPT 5.2. And now Gemini 3.1 Pro comes out and lands at 68.5. And that is a pretty massive leap from the previous Gemini model, Gemini 3 Pro, which was at 56.2. So again, it's important to keep in mind that we're talking about a period of time of months, of weeks in some of these cases.

### ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 23

So these are pretty notable improvements in a a very short amount of time. But once again, Gemini 3.1 Pro is the leader. It's the leader in that category, the use of command line interfaces as tested by terminal bench 2.0. And finally, we have the tow bench. Last one, I promise.

### ğŸ’­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 24

But I think it's important to understand what these are, what they're testing, and the rapid progress that we're making in just the last few months because again, a lot of the questionans answering benchmarks, we've saturated them, and they're just not as interesting. These are the brand new frontier, the brand new Agentic benchmarks, and I don't think you can really understand AI progress right now, where AI is going without understanding these benchmarks and what they represent. So the next one is Tao 2. TOAO kind of looks like a T lowercase T. So, this is the TOAO 2 bench and it's a conversational agent benchmark with dual control environments.

### ğŸ“Œ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 25

Basically, it's how well does it play nice with a partner. So, if you ever saw that movie Top Gun with the two jet pilots kind of working together, you can kind of understand how the actions of one have to reflect what on what the other one is doing. So like in this test they say you know one of the agents can change a shared dynamic world state basically means respon it basically means responding to the world and a simulated partner kind of in real time and properly. So if one jet pilots shoots missiles at a building and blows it up, you shouldn't also shoot missiles there cuz he he did it already. So you can do something else.

### âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 26

Or if you're flying together and he goes split and then goes right, what are you supposed to do? You probably go left. So you have to kind of respond to the world and the actions of your partner as well as the communications of your partner. So kind of like the words, the actions and also how the world changes. But on this benchmark, instead of being two jet pilots, it's usually customer service scenarios.

### ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 27

So, for example, the AI agent is a telecom tech support operator and you have a 64year-old retired librarian calling in and the agent has to walk her through how to troubleshoot her PC without um you know seeing the screen just based on her sort of sorry just based on the 64year-old librarian's kind of like voice prompts. Good luck little agent. You're going to need it. Now, I probably shouldn't be laughing because here's the thing on this particular benchmark. the towel to bench.

### ğŸ”– ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 28

So, Claude Opus 4.6 is just the dominant leader, almost 92%, Gemini 3 Pro is sitting at 85.3%. The top three spots are various Claude models and using Claude Opus 4.6. So, again, it's at 91.9 on that TW bench. It's been really good and really patient with me when it needs to walk me through something that I'm just like not grasping when it comes to, you know, usually something pretty simple and I'm just like too lazy to to think it through. It's very patient.

### ğŸ¨ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 29

And it's like, you're in the wrong folder. You need to run that command, but in the right folder. Here's the right folder. Here's what you type in to go to the right folder. You know, 2 minutes later, I tell it, you know, it's still not working.

### ğŸš€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 30

It's giving me an error message. It's like, no, you're still in the wrong folder. Again, type in this command. I'm like, I I know. I know.

### âš¡ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 31

So, again, it's like really good at allowing for the human to be a little bit clueless and not assuming that the human carries out everything correctly. It sort of keeps a good sort of state representation of of where we are in the process and it doesn't move on to the next thing until it knows that the human actually did what it was supposed to do. So cladopus sitting at 91.9 where is Gemini 3.1 Pro? They kind of did this weird thing where they wanted to show that hey we beat Claude right but basically there's a number of different categories under the TOA 2 bench. In retail Gemini 3.1 Pro got 90.8 8 right so slightly below clot opus 4.6 six the previously rating champion or maybe even currently but on telecom Gemini 3.1 Pro got 99.3 right so for telecom operations it's almost flawless so unfortunately I couldn't find Google deep mine posting the sort of the average for that particular benchmark it sounds like it's slightly behind OPUS which is the current leader but whatever the case is we're seeing these new benchmarks emerge they're brand new and they kind of represent the agentic capabilities of these models models.

### ğŸŒŸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 32

And this is what we should be looking at. Google is very clearly kind of framing it from that perspective. It's saying here are the benchmarks that matter. Here are the four that really kind of represent the agent capabilities and they're showing that they're leading or at least tied for first place, very close to first place. And every single one of them also whatever they figured out with the Deep Think version definitely transfers.

### ğŸ¬ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 33

As they said in their release post, the same kind of core intelligence that powers deep think is available now for more practical applications. So definitely a noteworthy achievement, especially since we need to remember what the timelines here are. We're comparing the progress with models that were best in the world, you know, months ago, 3 months ago when we're comparing against the Gemini 3.0 Pro. But the real test will of course be when we're able to actually test this thing out. I've been having a very hard time actually getting anything out of the API.

### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 34

Everything basically crashed, which is very normal on a launch day because basically the entire world is attempting to do it right now. So everything is slow and buggy and crashy, but usually that resolves within sometimes the same day. So I'm hoping to get a second video out either later today or tomorrow where we really are going to put this thing through its paces. Some numbers on a benchmark is one thing. gonna do real world things for me and and for you and for whatever our use cases are.

### â­ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 35

That's really what tells the real story. So, let me know what you think and I'll see you in the next

---

<div align="center">

**ğŸ“ ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™**

ç”Ÿæˆæ—¥: 2026å¹´02æœˆ20æ—¥

</div>
